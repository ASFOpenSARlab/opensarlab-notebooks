{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.jpg\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 8: Change Detection in <font color='rgba(200,0,0,0.2)'>Your Own</font> SAR Amplitude Time Series Stack </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer; University of Alaska Fairbanks & Josef Kellndorfer, <a href=\"http://earthbigdata.com/\" target=\"_blank\">Earth Big Data, LLC</a> </b> <br>\n",
    "<img style=\"padding: 7px\" src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\"/>\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. It is introducing you to the methods of change detection in deep multi-temporal SAR image data stacks. Specifically, the lab applies the method of <i>Cumulative Sums</i> to perform change detection in a 60 image deep Sentinel-1 data stack over Niamey, Niger. As previously, the work will be done within the framework of a Jupyter Notebook.\n",
    "\n",
    "<font color='rgba(200,0,0,0.2)'> <b>Note:</b> This version of Lab 8 is modified to allow for change detection analysis on your own data stack created within ASF HyP3</font> \n",
    "<br><br>\n",
    "\n",
    "<b>In this chapter we introduce the following data analysis concepts:</b>\n",
    "\n",
    "- How to use your own HyP3-generated data stack in a change detection effort\n",
    "- The concepts of time series slicing by month, year, and date.\n",
    "- The concepts and workflow of Cumulative Sum-based change point detection.\n",
    "- The identification of change dates for each identified change point.\n",
    "</font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>THIS NOTEBOOK INCLUDES NO HOMEWORK ASSIGNMENTS.</b></font> \n",
    "\n",
    "<font size=\"3\">Contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 0. Importing Relevant Python Packages </b> </font>\n",
    "\n",
    "<font size=\"3\"> The first step of this lab exercise on SAR image time series analysis is the import of necessary python libraries into your Jupyter Notebook. See the code cell below for information on which libraries are needed. Information on these libraries is provided in the instructions to a previous lab of this course (Lab 3). \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gdal\n",
    "import numpy as np\n",
    "import time,os\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "          'weight' : 'bold',\n",
    "          'size'   : 18}\n",
    "plt.rc('font',**font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 1. Load Your Own Data Stack Into the Notebook </b> </font> \n",
    "\n",
    "<font size=\"3\"> This lab assumes that you've created your own data stack over your personal area of interest using the <a href=\"https://www.asf.alaska.edu/\" target=\"_blank\">Alaska Satellite Facility's</a> value-added product system <a href=\"http://hyp3.asf.alaska.edu/\" target=\"_blank\">HyP3</a>. HyP3 is an environment that is used by ASF to prototype value added products and provide them to users to collect feedback. \n",
    "\n",
    "This lab expects Radiometric Terrain Corrected (RTC) image products as input. When creating your input data within HyP3, I recommend to stick to a unique orbit geometry (ascending or descending) to keep geometric differences between images low. \n",
    "\n",
    "We will retrieve HyP3 data via the HyP3 API. As both HyP3 and the Notebook environment sit in the <a href=\"https://aws.amazon.com/\" target=\"_blank\">Amazon Web Services (AWS)</a> cloud, data transfer is quick and cost effective.</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> To download data from ASF, you need to provide your <a href=\"https://www.asf.alaska.edu/get-data/get-started/free-earthdata-account/\" target=\"_blank\">NASA Earth Data</a> username to the system. <b>The following field allows you to store your NASA Earth Data <font color='rgba(200,0,0,0.2)'>username and password</font> in this notebook for later use in data downloading:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URS creds and add to .netrc so we can do stuff!!\n",
    "from getpass import getpass\n",
    "\n",
    "NEDusr = 'fmeyer'\n",
    "NEDpass = getpass()\n",
    "\n",
    "with open('/home/jovyan/.netrc', 'w') as f:\n",
    "    f.write('machine urs.earthdata.nasa.gov login ' + NEDusr + ' password ' + NEDpass + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> With your username/password now in place, you can now <b>log into the HyP3 API and query your existing subscriptions:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into the Hyp3 API\n",
    "from asf_hyp3 import API\n",
    "api = API(NEDusr)\n",
    "api.login(NEDpass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HyP3 Sub Name\n",
    "subs = api.get_subscriptions()\n",
    "#print(subs)\n",
    "#xx = 10\n",
    "for sub in subs:\n",
    "    print(f\"\\nSubscription id: {sub['id']} {sub['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Look through your existing subscriptions and <b>select the subscription ID</b> you would like to work with. <b>Enter the selected ID in the code cell below</b> (replace \"1478\" with the ID of your choice). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at products in subscription and print out download urls\n",
    "#products = api.get_products(sub_id=1478, page=0)\n",
    "products = api.get_products(sub_id=1476)\n",
    "print(products)\n",
    "\n",
    "#print(f\"\\nURLs of products: \\n\")\n",
    "for p in products:\n",
    "    print(f\"{p['url']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Now you are ready to <b>download</b> all files associated with your subscription. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download products from subscription.\n",
    "!mkdir -p granules\n",
    "\n",
    "import re\n",
    "for p in products:\n",
    "    url = p['url']\n",
    "    _match = re.match(r'https://hyp3-download.asf.alaska.edu/asf/data/(.*).zip', url)\n",
    "    granule = _match.group(1)\n",
    "    \n",
    "    \n",
    "    filename = 'granules/' + granule\n",
    "    # Guess we need to download and unzip\n",
    "    if not os.path.isdir(filename):\n",
    "        print(f\"Downloading new granule '{granule}' from '{url}'\")\n",
    "        !wget -O granules/{granule}.zip \"{url}\"\n",
    "        !unzip -d granules granules/{granule}.zip\n",
    "        !rm granules/{granule}.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Run the following code cell to visualize the image acquisition dates in your subscription. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need dates for\n",
    "!ls granules/*/*_VV.tif | sort -t_ -k5,5 | cut -c 27-34 > butte.dates\n",
    "!cat butte.dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> You may notice duplicates in your acquisition dates. As HyP3 processes SAR data on a frame-by-frame basis, duplicates may occur if your area of interest is covered by two consecutive  image frames. In this case, two separate images are generated that need to be merged together before time series processing can commence. <b>The next code cell is identifying frames in need to merging and is mosaicking these frames together.</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the paths of the VV\n",
    "tiff_paths = !ls granules/*/*_VV.tif | sort -t_ -k5,5\n",
    "print(f\"Tiff paths: {tiff_paths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Before you can merge frames, you need to <b>fix multiple UTM Zones-related issues</b> should they exist in your data set. If multiple UTM zones are fond, the following code cells will identify the predominant UTM zone and reproject the rest of your data stack into that zone. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls granules/*/*_VV.tif | sort -t_ -k5,5 > butte.files\n",
    "if os.path.exists(\"test\"):\n",
    "    os.remove(\"test\")\n",
    "#!cat butte.files\n",
    "dates=open('butte.dates').readlines()\n",
    "files=open('butte.files').readlines()\n",
    "output_file = ('utmzones.txt', 'w')\n",
    "print('Checking UTM Zones in the data stack ...')\n",
    "for  k in range(0, len(dates)):\n",
    "    gdal_command = f\"gdalinfo {tiff_paths[k]} | grep '^    AUTHORITY' | cut -d '\\\"' -f 2,4 | tr '\\\"' ':'\"\n",
    "    #print(f\"Calling the command: {gdal_command}\")\n",
    "    !{gdal_command} >> test\n",
    "    if ((k+1)/len(dates)*100)%5 == 0:\n",
    "        print(\"%4.1f percent completed ...\" % ((k+1)/len(dates)*100))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f 2 -d ':' test > utmzones\n",
    "utmzones=[i.strip() for i in open('utmzones').readlines()]\n",
    "utmzones2=[i.strip() for i in open('test').readlines()]\n",
    "\n",
    "utmunique, counts = np.unique(utmzones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "maxutm = utmunique[a][0]\n",
    "reproind = [i for i, j in enumerate(utmzones) if j != maxutm]\n",
    "print('--------------------------------------------')\n",
    "print('Reprojecting %4.1f files' %(len(reproind)))\n",
    "print('--------------------------------------------')\n",
    "for k in reproind:\n",
    "    temppath = files[k].strip()\n",
    "    _, granule_name, tiff_name = temppath.split('/')\n",
    "    cmd = f\"gdalwarp -overwrite granules/{granule_name}/{tiff_name} granules/{granule_name}/r{tiff_name} -s_srs {utmzones2[k]} -t_srs EPSG:{maxutm}\"\n",
    "    #print(f\"Calling the command: {cmd}\")\n",
    "    !{cmd}\n",
    "    rm_command = f\"rm {files[k].strip()}\"\n",
    "    #print(f\"Calling the command: {rm_command}\")\n",
    "    !{rm_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Now you are ready to <b>concatenate neighboring image frames</b> should your area be covered by more than one frame. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls granules/*/*_VV.tif | sort -t_ -k5,5 > butte.files\n",
    "#!cat butte.files\n",
    "dates=open('butte.dates').readlines()\n",
    "files=open('butte.files').readlines()\n",
    "for  k in range(1, len(dates)):\n",
    "    if dates[k] == dates[k-1]:\n",
    "        #gdal_merge -o files[k-1] files[k] files[k-1]\n",
    "        print(k)\n",
    "        temp = tiff_paths[k-1]\n",
    "        _, granule_name, tiff_name = temp.split('/')\n",
    "        gdal_command = f\"gdal_merge.py -o granules/{granule_name}/new-{tiff_name} {tiff_paths[k]} {tiff_paths[k-1]}\"\n",
    "        print(f\"Calling the command: {gdal_command}\")\n",
    "        !{gdal_command}\n",
    "        rm_command = f\"rm {tiff_paths[k]}\"\n",
    "        print(f\"Calling the command: {rm_command}\")\n",
    "        !{rm_command}\n",
    "        rm_command = f\"rm {tiff_paths[k-1]}\"\n",
    "        print(f\"Calling the command: {rm_command}\")\n",
    "        !{rm_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Let's verify that all date duplicates were resolved: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need dates for\n",
    "!ls granules/*/*_VV.tif | sort -t_ -k5,5 | cut -c 27-34 > butte.dates\n",
    "!cat butte.dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 2. Create Subset and Stack Up Your Data </b> </font> \n",
    "\n",
    "<font size=\"3\"> Now you are ready to work with your data. The next cells allow you to select an area of interest (AOI; via bounding-box corner coordinates) for your data analysis. Once selected, the AOI is being extracted and a data stack is formed.\n",
    "\n",
    "<b>As a first step, we extract your AOI from the full frames:</b>\n",
    "</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google Maps, get the rough bounding box for the subset\n",
    "ulx = -121.65\n",
    "lrx = -121.4\n",
    "uly = 39.85\n",
    "lry = 39.7\n",
    "!echo {ulx} {lrx} {lry} {uly}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the paths of the VV\n",
    "tiff_paths = !ls granules/*/*_VV.tif | sort  -t_ -k5,5\n",
    "#print(f\"Tiff paths: {tiff_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through and subset the tiffs in the products\n",
    "!mkdir -p tiffs\n",
    "for tiff_path in tiff_paths:\n",
    "    _, granule_name, tiff_name = tiff_path.split('/')\n",
    "    g1, g2, g3, date, g4, g5, g6 = tiff_name.split('_')\n",
    "    # Using the GDAL subset service, get a small subset around the Butte\n",
    "    #!wget -O {granule_name}_VV.tiff \"https://services.asf.alaska.edu/geospatial/subset?ulx={ulx}&lrx={lrx}&lry={lry}&uly={uly}&product={granule_name}.zip/{granule_name}/{tiff_name}\"\n",
    "\n",
    "    # GDAL service is out of service. Pretend that it isn't when calling the following equivalent command\n",
    "    gdal_command = f\"gdal_translate -projwin {ulx} {uly} {lrx} {lry} -projwin_srs 'WGS84' -co \\\"COMPRESS=DEFLATE\\\" -a_nodata 0 {tiff_path} tiffs/{date}_VV.tiff\"\n",
    "    print(f\"Calling the command: {gdal_command}\")\n",
    "    !{gdal_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Now we stack up your data by creating a virtual raster table with links to all subset data files: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VRT for the downloaded subset geotiffs\n",
    "# Grab all tiffs in the directory\n",
    "!gdalbuildvrt -separate butte.vrt tiffs/*.tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need dates for\n",
    "!ls tiffs/*_VV.tiff | sort | cut -c 7-21 > butte.dates\n",
    "!cat butte.dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 3. Now You Can Work With Your Data </b> </font> \n",
    "\n",
    "<font size=\"3\"> Now you are ready to perform time series change detection on your data stack.\n",
    "</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.1 Define Data Directory and Path to VRT </b> </font> \n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> Just some path definitions. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some paths\n",
    "datadirectory='/home/jovyan/notebooks/ASF/GEOS_657_Labs'\n",
    "datefile='butte.dates'\n",
    "imagefile='butte.vrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some indices for plotting\n",
    "dates=open(datefile).readlines()\n",
    "tindex=pd.DatetimeIndex(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bands and times\n",
    "j=1\n",
    "print('Bands and dates for',imagefile)\n",
    "for i in tindex:\n",
    "    print(\"{:4d} {}\".format(j, i.date()),end=' ')\n",
    "    j+=1\n",
    "    if j%5==1: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.2 Open Your Data Stack and Visualize Some Layers </b> </font> \n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> We will open your VRT and visualize some layers using Matplotlib. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open virtual dataset\n",
    "img=gdal.Open(imagefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.RasterCount) # Number of Bands\n",
    "print(img.RasterXSize) # Number of Pixels\n",
    "print(img.RasterYSize) # Number of Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raster data for the first two bands\n",
    "raster_1 = img.GetRasterBand(1).ReadAsArray()\n",
    "where_are_NaNs = np.isnan(raster_1)\n",
    "raster_1[where_are_NaNs] = 0\n",
    "\n",
    "raster_3 = img.GetRasterBand(16).ReadAsArray()\n",
    "where_are_NaNs = np.isnan(raster_3)\n",
    "raster_3[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some things\n",
    "fig = plt.figure(figsize=(18,10)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(221)  # 121 determines: 2 rows, 2 plots, first plot\n",
    "ax2 = fig.add_subplot(222)  # 122 determines: 2 rows, 2 plots, second plot\n",
    "ax3 = fig.add_subplot(223)  # 223 determines: 2 rows, 2 plots, third plot\n",
    "ax4 = fig.add_subplot(224)  # 224 determines: 2 rows, 2 plots, fourth plot\n",
    "\n",
    "# First plot: Image\n",
    "bandnbr=1\n",
    "ax1.imshow(raster_1,cmap='gray',vmin=0,vmax=0.2) #,vmin=2000,vmax=10000)\n",
    "ax1.set_title('Image Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))\n",
    "\n",
    "# Second plot: Histogram\n",
    "# IMPORTANT: To get a histogram, we first need to *flatten* \n",
    "# the two-dimensional image into a one-dimensional vector.\n",
    "h = ax2.hist(raster_1.flatten(),bins=200,range=(0,0.3))\n",
    "ax2.xaxis.set_label_text('Amplitude? (Uncalibrated DN Values)')\n",
    "ax2.set_title('Histogram Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))\n",
    "\n",
    "\n",
    "# Third plot: Image\n",
    "bandnbr=2\n",
    "ax3.imshow(raster_3,cmap='gray',vmin=0,vmax=0.2) #,vmin=2000,vmax=10000)\n",
    "ax3.set_title('Image Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))\n",
    "\n",
    "# Fourth plot: Histogram\n",
    "# IMPORTANT: To get a histogram, we first need to *flatten* \n",
    "# the two-dimensional image into a one-dimensional vector.\n",
    "h = ax4.hist(raster_3.flatten(),bins=200,range=(0,0.3))\n",
    "ax4.xaxis.set_label_text('Amplitude? (Uncalibrated DN Values)')\n",
    "ax4.set_title('Histogram Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.3 Create a Time Series Animation </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> Now we are ready to <b>create a time series animation</b> from the calibrated SAR data. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = img.GetRasterBand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster0 = band.ReadAsArray()\n",
    "bandnbr=0 # Needed for updates\n",
    "rasterstack=img.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = np.ma.masked_where(rasterstack == 0, rasterstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "\n",
    "fig=plt.figure(figsize=(14,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "vmin=np.percentile(rasterstack.flatten(),5)\n",
    "vmax=np.percentile(rasterstack.flatten(),95)\n",
    "\n",
    "r0dB=20*np.log10(raster0)-83\n",
    "\n",
    "im = ax.imshow(raster0,cmap='gray',vmin=vmin,vmax=vmax)\n",
    "ax.set_title(\"{}\".format(tindex[0].date()))\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"{}\".format(tindex[i].date()))\n",
    "    im.set_data(rasterstack[i])\n",
    "\n",
    "# Interval is given in milliseconds\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, \n",
    "                                         frames=rasterstack.shape[0],\n",
    "                                        interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "rc('animation',embed_limit=40971520.0)  # We need to increase the \n",
    "            # limit maybe to show the entire animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('animation.gif', writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"5\"> <b> 4. Cummulative Sum-based Change Detection Across an Entire Image</b> </font> \n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> With numpy arrays we can apply the concept of **cumulative sum change detection** analysis effectively on the entire image stack. We take advantage of array slicing and axis-based computing in numpy. Axis 0 is the time domain in our raster stacks.\n",
    "    \n",
    "<hr>\n",
    "<font size=\"4\"><b>4.1 We first create our time series stack:</b>\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= 10.*np.log10(rs2)  # Uncomeent to test dB scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">Sometimes it makes sense to <b>extract a reduced time span</b> from the full time series to reduce the number of different change objects in a scene. In the following, we extract a shorter time span:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dind = np.where((tindex >'2017-10-20') & (tindex <'2018-10-31'))\n",
    "X_sub=np.squeeze(X[dind,:,:])\n",
    "tindex_sub=tindex[dind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "bandnbr=0\n",
    "vmin=np.percentile(X_sub[bandnbr],5)\n",
    "vmax=np.percentile(X_sub[bandnbr],95)\n",
    "plt.title('Band  {} {}'.format(bandnbr+1,tindex_sub[bandnbr].date()))\n",
    "plt.imshow(X_sub[0],cmap='gray',vmin=vmin,vmax=vmax)\n",
    "_=plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 4.2 Calculate Mean Across Time Series to Prepare for Calculation of Cummulative Sum $S$:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmean=np.mean(X_sub,axis=0)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(Xmean,cmap='gray')\n",
    "_=plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R=X_sub-Xmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(R[0])\n",
    "plt.title('Residuals for Band  {} {}'.format(bandnbr+1,tindex_sub[bandnbr].date()))\n",
    "_=plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 4.3 Calculate Cummulative Sum $S$ as well as Change Magnitude $S_{diff}$:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.cumsum(R,axis=0)\n",
    "Smax= np.max(S,axis=0)\n",
    "Smin= np.min(S,axis=0)\n",
    "Sdiff=Smax-Smin\n",
    "fig,ax=plt.subplots(1,3,figsize=(16,4))\n",
    "vmin=np.percentile(Smin.flatten(),3)\n",
    "vmax=np.percentile(Smax.flatten(),97)\n",
    "p=ax[0].imshow(Smax,vmin=vmin,vmax=vmax)\n",
    "ax[0].set_title('$S_{max}$')\n",
    "ax[1].imshow(Smin,vmin=vmin,vmax=vmax)\n",
    "ax[1].set_title('$S_{min}$')\n",
    "ax[2].imshow(Sdiff,vmin=vmin,vmax=vmax)\n",
    "ax[2].set_title('$S_{diff}$')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])\n",
    "_=fig.colorbar(p,cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 4.4 Mask $S_{diff}$ With a-priori Threshold To Idenfity Change Candidates:</b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\">To identified change candidate pixels, we can threshold $S_{diff}$ to reduce computation of the bootstrapping. For land cover change, we would not expect more than 5-10% change pixels in a landscape. So, if the test region is reasonably large, setting a threshold for expected change to 10% is appropriate. In our example, we'll start out with a very conservative threshold of 50%.\n",
    "\n",
    "The histogram for $S_{diff}$ is shown below.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "fig = plt.figure(figsize=(14,6)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(121)  # 121 determines: 2 rows, 2 plots, first plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "# Second plot: Histogram\n",
    "# IMPORTANT: To get a histogram, we first need to *flatten* \n",
    "# the two-dimensional image into a one-dimensional vector.\n",
    "h = ax1.hist(Sdiff.flatten(),bins=200,range=(0,np.max(Sdiff)))\n",
    "ax1.xaxis.set_label_text('Change Magnitude')\n",
    "ax1.set_title('Change Magnitude Histogram')\n",
    "plt.grid()\n",
    "n, bins, patches = ax2.hist(Sdiff.flatten(), bins=200, range=(0,np.max(Sdiff)), cumulative='True', density='True', histtype='step', label='Empirical')\n",
    "ax2.xaxis.set_label_text('Change Magnitude')\n",
    "ax2.set_title('Change Magnitude CDF')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precentile=0.9\n",
    "outind = np.where(n > precentile)\n",
    "threshind = np.min(outind)\n",
    "thres = bins[threshind]\n",
    "print('At the {}% percentile, the threshold value is {:2.2f}'.format(precentile*100,thres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">Using this threshold, we can <b>visualize our change candidate areas</b>:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sdiffmask=Sdiff<thres\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Change Candidate Areas (black)')\n",
    "_=plt.imshow(Sdiffmask,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 4.5 Bootstrapping to Prepare for Change Point Selection:</b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\">We can now perform bootstrapping over the candidate pixels. The workflow is as follows:\n",
    "<ul>\n",
    "    <li>Filter our residuals to the change candidate pixels</li>\n",
    "    <li>Perform bootstrapping over candidate pixels</li>\n",
    "</ul>\n",
    "For efficient computing we permutate the index of the time axis.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmask = np.broadcast_to(Sdiffmask,R.shape)\n",
    "Rmasked = np.ma.array(R,mask=Rmask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">On the masked time series stack of residuals, we can re-compute the cumulative sums:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smasked = np.ma.cumsum(Rmasked,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smasked_max= np.ma.max(Smasked,axis=0)\n",
    "Smasked_min= np.ma.min(Smasked,axis=0)\n",
    "Smasked_diff=Smasked_max-Smasked_min\n",
    "fig,ax=plt.subplots(1,3,figsize=(16,4))\n",
    "vmin=Smasked_min.min()\n",
    "vmax=Smasked_max.max()\n",
    "p=ax[0].imshow(Smasked_max,vmin=vmin,vmax=vmax)\n",
    "ax[0].set_title('$S_{max}$')\n",
    "ax[1].imshow(Smasked_min,vmin=vmin,vmax=vmax)\n",
    "ax[1].set_title('$S_{min}$')\n",
    "ax[2].imshow(Smasked_diff,vmin=vmin,vmax=vmax)\n",
    "ax[2].set_title('$S_{diff}$')\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])\n",
    "_=fig.colorbar(p,cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">Now let's perform <b>bootstrapping</b>:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index=np.random.permutation(Rmasked.shape[0])\n",
    "Rrandom=Rmasked[random_index,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps=100  # bootstrap sample size\n",
    "\n",
    "# to keep track of the maxium Sdiff of the bootstrapped sample:\n",
    "Sdiff_random_max = np.ma.copy(Smasked_diff) \n",
    "Sdiff_random_max[~Sdiff_random_max.mask]=0\n",
    "# to compute the Sdiff sums of the bootstrapped sample:\n",
    "Sdiff_random_sum = np.ma.copy(Smasked_diff) \n",
    "Sdiff_random_sum[~Sdiff_random_max.mask]=0\n",
    "# to keep track of the count of the bootstrapped sample\n",
    "n_Sdiff_gt_Sdiff_random = np.ma.copy(Smasked_diff) \n",
    "n_Sdiff_gt_Sdiff_random[~n_Sdiff_gt_Sdiff_random.mask]=0\n",
    "print(\"Running Bootstrapping for %4.1f iterations ...\" % (n_bootstraps))\n",
    "for i in range(n_bootstraps):\n",
    "    # For efficiency, we shuffle the time axis index and use that \n",
    "    #to randomize the masked array\n",
    "    random_index=np.random.permutation(Rmasked.shape[0])\n",
    "    # Randomize the time step of the residuals\n",
    "    Rrandom = Rmasked[random_index,:,:]  \n",
    "    Srandom = np.ma.cumsum(Rrandom,axis=0)\n",
    "    Srandom_max=np.ma.max(Srandom,axis=0)\n",
    "    Srandom_min=np.ma.min(Srandom,axis=0)\n",
    "    Sdiff_random=Srandom_max-Srandom_min\n",
    "    Sdiff_random_sum += Sdiff_random\n",
    "    Sdiff_random_max[np.ma.greater(Sdiff_random,Sdiff_random_max)]=\\\n",
    "    Sdiff_random[np.ma.greater(Sdiff_random,Sdiff_random_max)]\n",
    "    n_Sdiff_gt_Sdiff_random[np.ma.greater(Smasked_diff,Sdiff_random)] += 1\n",
    "    if ((i+1)/n_bootstraps*100)%10 == 0:\n",
    "        print(\"%4.1f percent completed ...\" % ((i+1)/n_bootstraps*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 4.6 Extract Confidence Metrix and Select Final Change Points:</b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\">We first compute for all pixels the confidence level $CL$, the change point significance metric $CP_{significance}$ and the product of the two as our confidence metric for identified change points:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL = n_Sdiff_gt_Sdiff_random/n_bootstraps\n",
    "CP_significance = 1.- (Sdiff_random_sum/n_bootstraps)/Sdiff \n",
    "#Plot\n",
    "fig,ax=plt.subplots(1,3,figsize=(16,4))\n",
    "a = ax[0].imshow(CL*100)\n",
    "fig.colorbar(a,ax=ax[0])\n",
    "ax[0].set_title('Confidence Level %')\n",
    "a = ax[1].imshow(CP_significance)\n",
    "fig.colorbar(a,ax=ax[1])\n",
    "ax[1].set_title('Significance')\n",
    "a = ax[2].imshow(CL*CP_significance)\n",
    "fig.colorbar(a,ax=ax[2])\n",
    "_=ax[2].set_title('CL x S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">Now we can set a change point threshold to identify most likely change pixels in our map of change candidates:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_thres=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.title('Detected Change Pixels based on Threshold %2.1f' % (cp_thres))\n",
    "a = ax.imshow(CL*CP_significance <  cp_thres,cmap='cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 4.7 Derive Timing of Change for Each Change Pixel:</b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\">Our last step in the identification of the change points is to extract the timing of the change. We will produce a raster layer that shows the band number of this first date after a change was detected. We will make use of the numpy indexing scheme. First, we create a combined mask of the first threshold and the identified change points after the bootstrapping. For this we use the numpy \"mask_or\" operation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mask of our change points from the new threhold and the previous mask\n",
    "cp_mask=np.ma.mask_or(CL*CP_significance<cp_thres,CL.mask)\n",
    "# Broadcast the mask to the shape of the masked S curves\n",
    "cp_mask2 = np.broadcast_to(cp_mask,Smasked.shape)\n",
    "# Make a numpy masked array with this mask\n",
    "CPraster = np.ma.array(Smasked.data,mask=cp_mask2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">To retrieve the dates of the change points we find the band indices in the time series along the time axis where the maximum of the cumulative sums was located. Numpy offers the \"argmax\" function for this purpose.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_index= np.ma.argmax(CPraster,axis=0)\n",
    "change_indices = list(np.unique(CP_index))\n",
    "change_indices.remove(0)\n",
    "print(change_indices)\n",
    "# Look up the dates from the indices to get the change dates\n",
    "alldates=tindex_sub\n",
    "change_dates=[str(alldates[x].date()) for x in change_indices]\n",
    "print(change_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">Lastly, we visualize the change dates by showing the $CP_{index}$ raster and label the change dates.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks=change_indices\n",
    "ticklabels=change_dates\n",
    "\n",
    "cmap=plt.cm.get_cmap('tab20',ticks[-1])\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "cax = ax.imshow(CP_index,interpolation='nearest',cmap=cmap)\n",
    "# fig.subplots_adjust(right=0.8)\n",
    "# cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "# fig.colorbar(p,cax=cbar_ax)\n",
    "\n",
    "ax.set_title('Dates of Change')\n",
    "# cbar = fig.colorbar(cax,ticks=ticks)\n",
    "cbar=fig.colorbar(cax,ticks=ticks,orientation='horizontal')\n",
    "_=cbar.ax.set_xticklabels(ticklabels,size=10,rotation=45,ha='right')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657 Microwave Remote Sensing - Version 1.0 - April 2019 </i>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
