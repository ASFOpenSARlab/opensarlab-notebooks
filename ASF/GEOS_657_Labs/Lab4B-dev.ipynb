{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.jpg\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 4B: Exploring SAR Data and SAR Time Series Analysis using Jupyter Notebooks </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer; University of Alaska Fairbanks & Josef Kellndorfer, <a href=\"http://earthbigdata.com/\" target=\"_blank\">Earth Big Data, LLC</a> </b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" />\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. It introduces you to the analysis of deep multi-temporal SAR image data stacks in the framework of *Jupyter Notebooks*. The Jupyter Notebook environment is easy to launch in any web browser for interactive data exploration with provided or new training data. Notebooks are comprised of text written in a combination of executable python code and markdown formatting including latex style mathematical equations. Another advantage of Jupyter Notebooks is that they can easily be expanded, changed, and shared with new data sets or newly available time series steps. Therefore, they provide an excellent basis for collaborative and repeatable data analysis. <br>\n",
    "\n",
    "<b>In this chapter we introduce the following data analysis concepts:</b>\n",
    "\n",
    "- How to load your own SAR data into Jupyter Notebooks and create a time series stack \n",
    "- How to apply calibration constants to covert initial digital number (DN) data into calibrated radar cross section information.\n",
    "- How to subset images and create a time series of your subset data.\n",
    "- How to explore the time-series information in SAR data stacks for environmental analysis.\n",
    "\n",
    "\n",
    "This Notebook is an addon to Lab 4 and doesn't include any homework assignments. Free free to play around with your own data and contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 0. Importing Relevant Python Packages </b> </font>\n",
    "\n",
    "<font size=\"3\">In this notebook we will use the following scientific libraries:\n",
    "<ol type=\"1\">\n",
    "    <li> <b><a href=\"https://pandas.pydata.org/\" target=\"_blank\">Pandas</a></b> is a Python library that provides high-level data structures and a vast variety of tools for analysis. The great feature of this package is the ability to translate rather complex operations with data into one or two commands. Pandas contains many built-in methods for filtering and combining data, as well as the time-series functionality. </li>\n",
    "    <li> <b><a href=\"https://www.gdal.org/\" target=\"_blank\">GDAL</a></b> is a software library for reading and writing raster and vector geospatial data formats. It includes a collection of programs tailored for geospatial data processing. Most modern GIS systems (such as ArcGIS or QGIS) use GDAL in the background.</li>\n",
    "    <li> <b><a href=\"http://www.numpy.org/\" target=\"_blank\">NumPy</a></b> is one of the principal packages for scientific applications of Python. It is intended for processing large multidimensional arrays and matrices, and an extensive collection of high-level mathematical functions and implemented methods makes it possible to perform various operations with these objects. </li>\n",
    "    <li> <b><a href=\"https://matplotlib.org/index.html\" target=\"_blank\">Matplotlib</a></b> is a low-level library for creating two-dimensional diagrams and graphs. With its help, you can build diverse charts, from histograms and scatterplots to non-Cartesian coordinates graphs. Moreover, many popular plotting libraries are designed to work in conjunction with matplotlib. </li>\n",
    "    <li> The <b><a href=\"https://www.pydoc.io/pypi/asf-hyp3-1.1.1/index.html\" target=\"_blank\">asf-hyp3 API</a></b> provides useful functions and scripts for accessing and processing SAR data via the Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline, or HyP3 (pronounced \"hype\"). </li>\n",
    "<li><b><a href=\"https://www.scipy.org/about.html\" target=\"_blank\">SciPY</a></b> is a library that provides functions for numerical integration, interpolation, optimization, linear algebra and statistics. </li>\n",
    "\n",
    "</font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> Our first step is to <b>import them:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific Libraries and hyp3 API\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import gdal # for gdalbuildvr, gdalinfo, gdalmerge, gdalwarp, gdal_translate, Open\n",
    "import numpy as np # for copy, isnan, log10, ma.masked_where, max, mean, min, percentile, power, unique, var, where \n",
    "import matplotlib.pylab as plb # for figure, grid, rcParams, savefig\n",
    "from asf_hyp3 import API # for get_products, get_subscriptions, login\n",
    "import scipy # for signal.savgol_filter()\n",
    "import scipy.signal\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from asf_notebook import path_exists\n",
    "#from asf_notebook import download_hyp3_products\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.animation\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as patches  # for Rectangle\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "\n",
    "# General Purpose Libraries\n",
    "import os # for chdir, getcwd, path.exists\n",
    "import glob # for glob\n",
    "import re # for match\n",
    "from getpass import getpass # used to input URS creds and add to .netrc\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>set up matplotlib plotting</b> inside the notebook:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 1. Load Your Own Data Stack Into the Notebook </b> </font> \n",
    "\n",
    "<font size=\"3\"> This lab assumes that you've created your own data stack over your personal area of interest using the <a href=\"https://www.asf.alaska.edu/\" target=\"_blank\">Alaska Satellite Facility's</a> value-added product system <a href=\"http://hyp3.asf.alaska.edu/\" target=\"_blank\">HyP3</a>. HyP3 is an environment that is used by ASF to prototype value added products and provide them to users to collect feedback. \n",
    "\n",
    "This lab expects <a href=\"https://media.asf.alaska.edu/uploads/RTC/rtc_atbd_v1.2_final.pdf\" target=\"_blank\">Radiometric Terrain Corrected</a> (RTC) image products as input, so be sure to select an RTC process when creating the subscription for your input data within HyP. Prefer a **unique orbit geometry** (ascending or descending) to keep geometric differences between images low. \n",
    "\n",
    "We will retrieve HyP3 data via the HyP3 API. As both HyP3 and the Notebook environment sit in the <a href=\"https://aws.amazon.com/\" target=\"_blank\">Amazon Web Services (AWS)</a> cloud, data transfer is quick and cost effective.</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> To download data from ASF, you need to provide your <a href=\"https://www.asf.alaska.edu/get-data/get-started/free-earthdata-account/\" target=\"_blank\">NASA Earth Data</a> username to the system. <b>The following field allows you to store your NASA Earth Data <font color='rgba(200,0,0,0.2)'>username and password</font> in this notebook for later use in data downloading:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "NASA_earthdata_username = \"aflewandowski\"\n",
    "NASA_earthdata_password = getpass()\n",
    "\n",
    "filename=\"/home/jovyan/.netrc\"\n",
    "with open(filename, 'w+') as f:\n",
    "    f.write(f\"machine urs.earthdata.nasa.gov login {NASA_earthdata_username} password {NASA_earthdata_password}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Before we download anything, let's <b>first create a working directory for this analysis and change into it:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory: /home/jovyan/notebooks/ASF/GEOS_657_Labs/lab_4B_data\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/jovyan/notebooks/ASF/GEOS_657_Labs\"\n",
    "if path_exists(path):\n",
    "    os.chdir(path)\n",
    "    !mkdir -p lab_4B_data\n",
    "    path = f\"{path}/lab_4B_data\"\n",
    "    if path_exists(path):\n",
    "        os.chdir(path)\n",
    "print(f\"current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> With your username/password now in place, you can now <b>log into the HyP3 API</b> (Documentation <a href=\"https://www.pydoc.io/pypi/asf-hyp3-1.1.1/index.html\" target=\"_blank\">here</a>):</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " login successful!\n"
     ]
    }
   ],
   "source": [
    "hyp3_api = API(NASA_earthdata_username)\n",
    "hyp3_api.login(NASA_earthdata_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\">Now,<b> Query your existing subscriptions.</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#precondition: must already be logged into hyp3\n",
    "def get_hyp3_subscriptions(hyp3_api_object):\n",
    "    subscriptions = hyp3_api.get_subscriptions(enabled=True)\n",
    "    if not subscriptions:\n",
    "        print(\"There are no subscriptions associated with this Hyp3 account.\")\n",
    "    return subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_hyp3_subscription(subscriptions):\n",
    "    if subscriptions:\n",
    "        subscription_ids = []\n",
    "        while(True):\n",
    "            subscription_id = None\n",
    "            while not subscription_id:\n",
    "                print(f\"Enter a subscription ID number:\")\n",
    "                for subscription in subscriptions:\n",
    "                    print(f\"\\nSubscription id: {subscription['id']} {subscription['name']}\")\n",
    "                    subscription_ids.append(subscription['id'])\n",
    "                try:\n",
    "                    subscription_id = int(input())\n",
    "                except ValueError:\n",
    "                    print(\"Invalid ID\\nPick a subscription ID from the above list.\")\n",
    "                    clear_output()\n",
    "            if subscription_id in subscription_ids: \n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid ID\\nPick a subscription ID from the above list.\")\n",
    "                clear_output()\n",
    "        return subscription_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASF_unzip(directory_path, file_path ):\n",
    "    file_name, ext = os.path.splitext(file_path)\n",
    "    if ext == \".zip\":\n",
    "        print(f\"Extracting: {file_path}\")\n",
    "        try:\n",
    "            zipfile.ZipFile(file_path).extractall(directory_path)\n",
    "        except zipfile.Zipfile.BadZipFile:\n",
    "            print(f\"Zipfile Error.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hyp3_products(hyp3_api_object, path):\n",
    "    subscriptions = get_hyp3_subscriptions(hyp3_api_object)\n",
    "    subscription_id = pick_hyp3_subscription(subscriptions)\n",
    "    if subscription_id:\n",
    "        products = hyp3_api.get_products(sub_id=subscription_id)\n",
    "        if path_exists(products_path):\n",
    "            for p in products:\n",
    "                url = p['url']\n",
    "                _match = re.match(r'https://hyp3-download.asf.alaska.edu/asf/data/(.*).zip', url)\n",
    "                product = _match.group(1)\n",
    "                filename = f\"{path}/{product}\"\n",
    "                if not os.path.exists(filename): # if not already present, we need to download and unzip products\n",
    "                    print(f\"\\n{product} is not present.\\nDownloading from {url}\")\n",
    "                    r = requests.get(url, stream=True)\n",
    "                    total_length = int(r.headers.get('content-length'))\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        start = time.perf_counter()\n",
    "                        dl = 0\n",
    "                        for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                            dl += len(chunk)\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                f.flush()\n",
    "                                done = int(50 * dl / int(total_length))\n",
    "                                print(\"\\r[%s%s] %s bps, %s%%    \" % ('=' * done, ' ' * (50-done), dl//(time.perf_counter() - start), int((100*dl)/total_length)), end='\\r', flush=True)    \n",
    "            print(f\"\\n\")\n",
    "            os.rename(filename, f\"{filename}.zip\")\n",
    "            filename = f\"{filename}.zip\"\n",
    "            ASF_unzip(path, filename)\n",
    "            os.remove(filename)\n",
    "            print(f\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p rtx_products\n",
    "products_path = f\"{path}/rtx_products\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a subscription ID number:\n",
      "\n",
      "Subscription id: 1658 Ted_Stevens_Airport_2018_Earthquake_RTC_GAMMA\n",
      "1658\n",
      "\n",
      "S1A_IW_GRDH_1SDV_20181204T162922_20181204T162947_024878_02BD62_878D-POEORB-30m-power-rtc-gamma is not present.\n",
      "Downloading from https://hyp3-download.asf.alaska.edu/asf/data/S1A_IW_GRDH_1SDV_20181204T162922_20181204T162947_024878_02BD62_878D-POEORB-30m-power-rtc-gamma.zip\n",
      "[==================================================] 97610096.0 bps, 100%    \n",
      "\n",
      "Extracting: /home/jovyan/notebooks/ASF/GEOS_657_Labs/lab_4B_data/rtx_products/S1A_IW_GRDH_1SDV_20181204T162922_20181204T162947_024878_02BD62_878D-POEORB-30m-power-rtc-gamma.zip\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "download_hyp3_products(hyp3_api, products_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASF_unzip(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Run the following code cell to <b>create a file containing the image acquisition dates</b> in your subscription. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !unzip -d {products_path} {products_path}/S1A_IW_GRDH_1SDV_20181204T162922_20181204T162947_024878_02BD62_878D-POEORB-30m-power-rtc-gamma.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need dates for our products\n",
    "path = \"rtx_products\"\n",
    "if path_exists(path):\n",
    "    paths_gamma = f\"{path}/*/*_VV.tif\" # filename format for RTC-GAMMA\n",
    "    vv_gamma = glob.glob(paths_gamma)\n",
    "    paths_s1tbx = f\"{path}/*/*-VV.tif\" # filename format for RTC-S1TBX\n",
    "    vv_s1tbx = glob.glob(paths_s1tbx)\n",
    "    if vv_gamma and vv_s1tbx:\n",
    "        print(f\"There are both RTC-GAMMA and RTC-S1TBX data present in /{path}.\")\n",
    "        print(f\"Remove any leftover data from previous analyses before continuing.\")\n",
    "    elif vv_gamma:\n",
    "        !ls  $paths_gamma | sort -t_ -k5,5 | cut -c 31-38 > rtx_products.dates #27-34\n",
    "    elif vv_s1tbx:\n",
    "        !ls  $paths_s1tbx | sort -t_ -k5,5 | cut -c 31-38 > rtx_products.dates\n",
    "    else:\n",
    "        print(\"This recipe requires RTC imagery.\\nMake sure your Hyp subscription uses either RTC-GAMMA or RTC-S1TBX processes.\")\n",
    "    if os.path.exists(\"rtx_products.dates\"):\n",
    "        !cat rtx_products.dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> You may notice duplicates in your acquisition dates. As HyP3 processes SAR data on a frame-by-frame basis, duplicates may occur if your area of interest is covered by two consecutive  image frames. In this case, two separate images are generated that need to be merged together before time series processing can commence. <b>The next code cell is identifying frames in need to merging and is mosaicking these frames together.</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the paths of the VV\n",
    "if vv_gamma:\n",
    "    tiff_paths = !ls $paths_gamma | sort -t_ -k5,5\n",
    "elif vv_s1tbx:\n",
    "    tiff_paths = !ls $paths_s1tbx | sort -t_ -k5,5 \n",
    "if tiff_paths:\n",
    "    print(\"Tiff paths:\")\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Fix multiple UTM Zones-related issues</b> should they exist in your data set. If multiple UTM zones are fond, the following code cells will identify the predominant UTM zone and reproject the rest into that zone. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if vv_gamma:\n",
    "    !ls $paths_gamma | sort -t_ -k5,5 > rtx_products.files\n",
    "elif vv_s1tbx:\n",
    "    !ls $paths_s1tbx | sort -t_ -k5,5 > rtx_products.files\n",
    "if path_exists('rtx_products.dates'):\n",
    "    with open('rtx_products.dates', 'r') as d:\n",
    "        dates = d.readlines()\n",
    "if path_exists('rtx_products.files'):\n",
    "    with open('rtx_products.files', 'r') as f:\n",
    "        files = f.readlines()\n",
    "output_file = ('utmzones.txt', 'w')\n",
    "print('Checking UTM Zones in the data stack ...')\n",
    "for  k in range(0, len(dates)):\n",
    "    gdal_command = f\"gdalinfo {tiff_paths[k]} | grep '^    AUTHORITY' | cut -d '\\\"' -f 2,4 | tr '\\\"' ':'\"\n",
    "    !{gdal_command} >> test\n",
    "    if ((k+1)/len(dates)*100)%5 == 0:\n",
    "        print(\"%i%% completed ...\" % ((k+1)/len(dates)*100), end='\\r', flush=True)\n",
    "print('\\nDone!')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vv_gamma:\n",
    "    !ls $paths_gamma | sort -t_ -k5,5 > rtx_products.files\n",
    "elif vv_s1tbx:\n",
    "    !ls $paths_s1tbx | sort -t_ -k5,5 > rtx_products.files\n",
    "if path_exists('rtx_products.dates'):\n",
    "    with open('rtx_products.dates', 'r') as d:\n",
    "        dates = d.readlines()\n",
    "if path_exists('rtx_products.files'):\n",
    "    with open('rtx_products.files', 'r') as f:\n",
    "        files = f.readlines()\n",
    "output_file = ('utmzones.txt', 'w')\n",
    "print('Checking UTM Zones in the data stack ...')\n",
    "for  k in range(0, len(dates)):\n",
    "    gdal_command = f\"gdalinfo {tiff_paths[k]} | grep '^    AUTHORITY' | cut -d '\\\"' -f 2,4 | tr '\\\"' ':'\"\n",
    "        \n",
    "    !{gdal_command} >> test\n",
    "    if ((k+1)/len(dates)*100)%5 == 0:\n",
    "        print(\"%i%% completed ...\" % ((k+1)/len(dates)*100), end='\\r', flush=True)\n",
    "print('\\nDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f 2 -d ':' test > utmzones\n",
    "utmzones=[i.strip() for i in open('utmzones').readlines()]\n",
    "utmzones2=[i.strip() for i in open('test').readlines()]\n",
    "\n",
    "utmunique, counts = np.unique(utmzones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "maxutm = utmunique[a][0]\n",
    "reproind = [i for i, j in enumerate(utmzones) if j != maxutm]\n",
    "print('--------------------------------------------')\n",
    "print('Reprojecting %4.1f files' %(len(reproind)))\n",
    "print('--------------------------------------------')\n",
    "for k in reproind:\n",
    "    temppath = files[k].strip()\n",
    "    _, product_name, tiff_name = temppath.split('/')\n",
    "    cmd = f\"gdalwarp -overwrite rtx_products/{product_name}/{tiff_name} rtx_products/{product_name}/r{tiff_name} -s_srs {utmzones2[k]} -t_srs EPSG:{maxutm}\"\n",
    "    #print(f\"Calling the command: {cmd}\")\n",
    "    !{cmd}\n",
    "    rm_command = f\"rm {files[k].strip()}\"\n",
    "    #print(f\"Calling the command: {rm_command}\")\n",
    "    !{rm_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Concatenate neighboring image frames</b> should your area be covered by more than one frame. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $paths | sort -t_ -k5,5 > rtx_products.files\n",
    "#!cat granule.files\n",
    "with open('rtx_products.dates', 'r') as f:\n",
    "    dates = f.readlines()\n",
    "print(dates)\n",
    "for  k in range(1, len(dates)):\n",
    "    if dates[k] == dates[k-1]:\n",
    "        print(k)\n",
    "        temp = tiff_paths[k-1]\n",
    "        _, product_name, tiff_name = temp.split('/')\n",
    "        gdal_command = f\"gdal_merge.py -o rtx_products/{product_name}/new-{tiff_name} {tiff_paths[k]} {tiff_paths[k-1]}\"\n",
    "        print(f\"Calling the command: {gdal_command}\")\n",
    "        !{gdal_command}\n",
    "        rm_command = f\"rm {tiff_paths[k]}\"\n",
    "        print(f\"Calling the command: {rm_command}\")\n",
    "        !{rm_command}\n",
    "        rm_command = f\"rm {tiff_paths[k-1]}\"\n",
    "        print(f\"Calling the command: {rm_command}\")\n",
    "        !{rm_command}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Verify that all duplicate dates were resolved:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need dates for\n",
    "if vv_gamma:\n",
    "    !ls $paths_gamma | sort -t_ -k5,5 | cut -c 31-38 > rtx_products.dates\n",
    "elif vv_s1tbx:\n",
    "    !ls $paths_s1tbx | sort -t_ -k5,5 | cut -c 31-38 > rtx_products.dates\n",
    "!cat rtx_products.dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 2. Create Subset and Stack Up Your Data </b> </font> \n",
    "\n",
    "<font size=\"3\"> Now you are ready to work with your data. The next cells allow you to select an area of interest (AOI; via bounding-box corner coordinates) for your data analysis. Once selected, the AOI is being extracted and a data stack is formed.\n",
    "\n",
    "<b>As a first step, we extract your AOI from the full frames:</b>\n",
    "</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Example #1: Using Google Maps, get the rough bounding box for the subset\n",
    "# These are the corner coordinates focused on the location of the 2018 Camp Fire\n",
    "ulx = -121.7\n",
    "lrx = -121.4\n",
    "lry = 39.7\n",
    "uly = 39.9\n",
    "!echo {ulx} {lrx} {lry} {uly}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Example #2: Using Google Maps, get the rough bounding box for the subset\n",
    "# Corner coordinates for the Huntsville Ag and Forest Time Series\n",
    "ulx = -86.75\n",
    "lrx = -86.3\n",
    "lry = 34.6\n",
    "uly = 34.88\n",
    "!echo {ulx} {lrx} {lry} {uly}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google Maps, get the rough bounding box for the subset\n",
    "# Enter your corner coordinates below\n",
    "upper_left_x = -144.58\n",
    "lower_right_x = -144.48\n",
    "lower_right_y = 65.48\n",
    "upper_left_y = 65.53\n",
    "print(f\"upper left x coord: {upper_left_x}\\nupper left y coord: {upper_left_y}\\nlower right x coord: {lower_right_x}\\nlower right y coord: {lower_right_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the paths of the VV\n",
    "tiff_paths = None\n",
    "if vv_gamma:\n",
    "    tiff_paths = !ls $paths_gamma | sort  -t_ -k5,5\n",
    "elif vv_s1tbx:\n",
    "    tiff_paths = !ls $paths_s1tbx | sort  -t_ -k5,5\n",
    "if tiff_paths:\n",
    "    print(\"Tiff paths:\")\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through and subset the tiffs in the products\n",
    "!mkdir -p tiffs\n",
    "if path_exists('tiffs'):\n",
    "    for tiff_path in tiff_paths:\n",
    "        _, granule_name, tiff_name = tiff_path.split('/')\n",
    "        g1, g2, g3, date, g4, g5, g6 = tiff_name.split('_')\n",
    "        # Using the GDAL subset service, get a small subset around the Butte\n",
    "        #!wget -O {granule_name}_VV.tiff \"https://services.asf.alaska.edu/geospatial/subset?ulx={ulx}&lrx={lrx}&lry={lry}&uly={uly}&product={granule_name}.zip/{granule_name}/{tiff_name}\"\n",
    "\n",
    "        # GDAL service is out of service. Pretend that it isn't when calling the following equivalent command\n",
    "        gdal_command = f\"gdal_translate -projwin {upper_left_x} {upper_left_y} {lower_right_x} {lower_right_y} -projwin_srs 'WGS84' -co \\\"COMPRESS=DEFLATE\\\" -a_nodata 0 {tiff_path} tiffs/{date}_VV.tiff\"\n",
    "        print(f\"Calling the command: {gdal_command}\")\n",
    "        !{gdal_command}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info on the last subsetted tiff\n",
    "!gdalinfo tiffs/{date}_VV.tiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> Now we stack up your data by creating a virtual raster table with links to all subset data files: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VRT for the downloaded subset geotiffs\n",
    "# Grab all tiffs in the directory\n",
    "!gdalbuildvrt -separate rtx_products.vrt tiffs/*.tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need dates for\n",
    "!ls tiffs/*_VV.tiff | sort | cut -c 7-21 > rtx_products.dates\n",
    "!cat rtx_products.dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 3. Now You Can Work With Your Data </b> </font> \n",
    "\n",
    "<font size=\"3\"> Now you are ready to perform time series analysis on your data stack\n",
    "</font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.1 Define Data Directory and Path to VRT </b> </font> \n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> Just some path definitions. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some paths\n",
    "datadirectory = \"/home/jovyan/notebooks/GEOS 657 Labs/lab_4B_data\"\n",
    "datefile = \"rtx_products.dates\"\n",
    "imagefile = \"rtx_products.vrt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some indices for plotting\n",
    "if path_exists(datefile):\n",
    "    with open(datefile,'r') as f:\n",
    "        dates=f.readlines()\n",
    "        tindex=pd.DatetimeIndex(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bands and times\n",
    "j=1\n",
    "print(f\"Bands and dates for {imagefile}\")\n",
    "for i in tindex:\n",
    "    print(\"{:4d} {}\".format(j, i.date()),end=' ')\n",
    "    j+=1\n",
    "    if j%5==1: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.2 Open Your Data Stack and Visualize Some Layers </b> </font> \n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> We will open your VRT and visualize some layers using Matplotlib. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open virtual dataset\n",
    "img=gdal.Open(imagefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.RasterCount) # Number of Bands\n",
    "print(img.RasterXSize) # Number of Pixels\n",
    "print(img.RasterYSize) # Number of Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raster data for the first two bands\n",
    "raster_1 = img.GetRasterBand(1).ReadAsArray()\n",
    "where_are_NaNs = np.isnan(raster_1)\n",
    "raster_1[where_are_NaNs] = 0\n",
    "\n",
    "raster_3 = img.GetRasterBand(16).ReadAsArray()\n",
    "where_are_NaNs = np.isnan(raster_3)\n",
    "raster_3[where_are_NaNs] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot some things\n",
    "fig = plb.figure(figsize=(18,10)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(221)  # 221 determines: 2 rows, 2 plots, first plot\n",
    "ax2 = fig.add_subplot(222)  # 222 determines: 2 rows, 2 plots, second plot\n",
    "ax3 = fig.add_subplot(223)  # 223 determines: 2 rows, 2 plots, third plot\n",
    "ax4 = fig.add_subplot(224)  # 224 determines: 2 rows, 2 plots, fourth plot\n",
    "\n",
    "# First plot: Image\n",
    "bandnbr=1\n",
    "ax1.imshow(raster_1,cmap='gray',vmin=0,vmax=0.2) #,vmin=2000,vmax=10000)\n",
    "ax1.set_title('Image Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))\n",
    "\n",
    "# Second plot: Histogram\n",
    "# IMPORTANT: To get a histogram, we first need to *flatten* \n",
    "# the two-dimensional image into a one-dimensional vector.\n",
    "h = ax2.hist(raster_1.flatten(),bins=200,range=(0,0.3))\n",
    "ax2.xaxis.set_label_text('Amplitude? (Uncalibrated DN Values)')\n",
    "ax2.set_title('Histogram Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))\n",
    "\n",
    "\n",
    "# Third plot: Image\n",
    "bandnbr=2\n",
    "ax3.imshow(raster_3,cmap='gray',vmin=0,vmax=0.2) #,vmin=2000,vmax=10000)\n",
    "ax3.set_title('Image Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))\n",
    "\n",
    "# Fourth plot: Histogram\n",
    "# IMPORTANT: To get a histogram, we first need to *flatten* \n",
    "# the two-dimensional image into a one-dimensional vector.\n",
    "h = ax4.hist(raster_3.flatten(),bins=200,range=(0,0.3))\n",
    "ax4.xaxis.set_label_text('Amplitude? (Uncalibrated DN Values)')\n",
    "ax4.set_title('Histogram Band {} {}'.format(bandnbr, tindex[bandnbr-1].date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Open the image and read the first raster band\n",
    "band = img.GetRasterBand(1)\n",
    "\n",
    "# Define the subset\n",
    "subset = (45, 55, 110, 110)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Plot one band together with the outline of the selected subset to verify its geographic location.\n",
    "raster = band.ReadAsArray()\n",
    "vmin = np.percentile(raster.flatten(), 5)\n",
    "vmax = np.percentile(raster.flatten(), 95)\n",
    "fig = plb.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(raster, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "# plot the subset as rectangle\n",
    "_ = ax.add_patch(patches.Rectangle((subset[0], subset[1]), subset[2], subset[3], fill=False, edgecolor='red'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "raster0 = band.ReadAsArray(*subset)\n",
    "bandnbr = 0 # Needed for updates\n",
    "rasterstack = img.ReadAsArray(*subset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.3 Calibration and Data Conversion between dB and Power Scales </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> <font color='rgba(200,0,0,0.2)'> <b>Note, that if your data were generated by HyP3, this step is not necessary!</b> HyP3 performs the full data calibration and provides you with calibrated data in power scale. </font>\n",
    "    \n",
    "If, your data is from a different source, however, calibration may be necessary to ensure that image gray values correspond to proper radar cross section information. \n",
    "\n",
    "Calibration coefficients for SAR data are often defined in the decibel (dB) scale due to the high dynamic range of the imaging system. For the L-band ALOS PALSAR data at hand, the conversion from uncalibrated DN values to calibrated radar cross section values in dB scale is performed by applying a standard **calibration factor of -83 dB**. \n",
    "<br> <br>\n",
    "$\\gamma^0_{dB} = 20 \\cdot log10(DN) -83$\n",
    "\n",
    "The data at hand are radiometrically terrain corrected images, which are often expressed as terrain flattened $\\gamma^0$ backscattering coefficients. For forest and land cover monitoring applications $\\gamma^o$ is the preferred metric.\n",
    "\n",
    "<b>To apply the calibration constant for your data and export in *dB* scale, uncomment the following code cell</b>: </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #caldB=20*np.log10(rasterstack)-83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"> While **dB**-scaled images are often \"visually pleasing\", they are often not a good basis for mathematical operations on data. For instance, when we compute the mean of observations, it makes a difference whether we do that in power or dB scale. Since dB scale is a logarithmic scale, we cannot simply average data in that scale. \n",
    "    \n",
    "Please note that the **correct scale** in which operations need to be performed **is the power scale.** This is critical, e.g. when speckle filters are applied, spatial operations like block averaging are performed, or time series are analyzed.\n",
    "\n",
    "To **convert from dB to power**, apply: $\\gamma^o_{pwr} = 10^{\\frac{\\gamma^o_{dB}}{10}}$ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calPwr=np.power(10.,caldB/10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.4 Create a Time Series Animation </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> Now we are ready to <b>create a time series animation</b> from the calibrated SAR data. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = img.GetRasterBand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster0 = band.ReadAsArray()\n",
    "bandnbr=0 # Needed for updates\n",
    "rasterstack=img.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = np.ma.masked_where(rasterstack == 0, rasterstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "fig=plt.figure(figsize=(14,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "vmin=np.percentile(rasterstack.flatten(),5)\n",
    "vmax=np.percentile(rasterstack.flatten(),95)\n",
    "\n",
    "r0dB=20*np.log10(raster0)-83\n",
    "\n",
    "im = ax.imshow(raster0, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "ax.set_title(\"{}\".format(tindex[0].date()))\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"{}\".format(tindex[i].date()))\n",
    "    im.set_data(rasterstack[i])\n",
    "\n",
    "# Interval is given in milliseconds\n",
    "ani = animation.FuncAnimation(fig, animate, frames=rasterstack.shape[0], interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('animation',embed_limit=40971520.0)  # We need to increase the limit maybe to show the entire animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('animation.gif', writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.5 Plot the Time Series of Means Calculated Across the Subset </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> To create the time series of means, we will go through the following steps:\n",
    "1. Ensure that you use the data in **power scale** ($\\gamma^o_{pwr}$) for your mean calculations.\n",
    "2. compute means.\n",
    "3. convert the resulting mean values into dB scale for visualization.\n",
    "4. plot time series of means. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute Means \n",
    "rs_means_pwr = np.mean(rs2,axis=(1,2))\n",
    "# 3. Convert resulting mean value time-series to dB scale for visualization\n",
    "rs_means_dB = 10.*np.log10(rs_means_pwr)\n",
    "print(rs_means_pwr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Now let's plot the time series of means\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig=plt.figure(figsize=(16,4))\n",
    "ax1=fig.add_subplot(111)\n",
    "yhat = scipy.signal.savgol_filter(rs_means_pwr, 11, 4)\n",
    "ax1.plot(tindex,yhat, color='red', marker='o', markerfacecolor='white', linewidth=3, markersize=6)\n",
    "ax1.plot(tindex,rs_means_pwr, color='gray', linewidth=.5)\n",
    "plt.grid()\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('$\\overline{\\gamma^o}$ [power]')\n",
    "plt.savefig('RCSoverTime.png',dpi=300,transparent='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.6 Calculate Coefficient of Variance </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> The coefficient of variance describes how much the $\\sigma_{0}$ or $\\gamma_{0}$ measurements in a pixel vary over time. Hence, the coefficient of variance can indicate different vegetation cover and soil moisture regimes in your area.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.var(rasterstack,0)\n",
    "mtest = np.mean(rasterstack[rasterstack.nonzero()],0)\n",
    "coeffvar = test/(mtest+0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "fig=plt.figure(figsize=(13,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "vmin=np.percentile(coeffvar.flatten(),5)\n",
    "vmax=np.percentile(coeffvar.flatten(),95)\n",
    "ax.set_title('Coefficient of Variance Map')\n",
    "\n",
    "im = ax.imshow(coeffvar,cmap='jet',vmin=vmin,vmax=vmax)\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.savefig('Coeffvar.png',dpi=300,transparent='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b> 3.7 Threshold Coefficient of Variance Map </b> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> This is an example how to threshold the derived coefficient of variance map. This can be useful, e.g., to detect areas of active agriculture.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "fig = plt.figure(figsize=(14,6)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(121)  # 121 determines: 2 rows, 2 plots, first plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "# Second plot: Histogram\n",
    "# IMPORTANT: To get a histogram, we first need to *flatten* \n",
    "# the two-dimensional image into a one-dimensional vector.\n",
    "h = ax1.hist(coeffvar.flatten(),bins=200,range=(0,0.03))\n",
    "ax1.xaxis.set_label_text('Coefficient of Variation')\n",
    "ax1.set_title('Coeffvar Histogram')\n",
    "plt.grid()\n",
    "n, bins, patches = ax2.hist(coeffvar.flatten(), bins=200, range=(0,0.03), cumulative='True', density='True', histtype='step', label='Empirical')\n",
    "ax2.xaxis.set_label_text('Coefficient of Variation')\n",
    "ax2.set_title('Coeffvar CDF')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "outind = np.where(n > 0.85)\n",
    "threshind = np.min(outind)\n",
    "thresh = bins[threshind]\n",
    "coeffvarthresh = np.copy(coeffvar)\n",
    "coeffvarthresh[coeffvarthresh < thresh] = 0\n",
    "coeffvarthresh[coeffvarthresh > 0.1] = 0\n",
    "fig=plt.figure(figsize=(13,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "vmin=np.percentile(coeffvar.flatten(),5)\n",
    "vmax=np.percentile(coeffvar.flatten(),95)\n",
    "ax.set_title(r'Thresholded Coeffvar Map [$\\alpha=95%$]')\n",
    "\n",
    "im = ax.imshow(coeffvarthresh,cmap='jet',vmin=vmin,vmax=vmax)\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.savefig('Coeffvarthresh.png',dpi=300,transparent='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657 Microwave Remote Sensing - Version 1.0 - March 2019 </i>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
