{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.jpg\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 9: InSAR Time Series Analysis using GIAnT within Jupyter Notebooks<br>Part 1: Data Download & Preprocessing from a SARVIEWS Import <font color='rgba(200,0,0,0.2)'> -- [## Points] </font> </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer & Joshua J C Knicely; University of Alaska Fairbanks</b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /><font color='rgba(200,0,0,0.2)'> <b>Due Date: </b>NONE</font>\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. This lab is divided into 3 parts: 1) data download and preprocessing, 2) GIAnT time series, and 3) a simple Mogi source inversion. The primary goal of this lab is to demonstrate how to download the requisite data, specifically interferograms, and preprocess them for use with the Generic InSAR Analysis Toolbox (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">GIAnT</a>) in the framework of *Jupyter Notebooks*.<br>\n",
    "\n",
    "<b>Our specific objectives for this lab are to:</b>\n",
    "\n",
    "- Download data using ASF tools: \n",
    "    - From a SARVIEWS subscription. \n",
    "- Pre-process data: \n",
    "    - Subset (crop) the data to an Area of Interest (AOI). \n",
    "    - Verify the quality of the data.\n",
    "    - Cull data selection based on a timeframe and orbital characteristics. \n",
    "    - Reproject interferograms to a uniform UTM zone. \n",
    "    - Use TRAIN to remove static atmospheric effects related to surface elevation. (<i><b>TENTATIVE</i></b>)\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> Target Description </b> </font>\n",
    "\n",
    "<font size=\"3\"> In this lab, we will download interferograms covering a SARVIEWS area of interest.  </font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>THIS NOTEBOOK INCLUDES NO HOMEWORK ASSIGNMENTS.</b></font> <br>\n",
    "\n",
    "Contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>Overview</b></font>\n",
    "<br>\n",
    "<font size='3'><b>About TRAIN</b>\n",
    "<br>\n",
    "The tropospheric correction is one of the most significant challanges in InSAR. Without this correction, surface deformation signals can go completely unnoticed, or, perhaps worse, a false signal caused by the atmosphere can be taken as an accurate representation of surface deformation. This can often occur with volcanoes due to the characteristics of the atmosphere as well as the surface elevation (i.e., a taller point on the volcano means the InSAR signal passed through less atmosphere and will be affected differently from a point lower on the volcano). <br>We will use the Toolbox for Reducing Atmospheric InSAR Noise (TRAIN) today. The purpose of TRAIN is to add state of the art tropospheric correction methods to the InSAR processing chain. It can include corrections that are phase-based, using spectrometers, using weather models, and even data from balloon soundings. \n",
    "<br><br>\n",
    "<b>Limitations</b>\n",
    "<br>\n",
    "The particular version we are using was created by the Alaska Satellite Facility. ASF took the original MATLAB code developed by David Dekaert and converted it into Python 2.7. Currently, it only allows processing using MERRA2 data. Including other data types can be done relatively easily, though it does require modification of the existing python code. \n",
    "<br>\n",
    "Each of the correction methods included in TRAIN is ideal for different locations and conditions. Spectrometers provide the best correction, but can only be used in cloud-free and daylight conditions. Phase-based and weather model correction methods capture regional signals well, but fail to capture turbulent tropospheric signals. \n",
    "<br><br>\n",
    "More information about TRAIN, its capabilities, and its limitations can be found in <a href=\"http://www.sciencedirect.com/science/article/pii/S0034425715301231\">Bekaert et al. [2015]</a>, at David Dekaert's <a href=\"http://davidbekaert.com/#links\">webpage</a>, or at the <a href=\"https://github.com/asfadmin/hyp3-TRAIN\" target=\"_blank\">ASF Github</a>. \n",
    "<br><br>\n",
    "<b>Steps to use TRAIN</b><br>\n",
    "\n",
    "- System Setup\n",
    "    - Import Python Packages\n",
    "    - Set User Inputs\n",
    "- Download and Preprocess Data\n",
    "    - Access SARVIEWs Subscriptions\n",
    "    - Download and unzip the Data\n",
    "    - Project all Geotiffs to the Same UTM Zone\n",
    "    - Mosaic Geotiffs with Partial Coverage\n",
    "- Identify Area of Interest\n",
    "- Subset (Crop) Data to Area of Interest\n",
    "    - Subset the Data\n",
    "    - Check that Subsetted Geotiffs have Pixels\n",
    "    - Check the Dimensions of Subsetted Geotiffs\n",
    "- Create Input Files and Code for TRAIN\n",
    "    - Create <font face='Courier New'>parms_aps.txt</font> file\n",
    "    - Create <font face='Courier New'>ifgday.mat</font> file\n",
    "    - Convert Subsetted Tiffs to GCS Coordinates\n",
    "    - Adjust file names\n",
    "- Run TRAIN\n",
    "    - Minor Set Up\n",
    "    - Steps 0-3\n",
    "    - Step 4\n",
    "    - Comparison of Corrected and Uncorrected Unwrapped Phase\n",
    "    - Convert back to the original coordinate system\n",
    "\n",
    "<br><br>\n",
    "When you use TRAIN, cite the creator's work using:<br>\n",
    "Bekaert et al., RSE 2015, \"Statistical comparison of InSAR tropospheric correction techniques.\" <br>&emsp;Open access: http://www.sciencedirect.com/science/article/pii/S0034425715301231\n",
    "<!-- <br><br><b><i>DO WE NEED TO ALSO GIVE ASF A CITATION???</i></b> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>0. Move into Desired Directory</b></font><br>\n",
    "    <font size='3'>Before we start running code, we are going to move into our desired directory. This can be any folder. Typically, we do this to maintain better folder readability and organization. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>1. System Setup</b></font><br>\n",
    "    <font size='3'>We will first do some system setup. This involves importing requiesite Python libraries, activating Bokeh plotting, and defining all of our user inputs. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size=\"4\"> <b> 1.1 Import Python Packages and Enable Extensions</b></font>    <br>\n",
    "    <font size='3'>Let's <b>import the Python libraries</b> and packages we will need to run this lab, and then activate the Bokeh plotting. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "from datetime import date\n",
    "import tempfile\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import gdal\n",
    "import osr\n",
    "import pyproj\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from matplotlib import animation\n",
    "from matplotlib import rc\n",
    "from matplotlib.widgets import RectangleSelector\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Markdown\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from asf_notebook import new_directory\n",
    "from asf_notebook import path_exists\n",
    "from asf_notebook import remove_nan_filled_tifs\n",
    "from asf_notebook import AOI\n",
    "from asf_notebook import select_parameter\n",
    "from asf_notebook import EarthdataLogin\n",
    "from asf_notebook import get_wget_cmd\n",
    "from asf_notebook import asf_unzip\n",
    "from asf_notebook import get_hyp3_subscriptions\n",
    "from asf_notebook import gui_date_picker\n",
    "from asf_notebook import get_subscription_products_info\n",
    "from asf_notebook import get_products_dates_insar\n",
    "from asf_notebook import get_slider_vals\n",
    "from asf_notebook import input_path\n",
    "from asf_notebook import handle_old_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEVELOPMENT PURPOSES ONLY\n",
    "lst = ['MERRA2','ingrams','ingram_subsets','ingram_subsets_converted', \\\n",
    "       'GIAnT','GIAnT_Data','Geotiffs','DEM']\n",
    "for item in lst:\n",
    "    try:\n",
    "        shutil.rmtree(item)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size=\"4\"> <b> 1.2 Define an Analysis Directory</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    sub_dir = input_path(\n",
    "        f\"\\nPlease enter the name of a directory in which to store your data for this analysis.\")\n",
    "    if os.path.exists(sub_dir):\n",
    "        contents = glob.glob(f'{sub_dir}/*')\n",
    "        if len(contents) > 0:\n",
    "            choice = handle_old_data(sub_dir, contents)\n",
    "            if choice == 1:\n",
    "                shutil.rmtree(sub_dir)\n",
    "                os.mkdir(sub_dir)\n",
    "                break\n",
    "            if choice == 2:\n",
    "                break\n",
    "            else:\n",
    "                clear_output()\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        os.mkdir(sub_dir)\n",
    "        break\n",
    "os.chdir(sub_dir)\n",
    "home = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size=\"4\"> <b> 1.3 Set User Inputs </b></font>    <br>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>1.3.2 Directory Arrangement</b><br></font>\n",
    "    <font size='3'>Set some variables that affect how the folders are arranged.\n",
    "    </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the folder that will hold the full \n",
    "# interferograms and their associated files. \n",
    "ingram_folder = 'ingrams' \n",
    "replace_ingram = True # If True, any folder with the same \n",
    "                      # name will be deleted and recreated. \n",
    "\n",
    "# Designate the folder in which we wish to store our interferogram subsets. \n",
    "subset_folder = 'ingram_subsets' \n",
    "replace_subset = True \n",
    "delete_subsets = False # if True, this will delete the subset_folder at the end\n",
    "                       # of the lab. 'delete_subsets' should be set to False when\n",
    "                       # running this in class as we will use the uncorrected \n",
    "                       # subsets in Part 2. \n",
    "            \n",
    "# Designate the folder in which we wish to store our converted interferogram subsets. \n",
    "# This is important later in the program when we convert our subsets from a \n",
    "# local geographic coordinate system to decimal degrees. \n",
    "corrected_folder = 'ingram_subsets_converted'\n",
    "replace_corrected = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>1.3.3 HyP3 Login Information</b><br></font>\n",
    "<font size='3'>To download data from ASF, we need to provide our <a href=\"https://www.asf.alaska.edu/get-data/get-started/free-earthdata-account/\" target=\"_blank\">NASA Earth Data</a> username to the system. Setup an EarthData account if you do not yet have one. <font color='rgba(200,0,0,0.2)'><b>Note that EarthData's End User License Agreement (EULA) applies when accessing the Hyp3 API from this notebook. If you have not acknowleged the EULA in EarthData, you will need to navigate to <a href=\"https://earthdata.nasa.gov/\" target=\"_blank\">EarthData's home page</a> and complete that process.</b></font><br><br>\n",
    "    For some data processing later, we will also need to add <b>NASA GESDISC DATA ARCHIVE</b> to our list of approved applications. This is needed to provide access to MERRA2 data files needed by TRAIN to perfrom the atmospheric correction. This can be done by going to your <a href=\"https://urs.earthdata.nasa.gov/profile\" target=\"_blank\">EarthData's profile page</a>, clicking <b>Applications</b> and selecting <b>Approved Applications</b> from the drop down menu, select <b>Approve More Applications</b> at the bottom left, search for <b>NASA GESDISC DATA ARCHIVE</b>, select it, and agree to the terms and conditions. Once that is complete, you will have access to the MERRA2 data and TRAIN will be able to automatically download whichever files it requires. \n",
    "<br><br>\n",
    "<b>Login to Earthdata:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "login = EarthdataLogin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>1.3.4 Designate TRAIN Input Parameters</b><br></font>\n",
    "    <font size='3'><i></i></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = '' # temporary blank directory\n",
    "era_data_type = 'ECMWF'\n",
    "ifgday_file = 'ifgday.mat'\n",
    "merra2_datapath = './MERRA2'\n",
    "demPathNName = os.path.join(trainDir,'myDEM.tif')\n",
    "lambdaInMeters = 0.055465763 \n",
    "incidenceAngle = 38.5/180*np.pi # This needs to be in radians. \n",
    "extra = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 2. Download and Preprocess Data</b> </font>\n",
    "\n",
    "<font size=\"3\"> We will begin by acquiring the interferograms of selected in SARVIEWS.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>2.1 Access HyP3 Subscriptions</b><br></font>\n",
    "<font size='3'>We will now access our subscriptions for download. We will demonstrate 2 methods: 1. using a HyP3 subscription, and 2. using a SARVIEWS list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "<font face='Calibri'><font size='4'><b>2.2 Access SARVIEWs Subscription</b><br></font>\n",
    "<font size='3'>The below code is a demonstration of how to download data via a SARVIEWs subscription. The selected HyP3 product IDs are stored in the URL. <br>The first cell below acquires user credentials and creates a .netrc file in order to access Earthdata. The 2nd cell executes some javascript that loads this URL into the notebook's python kernel. Subsequently, this URL is parsed and the IDs are extracted. Using the supplied event ID, the following cell compares each of the IDs to all of the products in this event group and then returns a list of URLS from products whos id is included in the URL.  <font face='Courier New'>products_temp</font>. The user name and password are the corresponding Earthdata user login. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": [
    "# Make sure you are logged in\n",
    "try:\n",
    "    if login:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    login = EarthdataLogin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b> Use Javascript to extract this notebook's URL </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%javascript \n",
    "var kernel = Jupyter.notebook.kernel; \n",
    "var command = [\"notebookUrl = \",\n",
    "               \"'\", window.location, \"'\" ].join('')\n",
    "// alert(command)\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Notebook URL: {notebookUrl}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse URL and retreive product IDs and Event\n",
    "\n",
    "parsed_url = urlparse(notebookUrl)\n",
    "params = parse_qs(parsed_url.query)\n",
    "try:\n",
    "    ids = params['ids'][0].split(',')\n",
    "    event_id = params['event'][0]\n",
    "    \n",
    "    if len(ids) == 0:\n",
    "        print('No products were found from the url')\n",
    "    else:\n",
    "        print(f'Found {len(ids)} product IDs in the URL to prepare: ')\n",
    "        for product in ids:\n",
    "            print(product)\n",
    "\n",
    "    print(f'Using eventId: {event_id}')    \n",
    "except:\n",
    "    display(Markdown(f'<text style=color:red> ERROR: Missing Data</text>'))\n",
    "    display(Markdown(f'<text style=color:red> Go to <a href=\"http://sarviews-hazards.alaska.edu\" target=\"_blank\"> sarviews-hazards.alaska.edu </a> to find SARVIEWS data.</text>'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get download URLs using the event ID \n",
    "groups = login.api.get_groups_public(id=event_id)\n",
    "groupName = groups[0]['name']\n",
    "print(f'Loading Products from {groupName}...')\n",
    "\n",
    "# Load every page of products\n",
    "event_prods = []\n",
    "page = 1\n",
    "emptyQuery = False\n",
    "while not emptyQuery:\n",
    "    new_prods =  login.api.get_products_public(group_id=event_id, page=(page - 1))\n",
    "    event_prods += new_prods\n",
    "    if len(new_prods) == 0:\n",
    "        emptyQuery = True\n",
    "    else:\n",
    "        print(f'Loaded page {page} of products')\n",
    "        page += 1\n",
    "        \n",
    "download_urls = []\n",
    "for product in event_prods:\n",
    "    if str(product['id']) in ids:\n",
    "        download_urls.append(product['url'])\n",
    "        \n",
    "if (len(download_urls) == len(ids)):\n",
    "    print('All IDs are accounted for with download URLS! Ready to download.')\n",
    "else: \n",
    "    print(f'Only {len(download_urls)} products match those selected from the URL and are ready to download. ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>2.3 Download and unzip the data. </b></font>\n",
    "<br>\n",
    "<font size='3' face='Calibri'>We should now have a list of the products we wish to download. The code below will download the requisite zip files, unpack them into our designated folder, and then delete any remaining zip files. Removing the zip files helps to reduce space usage.</font>\n",
    "<br><br>\n",
    "<font size='3' face='Calibri'><b>Create the folder in which we wish to store our downloaded interferograms.</b> This deletes the current ingram directory if you set replace_ingram to true earlier in the notebook.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_ingram:\n",
    "    try:\n",
    "        # Try to remove the folder tree to ensure no other data exists in the folder. \n",
    "        shutil.rmtree(ingram_folder)\n",
    "    except: \n",
    "        pass\n",
    "!mkdir -p {ingram_folder} # create the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'><b>2.3.3 Download the Products</b><br></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "download_urls.sort()\n",
    "print(f\"There are {len(download_urls)} products to download.\")\n",
    "from asf_notebook import asf_unzip\n",
    "full_ingram_path = f\"{home}/{ingram_folder}\"\n",
    "if path_exists(full_ingram_path):\n",
    "    product_count = 1\n",
    "    print(f\"\\nEvent : {groupName}\")\n",
    "    for url in download_urls:\n",
    "        print(f\"\\nProduct Number {product_count} of {len(download_urls)}:\")\n",
    "        product_count += 1\n",
    "        \n",
    "        parsed = urlparse(url)\n",
    "        file_name = os.path.basename(parsed.path) \n",
    "        # print(f'Filename: {file_name}')\n",
    "        \n",
    "        # if not already present, we need to download and unzip products\n",
    "        newProductFolder = file_name.split('.zip')[0]\n",
    "        # print(f\"Location to Unzip to: {newProductFolder}\")\n",
    "        if not os.path.exists(newProductFolder):\n",
    "            print(\n",
    "                f\"\\n{newProductFolder} is not present.\\nDownloading from {url}\")\n",
    "            cmd = get_wget_cmd(url, login)\n",
    "            !$cmd\n",
    "            if os.path.exists(file_name):\n",
    "                zippedProduct = f\"{home}/{file_name}\"\n",
    "                zippedProduct = zippedProduct.split('\\n')[0]\n",
    "                print(f\"zipped Product: {zippedProduct}\")\n",
    "\n",
    "                asf_unzip(full_ingram_path, zippedProduct)\n",
    "\n",
    "                try:\n",
    "                    os.remove(file_name)\n",
    "                except OSError:\n",
    "                    pass\n",
    "                print(f\"\\nDone.\")\n",
    "            else:\n",
    "                print('Download failed, does this HyP3 product still exist?')\n",
    "        else:\n",
    "            print(f\"{newProductFolder} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Write functions to grab and print the path information for the amplitude, unwrapped phase, and the coherence files.</b> This information is useful later.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_paths(paths):\n",
    "    tiff_paths = !ls $paths | sort -t_ -k5,5\n",
    "    return tiff_paths\n",
    "\n",
    "def print_tiff_paths(tiff_paths):\n",
    "    print(\"Tiff paths:\")\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Call the functions we just wrote to gather the path information.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab the paths of the amplitude imagery\n",
    "paths_amp    = f\"{ingram_folder}/*/*_amp.tif\"\n",
    "paths_ingram = f\"{ingram_folder}/*/*_unw_phase.tif\"\n",
    "paths_cohr   = f\"{ingram_folder}/*/*_corr.tif\"\n",
    "amp_paths    = get_tiff_paths(paths_amp)\n",
    "ingram_paths = get_tiff_paths(paths_ingram)\n",
    "cohr_paths    = get_tiff_paths(paths_cohr)\n",
    "print(f\"amp_path[0]:    {amp_paths[0]}\")\n",
    "print(f\"ingram_path[0]: {ingram_paths[0]}\")\n",
    "print(f\"cohr_path[0]:   {cohr_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.5 Orbit Direction</b><br></font>\n",
    "<font size='3'><font face='Calibri'><font size='3'>We will acquire orbit information about our interferograms, retain only those that match our desired orbit direction, and then pickle the variable <font face='Courier New'>heading_avg</font> for use later in Part 2 of this lab. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Write a function that returns a list of only the paths to products in our selected orbit direction.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orbit(paths, txtToReplace):\n",
    "    filesToSort = paths.copy()\n",
    "    # Loop through 'filesToSort' in reverse order to remove \n",
    "    # those that don't meet our orbit requirement. \n",
    "    orbits = []\n",
    "    # We again must loop through these in reverse\n",
    "    # order to prevent skipping of entries. \n",
    "    for i in sorted(range(0,len(filesToSort)), reverse=True):\n",
    "        # parse filesToSort\n",
    "        path, file = os.path.split(filesToSort[i])\n",
    "        # get datetime stamp from file and use that to open \n",
    "        # the .txt file that holds the heading information\n",
    "        \n",
    "        # Get the name of the file to open and read. \n",
    "        file = file.replace(txtToReplace,'.txt')\n",
    "        metadata = open(os.path.join(path,file),\"r\")\n",
    "        for line in metadata:\n",
    "            t = line.split(':')\n",
    "            if 'Heading' in t[0]:\n",
    "                heading = float(t[1])\n",
    "                if abs(heading) >= 90.0:\n",
    "                    orbits.append('descending')\n",
    "                if abs(heading) < 90.0:\n",
    "                    orbits.append('ascending')\n",
    "    ascending = orbits.count('ascending')\n",
    "    descending = orbits.count('descending')\n",
    "    if ascending >= descending:\n",
    "        return 'ascending'\n",
    "    elif descending > ascending:\n",
    "        return 'descending'\n",
    "\n",
    "\n",
    "def sort_orbits(paths, orbit, txtToReplace):\n",
    "    filesToSort = paths.copy()\n",
    "    # Loop through 'filesToSort' in reverse order to remove \n",
    "    # those that don't meet our orbit requirement. \n",
    "    headings = []\n",
    "    # We again must loop through these in reverse\n",
    "    # order to prevent skipping of entries. \n",
    "    for i in sorted(range(0,len(filesToSort)), reverse=True):\n",
    "        # parse filesToSort\n",
    "        path, file = os.path.split(filesToSort[i])\n",
    "        # get datetime stamp from file and use that to open \n",
    "        # the .txt file that holds the heading information\n",
    "        \n",
    "        # Get the name of the file to open and read. \n",
    "        file = file.replace(txtToReplace,'.txt')\n",
    "        metadata = open(os.path.join(path,file),\"r\")\n",
    "        for line in metadata:\n",
    "            t = line.split(':')\n",
    "            if 'Heading' in t[0]:\n",
    "                heading = float(t[1])\n",
    "                headings.append(heading)\n",
    "        # Remove entries based on 'orbit'\n",
    "        if orbit.lower() == 'ascending':\n",
    "            # Remove any entries with a heading greater than \n",
    "            # or equal to 90.0 degrees from north\n",
    "            if abs(heading) >= 90.0:\n",
    "                del filesToSort[i]\n",
    "            #else:\n",
    "            # print(\"Keeping: {}\".format(filesToSort[i]))\n",
    "        elif orbit.lower() == 'descending':\n",
    "            # Remove any entries with a heading less than 90.0 degrees from north\n",
    "            if abs(heading) < 90.0:\n",
    "                del filesToSort[i]\n",
    "            #else:\n",
    "                #print(\"Keeping: {}\".format(filesToSort[i]))\n",
    "        else:\n",
    "            print(f\"Improper orbit designation.\")\n",
    "            print(f\"orbit: {orbit}\")\n",
    "            print(f\"Accepted Designations: 'ascending', 'descending'\")\n",
    "            break\n",
    "    heading_avg = np.mean(headings)\n",
    "    return filesToSort, heading_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Call sort_orbits to edit the paths to include only those that meet our orbit direction criterion.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbit = get_orbit(amp_paths,'_amp.tif') \n",
    "print(f'Primary orbit: {orbit}')\n",
    "\n",
    "amps,heading_avg = sort_orbits(amp_paths,orbit,'_amp.tif') \n",
    "ingrams, _ = sort_orbits(ingram_paths,orbit,'_unw_phase.tif')\n",
    "cohrs, _ = sort_orbits(cohr_paths,orbit,'_corr.tif')\n",
    "# In Python, the underscore symbol means to ignore output. \n",
    "# As the 'heading_avg' will be the same for all of these, \n",
    "# we can skip getting that information again. Alternatively, \n",
    "# if something seems to be going wrong, we could get \n",
    "# the average heading information all 3 times and place it \n",
    "# into a different variable each time for comparison. \n",
    "\n",
    "print('Average Heading: ', heading_avg)\n",
    "\n",
    "print(len(amps)) # if this is zero, then something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.5.2 Pickle <font face='Courier New'>heading_avg</font> for GIAnT</b><br></font>\n",
    "<font size='3'><font face='Calibri'><font size='3'>We will need the variable <font face='Courier New'>'filename = heading_avg'</font> for later use in Part 2 of the lab with GIAnT. As this is only a single variable that we must pass, this could be done manually, but it's good to know how to pass variables like this. We may need to transfer something much larger than a float at some point. </font></font>\n",
    "<br><br>\n",
    "<font face='Calibri' size='3'><b>Create an output file, pickle filename and store it for use in another lab.</b> We will unpickle this in Part 2.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'heading_avg'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(heading_avg, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.6 Project all 'tiff's to the Same UTM Zone</b><br></font>\n",
    "<font size='3'><font face='Calibri'><font size='3'>Some of the geotiffs may use different UTM zones. This can cause errors in processing the data. In the code below, we will identify the predominant UTM zone and reproject the rest into that zone. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3' face='Calibri'><b>Create a list of all of the geotiff files.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = amps + ingrams + cohrs\n",
    "print(f\"Example tiff and path: {tiff_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function that uses gdal.Info to determine the UTM definition types and zones in each product:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUTM_znt(tiff_paths):\n",
    "    utm_zones = []\n",
    "    utm_types = []\n",
    "    print('Checking UTM Zones in the data stack ...\\n')\n",
    "    for k in range(0, len(tiff_paths)):\n",
    "        info = (gdal.Info(tiff_paths[k], options = ['-json']))\n",
    "        info = (json.loads(info))['coordinateSystem']['wkt']\n",
    "        zone = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "        utm_zones.append(zone)\n",
    "        typ = info.split('ID')[-1].split('\"')[1]\n",
    "        utm_types.append(typ)\n",
    "    return utm_zones, utm_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Call getUTM_znt to determine the UTM definition types and zones in each product:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones, utm_types = getUTM_znt(tiff_paths)\n",
    "print(f\"Unique UTM Zones: {list(set(utm_zones))}\")\n",
    "print(f\"Unique UTM Types: {list(set(utm_types))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Identify the most commonly used UTM Zone in the data.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_unique, counts = np.unique(utm_zones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "predominate_utm = utm_unique[a][0]\n",
    "print(f\"Predominate UTM Zone: {predominate_utm}\")\n",
    "print(f\"Number of UTM Zones:  {len(utm_unique)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Make a list of indicies in utm_zones that need to be reprojected.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_indicies = [i for i, j in enumerate(utm_zones) if j != predominate_utm]\n",
    "print('Reprojecting %4.1f files' %(len(reproject_indicies)))\n",
    "print(reproject_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Call gdalwarp to reproject any geotiffs not in the predominant UTM zone.</b> These will be stored in a new file with a leading 'r' for identification. The originals are then deleted and the new geotiffs renamed. These new geotiffs are placed in an entirely new file as GDAL may overwrite parts of the original file before accessing them. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in reproject_indicies:\n",
    "    folder, tiff_name = os.path.split(tiff_paths[k])\n",
    "    cmd = (f\"gdalwarp -overwrite {folder}/{tiff_name} {folder}/r{tiff_name}\"\n",
    "           f\"-s_srs {utm_types[k]}:{utm_zones[k]} -t_srs EPSG:{predominate_utm}\")\n",
    "    print(f\"Calling the command: {cmd}\")\n",
    "    !{cmd}\n",
    "    rm_command = f\"rm {tiff_paths[k].strip()}\"\n",
    "    #print(f\"Calling the command: {rm_command}\")\n",
    "    #!{rm_command}\n",
    "    # remove the leading 'r' from the new file. \n",
    "    print(f\"Old = {os.path.join(folder,'r'+tiff_name)}\")\n",
    "    print(f\"New = {os.path.join(folder,tiff_name)}\")\n",
    "    os.rename(os.path.join(folder,'r'+tiff_name),os.path.join(folder,tiff_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Double check that all of the 'tiff's now have the same UTM Zone and type.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones, utm_types = getUTM_znt(tiff_paths)\n",
    "print(f\"Unique UTM Zones: {list(set(utm_zones))}\")\n",
    "print(f\"Unique UTM Types: {list(set(utm_types))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Assign the UTM zone to the 'utm' variable.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm = utm_zones[0][:]\n",
    "print(f\"UTM Zone: {utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.7 Mosaic Geotiffs with Partial Coverage</b><br></font>\n",
    "    <font size='3'>In this subsection, we will merge multiple frames from the same date into a single geotiff. This code makes the assumption that any imagery taken on the same day are frames that do not overlap the same areas. For Sentinel1 imagery, this holds true; for other data sources, it may not. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>Get the paths for the files to be merged.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_full_amp  = f\"{ingram_folder}/*/*_amp.tif\"\n",
    "paths_full_corr = f\"{ingram_folder}/*/*_corr.tif\"\n",
    "paths_full_unw  = f\"{ingram_folder}/*/*_unw_phase.tif\"\n",
    "amp_full_paths  = get_tiff_paths(paths_full_amp)\n",
    "corr_full_paths = get_tiff_paths(paths_full_corr)\n",
    "unw_full_paths  = get_tiff_paths(paths_full_unw)\n",
    "print_tiff_paths(amp_full_paths+corr_full_paths+unw_full_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>Write a function to get date info from filenames</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(paths):\n",
    "    dates = []\n",
    "    pths = glob.glob(paths)\n",
    "    for p in pths:\n",
    "        part = p.split(\"/\")[2]\n",
    "        date1 = part.split(\"_\")[0][0:8]\n",
    "        date2 = part.split(\"_\")[1][0:8]\n",
    "        dates.append([date1,date2])\n",
    "    dates.sort()\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a list containing each date pair</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = get_dates(paths_full_unw)\n",
    "for datepair in dates: print(datepair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a list of groups of paths to products which share acquisition dates</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_date_batches = []\n",
    "dup_dates = []\n",
    "for i in range(0,len(dates)-1):\n",
    "    dte = dates[i]\n",
    "    for j in range(i+1,len(dates)):\n",
    "        if dates[j] == dte:\n",
    "            dup_dates.append(dte)\n",
    "print(f\"dup_dates       : {dup_dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function using gdal_merge to merge products with duplicate dates</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dup_pairs(dups, paths):\n",
    "    for date_pair in dups:\n",
    "        matching = [s for s in paths if all(xs in s for xs in date_pair)]\n",
    "        if len(matching) == 2:\n",
    "            # create the output file name and merge the two files\n",
    "            # with matching sets of dates. \n",
    "            path,file = os.path.split(matching[0])\n",
    "            outputFile = os.path.join(path, 'MERGED'+file)\n",
    "            cmd = f\"gdal_merge.py -o {outputFile} {matching[0]} {matching[1]}\"\n",
    "            !{cmd}\n",
    "            \n",
    "            # The below code does some clean up. \n",
    "            # First, delete the unmerged original\n",
    "            os.remove(matching[0])\n",
    "            # Second, rename merged file to be that of the original file\n",
    "            os.rename(outputFile,matching[0])\n",
    "            # Third, remove matching[1] from the list of paths. \n",
    "            paths.remove(matching[1])\n",
    "        else:\n",
    "            print(f\"Error: there is not a pair of matching entries.\")\n",
    "            print(f\"Number of matches: {len(matching)}\")\n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Call merge_dup_pairs to merge amp, unw, and corr products with duplicated dates</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_dup_pairs(dup_dates, amp_full_paths)\n",
    "merge_dup_pairs(dup_dates, unw_full_paths)\n",
    "merge_dup_pairs(dup_dates, corr_full_paths)\n",
    "tiff_paths = amp_full_paths + unw_full_paths + corr_full_paths\n",
    "print_tiff_paths(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='5'> <b> 3. Identify Area of Interest</b> </font>\n",
    "    <br>\n",
    "    <font size='3'> Here we identify our area of interest (AOI). Our AOI must contain all of the expected deformation and a surrounding region of little to no deformation. Following our selection of this region, we will subset our data to this region. This helps reduce computation time. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amp_tiff_paths = get_tiff_paths(paths_full_amp)\n",
    "print_tiff_paths(amp_tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> <b>Create a string containing paths to one image for each area represented in the stack:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = {}\n",
    "for pth in amp_tiff_paths:\n",
    "    info = (gdal.Info(pth, options = ['-json']))\n",
    "    info = (json.loads(info))['wgs84Extent']['coordinates']\n",
    "    \n",
    "    coords = [info[0][0], info[0][3]]\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 2):\n",
    "            coords[i][j] = round(coords[i][j])\n",
    "    str_coords = f\"{str(coords[0])}{str(coords[1])}\"\n",
    "    if str_coords not in to_merge:\n",
    "        to_merge.update({str_coords: pth})\n",
    "merge_paths = \"\"\n",
    "for pth in to_merge:\n",
    "    merge_paths = f\"{merge_paths} {to_merge[pth]}\"\n",
    "    \n",
    "print(merge_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Merge the images for display in the Area-Of-Interest selector:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_scene = f\"{home}/full_scene.tif\"\n",
    "if os.path.exists(full_scene):\n",
    "    os.remove(full_scene)\n",
    "gdal_command = f\"gdal_merge.py -o {full_scene} {merge_paths}\"\n",
    "!{gdal_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a Virtual Raster Stack:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = f\"{home}/raster_stack.vrt\"\n",
    "!gdalbuildvrt -separate $image_file -overwrite $full_scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Convert the VRT into an array:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = gdal.Open(image_file)\n",
    "rasterstack = img.ReadAsArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Print the number of bands, pixels, and lines:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.RasterCount) # Number of Bands\n",
    "print(img.RasterXSize) # Number of Pixels\n",
    "print(img.RasterYSize) # Number of Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write an AOI selector class:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AOI_Selector:\n",
    "    def __init__(self, \n",
    "                 image,\n",
    "                 fig_xsize=None, fig_ysize=None,\n",
    "                 cmap=plt.cm.gist_gray,\n",
    "                 vmin=None, vmax=None\n",
    "                ):\n",
    "        display(Markdown(f\"<text style=color:blue><b>Area of Interest Selector Tips:\\n</b></text>\"))\n",
    "        display(Markdown(f'<text style=color:blue>- This plot uses \"matplotlib notebook\", whereas the other plots in this notebook use \"matplotlib inline\".</text>'))\n",
    "        display(Markdown(f'<text style=color:blue>-  If you run this cell out of sequence and the plot is not interactive, rerun the \"%matplotlib notebook\" code cell.</text>'))\n",
    "        display(Markdown(f'<text style=color:blue>- Use the pan tool to pan with the left mouse button.</text>'))\n",
    "        display(Markdown(f'<text style=color:blue>- Use the pan tool to zoom with the right mouse button.</text>'))\n",
    "        display(Markdown(f'<text style=color:blue>- You can also zoom with a selection box using the zoom to rectangle tool.</text>'))\n",
    "        display(Markdown(f'<text style=color:blue>- To turn off the pan or zoom to rectangle tool so you can select an AOI, click the selected tool button again.</text>'))\n",
    "        \n",
    "        display(Markdown(f'<text style=color:red><b>IMPORTANT!</b></text>'))\n",
    "        display(Markdown(f'<text style=color:red>- Upon loading the AOI selector, the selection tool is already active.</text>'))\n",
    "        display(Markdown(f'<text style=color:red>- Click, drag, and release the left mouse button to select an area.</text>'))\n",
    "        display(Markdown(f'<text style=color:red>- The square tool icon in the menu is <b>NOT</b> the selection tool. It is the zoom tool.</text>'))\n",
    "        display(Markdown(f'<text style=color:red>- If you select any tool, you must toggle it off before you can select an AOI</text>'))\n",
    "        \n",
    "        self.image = image\n",
    "        self.x1 = None\n",
    "        self.y1 = None\n",
    "        self.x2 = None\n",
    "        self.y2 = None\n",
    "        if not vmin:\n",
    "            self.vmin = np.nanpercentile(self.image, 1)\n",
    "        else:\n",
    "            self.vmin = vmin\n",
    "        if not vmax:\n",
    "            self.vmax=np.nanpercentile(self.image, 99)\n",
    "        else:\n",
    "            self.vmax = vmax\n",
    "        if fig_xsize and fig_ysize:\n",
    "            self.fig, self.current_ax = plt.subplots(figsize=(fig_xsize, fig_ysize)) \n",
    "        else:\n",
    "            self.fig, self.current_ax = plt.subplots() \n",
    "        self.fig.suptitle('Area-Of-Interest Selector', fontsize=16)\n",
    "        self.current_ax.imshow(self.image, cmap=plt.cm.gist_gray, vmin=self.vmin, vmax=self.vmax)\n",
    "\n",
    "\n",
    "        def toggle_selector(self, event):\n",
    "            print(' Key pressed.')\n",
    "            if event.key in ['Q', 'q'] and toggle_selector.RS.active:\n",
    "                print(' RectangleSelector deactivated.')\n",
    "                toggle_selector.RS.set_active(False)\n",
    "            if event.key in ['A', 'a'] and not toggle_selector.RS.active:\n",
    "                print(' RectangleSelector activated.')\n",
    "                toggle_selector.RS.set_active(True)\n",
    "                \n",
    "        toggle_selector.RS = RectangleSelector(self.current_ax, self.line_select_callback,\n",
    "                                               drawtype='box', useblit=True,\n",
    "                                               button=[1, 3],  # don't use middle button\n",
    "                                               minspanx=5, minspany=5,\n",
    "                                               spancoords='pixels',\n",
    "                                               rectprops = dict(facecolor='red', edgecolor = 'yellow', \n",
    "                                                                alpha=0.3, fill=True),\n",
    "                                               interactive=True)\n",
    "        plt.connect('key_press_event', toggle_selector)\n",
    "\n",
    "    def line_select_callback(self, eclick, erelease):\n",
    "        'eclick and erelease are the press and release events'\n",
    "        self.x1, self.y1 = eclick.xdata, eclick.ydata\n",
    "        self.x2, self.y2 = erelease.xdata, erelease.ydata\n",
    "        print(\"(%3.2f, %3.2f) --> (%3.2f, %3.2f)\" % (self.x1, self.y1, self.x2, self.y2))\n",
    "        print(\" The button you used were: %s %s\" % (eclick.button, erelease.button))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create an AOI selector from your raster stack:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_xsize = 7.5\n",
    "fig_ysize = 7.5\n",
    "aoi = AOI_Selector(rasterstack, fig_xsize, fig_ysize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Gather and define projection details:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotrans = img.GetGeoTransform()\n",
    "projlatlon = pyproj.Proj('EPSG:4326') # WGS84\n",
    "projimg = pyproj.Proj(f'EPSG:{utm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to convert the pixel, line coordinates from the AOI selector into geographic coordinates in the stack's EPSG projection:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geolocation(x, y, geotrans,latlon=True):\n",
    "    ref_x = geotrans[0]+x*geotrans[1]\n",
    "    ref_y = geotrans[3]+y*geotrans[5]\n",
    "    if latlon:\n",
    "        ref_y, ref_x = pyproj.transform(projimg, projlatlon, ref_x, ref_y)\n",
    "    return [ref_x, ref_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Call geolocation to gather the aoi_coords:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aoi.x2 - aoi.x1)\n",
    "print(aoi.y2 - aoi.y1)\n",
    "\n",
    "try:\n",
    "    aoi_coords = [geolocation(aoi.x1, aoi.y1, geotrans, latlon=False), geolocation(aoi.x2, aoi.y2, geotrans, latlon=False)]\n",
    "    print(f\"aoi_coords in EPSG {utm}: {aoi_coords}\")\n",
    "except TypeError:\n",
    "    print('TypeError')\n",
    "    display(Markdown(f'<text style=color:red>This error occurs if an AOI was not selected.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that the square tool icon in the AOI selector menu is <b>NOT</b> the selection tool. It is the zoom tool.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Read the tips above the AOI selector carefully.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='5'><b> 4. Subset (Crop) Data to Area of Interest </b> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'>We now subset our data to our AOI. We must do this for both the interferograms and the coherence files. In this lab, we will also subset the amplitude image files for later display purposes, though this is not necessary.</font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"><b>Create a subset folder</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_subset:\n",
    "    try:\n",
    "        # Try to remove the folder tree to ensure no other data exists in the folder. \n",
    "        shutil.rmtree(subset_folder)\n",
    "    except: \n",
    "        pass\n",
    "# Make the directory\n",
    "!mkdir -p {subset_folder} # create the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font face=\"Calibri\" size=\"3\"><b>Crop the interferograms</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = amp_full_paths + unw_full_paths + corr_full_paths\n",
    "# Loop through each interferogram in the in list of 'tiff_paths'. \n",
    "print(\"Subsetting amplitude, coherence, and interferogram files.\")\n",
    "for tiff_path in tiff_paths:\n",
    "    path,tiff = os.path.split(tiff_path)\n",
    "\n",
    "    gdal_command = (f\"gdal_translate -epo -eco -projwin {aoi_coords[0][0]} \"\n",
    "                    f\"{aoi_coords[0][1]} {aoi_coords[1][0]} {aoi_coords[1][1]} \"\n",
    "                    f\"-projwin_srs 'EPSG:{utm}' -co \\\"COMPRESS=DEFLATE\\\" \"\n",
    "                    f\"-a_nodata 0 {tiff_path} {subset_folder}/{tiff} > /dev/null\")\n",
    "    # Uncomment line below to see exactly what gdal command in being called\n",
    "    # print(f\"\\nCalling the command: {gdal_command}\") \n",
    "    !{gdal_command} # Call the GDAL command. \n",
    "print(\"Subsetting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'><font face='Calibri'><font size='3'>It is possible GDAL returned an error for one or more of the interferogram, coherence, and amplitude files. This is because of our use of the \"-epo\" and \"-eco\" options. Those rasters which return an error are either partially or completely outside of our AOI. Including these empty files would cause errors in TRAIN and GIAnT. It is possible to include those files that only have partial extent within our AOI, but would require significantly more advanced processing which we exclude for simplicity. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>4.2 Check that the Subsetted Tiffs have Pixels</b></font>\n",
    "<br>\n",
    "<font size='3'>Some of the subsetted geotiffs do not have pixels in our AOI despite the \"-epo\" and \"-eco\" options which should cause an error for all of these and skip them. Below, we will <b>check which geotiffs actually have pixels in our AOI and remove those that don't.</b> This can be done with a simple NaN search or by checking the band statistics of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_subsets = f\"{subset_folder}/*.tif\"\n",
    "subset_paths = get_tiff_paths(paths_subsets)\n",
    "remove_nan_filled_tifs(subset_folder,subset_paths) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>4.3 Check the Dimensions of the Subsetted Tiffs</b> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'>In some instances, the <font face='Courier New'><b>gdal_translate</b></font> function will return subsetted imagery with slightly different extents; for example, one subset may be 1000 x 1000 pixels while another is 1001 x 1000. This is usually more of a problem when different different data sensors are used as these sensors will often have different pixel sizes and/or their pixel locations will be slightly offset from each other. Since all of our data comes from Sentinel1, this is generally not a problem, but it is still good to double check.</font>\n",
    "<br><br>\n",
    "<font face='Calibri' size='3'>Loop through each of the files, find all of the unique dimensions, and compare them. We will do this by defining and calling a function named <font face='Courier New'>pixel_check</font>. We have chosen to define this as a function as we will use it to check the geotiff dimensions later in this lab and it is good programming practice to create a function for any task that is done multiple times.</font>\n",
    "<font face='Calibri'><font size='3'>Identify all of the '.tif' files, and put their paths and names in the list 'tiff_paths'.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find files in directory 'subset_folder' and add \n",
    "# them to list 'files' if they end with '.tiff'\n",
    "subset_paths = get_tiff_paths(paths_subsets)\n",
    "subset_paths.sort()\n",
    "print(f\"Number of '.tif' files: {len(subset_paths)}\")\n",
    "for path in subset_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Identify all of the '.tif' files, and put their paths and names in the list 'tiff_paths'.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_check(tiff_paths):\n",
    "    # get the pixel and line size for each tiff file \n",
    "    # and add it to lists 'Pixels' and 'Lines'\n",
    "    Pixels,Lines = [],[]\n",
    "    for file in tiff_paths:\n",
    "        im = gdal.Open(file)\n",
    "        raster = im.GetRasterBand(1).ReadAsArray()\n",
    "        XSize, YSize = im.RasterXSize, im.RasterYSize\n",
    "        Pixels.append(XSize)\n",
    "        Lines.append(YSize)\n",
    "        \n",
    "    # Get unique values of 'Pixels' and 'Lines'\n",
    "    Pixel_set, Line_set = set(Pixels), set(Lines)\n",
    "    # Check the number of unique values. \n",
    "    if len(Pixel_set) >1 or len(Line_set) > 1:\n",
    "        print(f\"Problem: More than 1 pixel or line value. This indicates \"\n",
    "              f\"two or more subsetted '.tif' files have different sizes.\")\n",
    "        print(f\"Number of unique Pixel Counts: {len(Pixel_set)}\")\n",
    "        print(f\"Number of unique Line Counts:  {len(Line_set)}\")\n",
    "    else: \n",
    "        print(\"All subsetted '.tif' files are the same size. Hurray!\")\n",
    "        print(f\"Pixels, Lines = {Pixels[0]}, {Lines[0]}\")\n",
    "    return Pixels,Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pixels, Lines = pixel_check(subset_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'> If the '.tif' files are different sizes, use GDAL to make them uniform. We will do this by defining and calling a function named <font face='Courier New'>pixel_correction</font>.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_correction(tiff_files, Pixels, Lines,coords):\n",
    "    # Find the index of the smallest of the '.tif' files. \n",
    "    idx_Pixel = np.argmin(Pixels)\n",
    "    idx_Lines = np.argmin(Lines)\n",
    "    idx = max([idx_Pixel, idx_Lines]) # 'idx' is 0 if all files are the same size. \n",
    "    \n",
    "    # Clip the other files according to that smallest '.tif' file. \n",
    "    if idx > 0:\n",
    "        PSize, LSize = Pixels[idx], Lines[idx] # Pixel and Line size for all rasters. \n",
    "        for file in tiff_files:\n",
    "            if file is not tiff_files[idx]:\n",
    "                cmd = (f\"gdal_translate -of GTIFF -projwin {coords[0][0]} \"\n",
    "                       f\"{coords[0][1]} {coords[1][0]} {coords[1][1]} {file} {file}\")\n",
    "                try: \n",
    "                    print(f\"Correcting file: {file}\")\n",
    "                    !{cmd}\n",
    "                except:\n",
    "                    print(f\"Drat, GDAL failed to correct this file: {file}!\")\n",
    "    else: \n",
    "        print(\"Nothing to do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_correction(subset_paths, Pixels, Lines,coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size='5'> <b> 5. Create Input Files and Code for TRAIN </b> </font>\n",
    "<br>\n",
    "<font size='3'> In this section, we will use TRAIN to remove static atmospheric effects that can cause decoherence of the interferograms. This primarily corrects for effects caused by elevation differences between different locations. <br>If we think of the propogating radar wave as a set of discrete rays, each ray will follow a different path. Those that reflect from elevated locations will pass through less of the atmosphere and therefore be less altered than those rays that reflect from locations at lower elevations. Without this correction, interferograms often produce exaggerated deformation. This is especially important in hazard monitoring of active volcanoes as a false alarm can be extremely costly and cause the general public to ignore future warnings. \n",
    "</font>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'>Let's create the input files and modify the files as required by TRAIN. The necessary items and actions are listed below. <br>\n",
    "        \n",
    "- parms_aps.txt\n",
    "    - List of parameters defining how TRAIN will run.\n",
    "- ifgday.txt\n",
    "    - Text file listing the master and slave date pairs. \n",
    "    - 2 column vector [master slave]\n",
    "    - Format: YYYYMMDD\n",
    "- Convert subsetted '.tif' Files to GCS Coordinates\n",
    "    - TRAIN requires the input '.tif' files to have a global coordinate system. \n",
    "    - We will convert them to EPSG:4326. \n",
    "- Adjust File Names\n",
    "    - Many SAR codes expect the input files to have a particular name format. \n",
    "    - For TRAIN, this is <font face='Courier New'>&lt;master\\_date&gt;\\_&lt;slave\\_date&gt;\\_&lt;unwrapped, amplitude, or coherence designation&gt;.tif</font>.\n",
    "<br></font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'> <b>5.1 Create parms_aps.txt file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>We will need to create the file 'parms_aps.txt'. TRAIN will read parameters from this file. In order to do this, we will first need to extract some information from the satellite metadata files. We'll start with getting the UTC time of the satellite pass over our study area. <br>\n",
    "        \n",
    "</font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>5.1.1 Extract the UTC Time</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>This information is contained in the file name. Alternatively, it can be found in the metadata file (titled <font face='Courier New'>$<$master timestamp$>$_$<$slave timestamp$>$.txt</font>) that comes with the interferogram. The function <font face='Courier New'>getUTC_sat</font> extracts and returns the median UTC time in HR:MIN format and as an integer representing the number of seconds since the start of the day.</font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract UTC time of satellite pass\n",
    "def getUTC_sat(files):\n",
    "    UTC_hr,UTC_min,UTC_sec = [],[],[]\n",
    "    for file in files:\n",
    "        vals = file.split('_')\n",
    "        tstamp = vals[0][9:16]\n",
    "        UTC_hr.append(int(tstamp[0:2]))\n",
    "        UTC_min.append(int(tstamp[2:4]))\n",
    "        UTC_sec.append(int(tstamp[4:6]))    \n",
    "    \n",
    "    # UTC time as HH:MIN; we extract the median value and pad with up to 2 zeroes. \n",
    "    ###############################UTC_sat = str(int(np.median(UTC_hr))).zfill(2)+':'+str(int(np.median(UTC_min))).zfill(2)\n",
    "    UTC_sat = (f\"{str(int(np.median(UTC_hr))).zfill(2)}:\"\n",
    "               f\"{str(int(np.median(UTC_min))).zfill(2)}\")\n",
    "    # UTC time as an integer; Method from Tom Logan's prepGIAnT code\n",
    "    # Can also be found inside <date>_<date>.txt file and hard coded/extracted\n",
    "    c_l_utc = np.median(UTC_hr)*3600 + np.median(UTC_min)*60 + np.median(UTC_sec) \n",
    "    return UTC_sat, c_l_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>Create a list of unwrapped, phase corrected tiffs.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(subset_folder) if f.endswith('_unw_phase.tif')] \n",
    "for file in files: print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>Find the UTC time of the satellite pass for each \\_unw\\_phase.tif.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTC_sat, c_l_utc = getUTC_sat(files)\n",
    "print(c_l_utc)\n",
    "print(UTC_sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>5.1.2 Create parms_aps.txt file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>Make the parms_aps.txt file. This gives TRAIN information on how to process the data. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inProj = pyproj.Proj(f'EPSG:{utm}')\n",
    "outProj = pyproj.Proj('EPSG:4326')\n",
    "llx, lly = pyproj.transform(inProj,outProj,coords[0][0],coords[0][1])\n",
    "urx, ury = pyproj.transform(inProj,outProj,coords[1][0],coords[1][1])\n",
    "region_lats = abs(np.diff([lly,ury])[0]) + extra\n",
    "region_lons = abs(np.diff([llx,urx])[0]) + extra\n",
    "\n",
    "!mkdir -p {merra2_datapath} # create the directory\n",
    "\n",
    "parms_aps_Template = '''\n",
    "# Input parameters for TRAIN\n",
    "\n",
    "crop_flag: n\n",
    "date_origin: file\n",
    "dem_null: -32768\n",
    "DEM_origin: asf\n",
    "DEM_file: {7}\n",
    "era_data_type: {1}\n",
    "ifgday_file: {2}\n",
    "incidence_angle: {9}\n",
    "lambda: {8}\n",
    "look_angle: 21\n",
    "meric_perc_coverage: 80\n",
    "merra2_datapath: {4}\n",
    "non_defo_flag: n\n",
    "region_lat_range: {5}\n",
    "region_lon_range: {6}\n",
    "region_res: 0.008333000000000\n",
    "save_folder_name: aps_estimation\n",
    "small_baseline_flag: n\n",
    "stamps_processed: n\n",
    "UTC_sat: {3}\n",
    "\n",
    "'''\n",
    "\n",
    "with open(os.path.join(trainDir,'parms_aps.txt'), 'w') as fid:\n",
    "    fid.write(parms_aps_Template.format(trainDir, era_data_type,\n",
    "                                        ifgday_file, UTC_sat,\n",
    "                                        merra2_datapath, region_lats,\n",
    "                                        region_lons, demPathNName,\n",
    "                                        lambdaInMeters,incidenceAngle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>You may notice that the <font face='Courier New'>parms_aps.txt</font> file we created has very little geographic information; we've only included the width and height of the subsetted interferograms in decimal degrees (the variables <font face='Courier New'>region_lats</font> and <font face='Courier New'>region_lons</font>). This is because we will use one of our subsetted and converted tiffs as a geographic reference file. Otherwise, we would need to create a text file that contains the latitude and longitude of each pixel in the interferogram.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>5.2 Create ifgday.mat file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>Make the ifgday.mat file. This gives TRAIN the master and slave dates of the interferograms as 2 column vectors. The dates must be separated by a single space.<br>\n",
    "        \n",
    "- Interferogram dates stored as a matrix with name ifgday and size [n_ifgs 2]. \n",
    "- Master image is in the first and slave in the second column. \n",
    "- Specify dates as a numeric value in YYYYMMDD format.\n",
    "\n",
    "    </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(subset_folder) if \\\n",
    "         f.endswith('_unw_phase.tif')] # Get file names. \n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the master and slave dates. \n",
    "masterDates,slaveDates = [],[]\n",
    "for file in files:\n",
    "    tstamps = file.split('_')\n",
    "    masterDates.append(tstamps[0][0:8])\n",
    "    slaveDates.append(tstamps[1][0:8])\n",
    "# Sort the dates according to the master dates. \n",
    "mDates,sDates = (list(t) for t in zip(*sorted(zip(masterDates,slaveDates))))\n",
    "\n",
    "with open( os.path.join(trainDir, ifgday_file), 'w') as fid:\n",
    "    for i in range(len(mDates)):\n",
    "        masterDate = mDates[i] # pull out master Date (first set of numbers)\n",
    "        slaveDate = sDates[i] # pull out slave Date (second set of numbers)\n",
    "        \n",
    "        # write values to the 'ifgday_file'; make sure there is only 1 space between the dates. \n",
    "        fid.write(f'{masterDate} {slaveDate}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the contents of 'ifgday_file'\n",
    "ifg = open(os.path.join(trainDir,ifgday_file),'r') # open the file.\n",
    "ifg_contents = ifg.read()                        # read the contents.\n",
    "print(ifg_contents)                              # print the contents. \n",
    "ifg.close()                                      # close the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>5.3 Convert Subsetted '.tif' Files to GCS Coordinates</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>The ASF version of TRAIN requries our subsets to be in GCS coordinates (i.e., for pixel every location in the subset to be designated by latitude and longitude). Currently, they are in a UTM coordinate system, which gives pixel location in meters based on a local coordinate system. We will convert our subsets. <br>We've already extracted and stored the original coordinate system in the variable <font face='Courier New'>utm</font>. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current Coordinate System - EPSG:{utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>We convert the coordinate system of each subsetted '.tif' to GCS coordinates using GDAL's warp function. This is typically designated as EPSG:4326. </font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set desired TRAIN coordinate system; this will be used later\n",
    "coord_TRAIN = '4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>Create a directory for converted files. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_corrected:\n",
    "    try:\n",
    "        shutil.rmtree(corrected_folder)\n",
    "    except: \n",
    "        pass\n",
    "# Make the directory\n",
    "!mkdir -p {corrected_folder} # create the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'>Create the converted subsets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # Designate the input file and its path\n",
    "    inFile = os.path.join(subset_folder,file)\n",
    "    # Designate the output file and its path; ideally these are the same. \n",
    "    # GDAL can't do that (it'll overwrite data sometimes), \n",
    "    # so we're going to create entirely new files in a new folder. \n",
    "    outFile = os.path.join(corrected_folder,file)\n",
    "    cmd = f\"gdalwarp -t_srs EPSG:{coord_TRAIN} {inFile} {outFile}\"\n",
    "    #print(cmd)\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Now that we have created our new files in the <font face='Courier New'>corrected_folder</font> directory, the original subsets in the <font face='Courier New'>subset_folder</font> is superfluous and could be removed. <b>This is an optional step.</b> We will keep the original files for the purpose of comparing the corrected and uncorrected time series in Part 2. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_subsets:\n",
    "    shutil.rmtree(subset_folder)\n",
    "    print(f\"Folder {subset_folder} removed.\")\n",
    "else:\n",
    "    print(\"This step has been skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>5.4 Adjust File Names</b></font>\n",
    "    <font size='3'><br>Many SAR codes expect the input files to have a particular name format. For TRAIN, this is <font face='Courier New'>&lt;master_date&gt;_&lt;slave_date&gt;_&lt;unwrapped, amplitude, or coherence designation&gt;.tif</font>. We will adjust the files to match this name convention. The code below assumes that the files all come from Sentinel-1 and that every interferogram has a unique master and slave date pair. <br><b>This is not always true; some interferograms will have identical master/slave date pairs, but have been taken at different times.</b> This is a relatively rare occurrence, but it is good to keep in mind. To keep this exercise relatively simple, we assume each interferogram has a unique master/slave date pair.</font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameFiles(datadirectory,files):\n",
    "    # Rename the interferogram, coherence, and amplitude (for plotting later) files.  \n",
    "    for file in files:\n",
    "        if \"T\" in file: # only change files needing to be renamed (containing a 'T') \n",
    "            oldname, oldExt = os.path.splitext(file)\n",
    "            # Uncomment print statement below to see parsed file name and extension. \n",
    "            #print(f\"\\nCurrent Name: {oldname}\\nCurrent Extension: {oldExt}\") \n",
    "            tstamps = oldname.split('_')\n",
    "            master,slave = tstamps[0][0:8],tstamps[1][0:8]\n",
    "            if \"_unw\" in file:\n",
    "                newname = master + '_' + slave + '_unw_phase' + oldExt\n",
    "            elif \"_corr\" in file:\n",
    "                newname = master + '_' + slave + '_corr' + oldExt\n",
    "            elif \"_amp\" in file:\n",
    "                newname = master + '_' + slave + '_amp' + oldExt\n",
    "            exists = os.path.isfile(os.path.join(datadirectory,newname))\n",
    "            if exists:\n",
    "                print(\"This one already exists: \"+newname)\n",
    "            else:\n",
    "                os.rename(os.path.join(datadirectory, file),\n",
    "                          os.path.join(datadirectory, newname))\n",
    "    print(\"Files renamed.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>We will rename both the GCS converted files and the original subsets. First, the original subsets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fExt = '.tif'\n",
    "files = [f for f in os.listdir(subset_folder) if f.endswith(fExt)] \n",
    "files.sort()\n",
    "print(len(files))\n",
    "\n",
    "# print every entry of list 'files' separated by the newline character, \"\\n\"\n",
    "print(*files, sep=\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renameFiles(subset_folder, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Now, the GCS converted files. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fExt = '.tif'\n",
    "files = [f for f in os.listdir(corrected_folder) if f.endswith(fExt)] \n",
    "files.sort()\n",
    "print(len(files))\n",
    "\n",
    "# print every entry of list 'files' separated by the newline character, \"\\n\"\n",
    "print(*files, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renameFiles(corrected_folder,files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Now that we've renamed our files, we get a list of the converted files. We later use on of these with TRAIN as our geographic reference file. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fExt = '.tif'\n",
    "files_converted = [f for f in os.listdir(corrected_folder)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>6. Run TRAIN</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We have now created all of the necessary files to run TRAIN, so let's do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>6.1 Minor Set Up</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We have to set up some path information in order to run TRAIN. <i>Eventually, this will be modified so we don't have to include the full path to TRAIN.</i> Additionally, we show multiple ways in which to call TRAIN.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General path to the TRAIN code. This is a temporary necessity.\n",
    "# In the future,the path to TRAIN will be unnecessary. \n",
    "train_path = \"/usr/local/TRAIN/src\" # only for the code that we will not modify. \n",
    "cmd = os.path.join(train_path,'aps_weather_model.py')\n",
    "print(cmd)\n",
    "georef_path = os.path.join(home, corrected_folder, files_converted[0])\n",
    "print(f\"Path to Georeference File: {georef_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some help information\n",
    "!python2.7 $train_path/aps_weather_model.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the same help information via a slightly different call method\n",
    "!python2.7 $cmd -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>6.2 Steps 0-3</b></font>\n",
    "    <br>\n",
    "    <font size='3'>Now we run steps 0 through 3. Step 4 requires a few extra actions. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "# Step 0 - Identify weather data files to download. \n",
    "!python2.7 $train_path/aps_weather_model.py -g {georef_path} 0 0\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any existing MERRA2 downloads. \n",
    "# This is to prevent unrelated MERRA2 files from being used. \n",
    "try:\n",
    "    shutil.rmtree(merra2_datapath)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "# Step 1 - Download weather data. \n",
    "# This will download a series of '*.nc4' files for each master. \n",
    "!python2.7 $train_path/aps_weather_model.py -g {georef_path} 1 1\n",
    "\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "\n",
    "# Step 2 - Calculate wet and hydrostatic zenith delays. \n",
    "# This will download a set of '*.xyz' files and use those \n",
    "# to calculate the necessary delays. \n",
    "!python2.7 $train_path/aps_weather_model.py -g {georef_path} 2 2\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "\n",
    "# Step 3 - Calculate the SAR delays.\n",
    "# This produces *_*_{hydro_correction, wet_correction, and correction}.bin files\n",
    "!python2.7 $train_path/aps_weather_model.py -g {georef_path} 3 3\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Step 3 may give a strange message: \"No correction for &lt;insert date&gt;\". If this occurs, it is most likely because TRAIN wasn't able to access the MERRA2 files due to missing permissions in your Earthdata user account. This can be done by going to your <a href=\"https://urs.earthdata.nasa.gov/profile\" target=\"_blank\">EarthData's profile page</a>, clicking <b>Applications</b> and selecting <b>Approved Applications</b> from the drop down menu. Select <b>Approve More Applications</b> at the bottom left, search for <b>NASA GESDISC DATA ARCHIVE</b>, select it, and agree to the terms and conditions. Once that is complete, restart from Step 0. <br><br>Note that you may have to correct the current working directory to the folder in which this notebook resides. </font></font>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>6.3 Step 4 of TRAIN</b></font>\n",
    "    <br>\n",
    "    <font size='3'>In step 4, we apply the correction to our <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase.tif&gt;</font> files. For this step, we need to do 2 things first: \n",
    "1. Move all of the '*.bin' files into the same directory as our converted geotiffs. \n",
    "2. Make our current working directy the same as our converted geotiffs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find files. \n",
    "\n",
    "\n",
    "print(os.listdir())\n",
    "\n",
    "fExt = '.bin'\n",
    "files = [f for f in os.listdir('.') if f.endswith(fExt)] \n",
    "files.sort()\n",
    "print(len(files))\n",
    "\n",
    "# print every entry of list 'files' separated by the newline character, \"\\n\"\n",
    "print(*files, sep=\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "# Move the files into the desired location\n",
    "for file in files:\n",
    "    shutil.move(file,os.path.join(corrected_folder,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change working directory\n",
    "os.chdir(corrected_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Subtract calculated delay from interferograms. \n",
    "!python2.7 $train_path/aps_weather_model.py -g {georef_path} 4 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return to the home directory\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>If we check our <font face='Courier New'>corrected_folder</font> directory, we will find new files with the naming convention <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase\\_corrected.tif&gt;</font>. The uncorrected files, <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase.tif&gt;</font>, are now technically superfluous and can be deleted. However, we will keep these files for the purpose of comparing the corrected and uncorrected times series in Part 2.\n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>6.4 Comparison of Corrected and Uncorrected Unwrapped Phase</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We will make a quick and simple comparison between the corrected and uncorrected unwrapped phase geotiffs. This is meant to highlight the importance of these atmospheric corrections.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_cor = f\"{home}/{corrected_folder}/*_unw_phase_corrected.tif\"\n",
    "paths_unc = f\"{home}/{corrected_folder}/*_unw_phase.tif\"\n",
    "cor_paths = get_tiff_paths(paths_cor)\n",
    "unc_paths = get_tiff_paths(paths_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cor_paths[0])\n",
    "print(unc_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = gdal.Open(cor_paths[0])\n",
    "uncorrected = gdal.Open(unc_paths[0])\n",
    "im_c = corrected.GetRasterBand(1).ReadAsArray()\n",
    "im_u = uncorrected.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "fig = plt.figure(figsize=(18,10))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax1.imshow(im_c,cmap='gray')\n",
    "ax2.imshow(im_u,cmap='gray')\n",
    "plt.title('Sierra Negra - Corrected and Uncorrected Unwrapped Phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "difference = np.subtract(im_c,im_u)\n",
    "fig = plt.figure(figsize=(18,10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "fig_plot = ax1.imshow(difference,cmap='RdBu')\n",
    "fig.colorbar(fig_plot, fraction=0.24, pad=0.02)\n",
    "ax1.set(title='Sierra Negra - TRAIN Correction Difference Map [mm]')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>In looking at the Correction Difference Map, we can see that size of the correction correlates with surface elevation. Without this correction, a volcanologist would see the extra elevation difference as an indication of magma injection and possible eruptive activity.</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>6.5 Convert back to original coordinate system</b></font>\n",
    "    <br>\n",
    "    <font size='3'>GIAnT requires the interferograms to be in a particular coordinate system. The original coordinate system is one of those accepted, so we will convert our interferograms back to that. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coord_TRAIN = '4326'\n",
    "#utm = '32715' # the original coordinate system identified when reprojecting the interferograms to the same UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"original coordiante system = EPSG:{utm}\")\n",
    "print(f\"TRAIN coordinate system =    EPSG:{coord_TRAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = f\"{corrected_folder}/*.tif\"\n",
    "tiff_paths = get_tiff_paths(paths)\n",
    "print(tiff_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check that the current coordinate system of the \n",
    "# files is different from the desired. \n",
    "utm_zones, utm_types = getUTM_znt(tiff_paths)\n",
    "print(f\"Current UTM Types & Zones = EPSG:{list(set(utm_zones))}\")\n",
    "print(f\"Expected current system   = EPSG:{coord_TRAIN}\")\n",
    "print(f\"Desired coordinate system = EPSG:{utm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create the converted subsets. \n",
    "for file in tiff_paths:\n",
    "    # Designate the input file and its path\n",
    "    inFile = file\n",
    "    # Designate the output file and its path; ideally these are the same. \n",
    "    # GDAL can't do that (it'll overwrite data sometimes), so we're going \n",
    "    # to create entirely new files in a new folder, delete the old files, \n",
    "    # and then rename the newly created file. \n",
    "    path,fl= os.path.split(file)\n",
    "    desig = 'TEMP_'\n",
    "    new_name = desig+fl\n",
    "    outFile = os.path.join(path,new_name)\n",
    "    # create the convert command\n",
    "    cmd = f\"gdalwarp -t_srs EPSG:{utm} {inFile} {outFile}\"\n",
    "    #print(cmd)\n",
    "    !{cmd} # convert the file\n",
    "    # delete the file in the EPSG:4326 coordinate system\n",
    "    try:\n",
    "        os.remove(inFile)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # rename the file in the utm coordinate system to our original name of 'inFile'. \n",
    "    try:\n",
    "        os.rename(outFile,inFile)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the coordinate conversion worked. \n",
    "utm_zones, utm_types = getUTM_znt(tiff_paths)\n",
    "print(f\"Current UTM Types & Zones = EPSG:{list(set(utm_zones))}\")\n",
    "print(f\"Expected current system   = EPSG:{utm}\")\n",
    "print(f\"Desired coordinate system = EPSG:{utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>6.6 Do another pixel check</b></font>\n",
    "    <br>\n",
    "    <font size='3'>Check the pixel sizes again, and then do the pixel correction if necessary.<br><br><i>This could be a student assignment</i>\n",
    "    </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tiff paths\n",
    "paths = f\"{corrected_folder}/*.tif\"\n",
    "tiff_paths = get_tiff_paths(paths)\n",
    "for tiff in tiff_paths:\n",
    "    print(tiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pixels, Lines = pixel_check(tiff_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_correction(tiff_paths,Pixels,Lines,coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>\n",
    "You have now corrected the interferograms for atmospheric conditions and can proceed to Part 2: GIAnT. \n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657 Microwave Remote Sensing - Version 1.0 - May 2020 </i>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
