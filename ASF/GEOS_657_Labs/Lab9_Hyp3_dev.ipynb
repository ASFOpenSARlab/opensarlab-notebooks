{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.jpg\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 9: InSAR Time Series Analysis using GIAnT within Jupyter Notebooks <font color='rgba(200,0,0,0.2)'> -- [## Points] </font> </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer & Joshua J C Knicely; University of Alaska Fairbanks</b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /><font color='rgba(200,0,0,0.2)'> <b>Due Date: </b>NONE</font>\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. The primary goal of this lab is to demonstrate how to process InSAR data, specifically interferograms, using the Generic InSAR Analysis Toolbox (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">GIAnT</a>) in the framework of *Jupyter Notebooks*.<br>\n",
    "\n",
    "<b>Our specific objectives for this lab are to:</b>\n",
    "\n",
    "- Learn how to download data from a Hyp subscription using the ASF tools. \n",
    "- Learn how to prepare data for GIAnT. \n",
    "- Use GIAnT to create maps of surface deformation. \n",
    "    -  Understand its capabilities. \n",
    "    -  Understand its limitations. \n",
    "</font>\n",
    "\n",
    "<br>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> Target Description </b> </font>\n",
    "\n",
    "<font size=\"3\"> In this lab, we will analyse the volcano Sierra Negra. This is a highly active volcano on the Galapagos hotpsot. The most recent eruption occurred from 29 June to 23 August 2018. The previous eruption occurred in October 2005, prior to the launch of the Sentinel-1 satellites, which will be the source of data we use for this lab. We will be looking at the deformation that occurred prior to the volcano's 2018 eruption. </font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>THIS NOTEBOOK INCLUDES NO HOMEWORK ASSIGNMENTS.</b></font> <br>\n",
    "\n",
    "Contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>Overview</b></font>\n",
    "<br>\n",
    "<font size='3'><b>About GIAnT</b>\n",
    "<br>\n",
    "GIAnT is a Python framework that allows rapid time series analysis of low amplitude deformation signals. It allows users to use multiple time series analysis technqiues: Small Baseline Subset (SBAS), New Small Baseline Subset (N-SBAS), and Multiscale InSAR Time-Series (MInTS). As a part of this, it includes the ability to correct for atmospheric delays by assuming a spatially uniform stratified atmosphere. \n",
    "<br><br>\n",
    "<b>Limitations</b>\n",
    "<br>\n",
    "GIAnT has a number of limitations that are important to keep in mind as these can affect its effectiveness for certain applications. It implements the simplest time-series inversion methods. Its single coherence threshold is very conservative in terms of pixel selection. It does not include any consistency checks for unwrapping errors. It has a limited dictionary of temporal model functions. It cannot correct for atmospheric effects due to differing surface elevations. \n",
    "<br><br>\n",
    "<b>Steps to use GIAnT</b><br>\n",
    "Although GIAnT is an incredibly powerful tool, it requires very specific input. Because of the input requirements, the bulk of the lab and code below is dedicated to getting our data into a form that GIAnT can manipulate and to creating files that tell GIAnT what to do. The general steps of this lab to use GIAnT are below. \n",
    "\n",
    "- Download Data\n",
    "- Identify Area of Interest\n",
    "- Subset (Crop) Data to Area of Interest\n",
    "- Prepare Data for GIAnT\n",
    "    - Adjust file names\n",
    "    - Remove potentially disruptive default values (optional)\n",
    "    - Convert data from '.tiff' to '.flt' format\n",
    "- Create Input Files for GIAnT\n",
    "    - Create 'ifg.list'\n",
    "    - Create 'date.mli.par'\n",
    "    - Make prepxml_SBAS.py\n",
    "    - Run prepxml_SBAS.py\n",
    "    - Make userfn.py\n",
    "- Run GIAnT\n",
    "    - PrepIgramStack.py\n",
    "    - ProcessStack.py\n",
    "    - SBASInvert.py\n",
    "    - SBASxval.py\n",
    "- Data Visualization\n",
    "\n",
    "<br><br>\n",
    "More information about GIAnT can be found here: (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">http://earthdef.caltech.edu/projects/giant/wiki</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>1. Import Python Libraries</b></font><br>\n",
    "    <font size='3'>Let's import the Python libraries we will need to run this lab. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal, os, osr, h5py, shutil, re, sys\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "from asf_hyp3 import API\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "from datetime import date\n",
    "import zipfile\n",
    "import datetime # for date\n",
    "import glob\n",
    "\n",
    "from asf_notebook import download_hyp3_products\n",
    "from asf_notebook import new_directory\n",
    "from asf_notebook import earthdata_login\n",
    "from asf_notebook import remove_nan_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 0. Download Data from Hyp3 Subscription </b> </font>\n",
    "\n",
    "<font size=\"3\"> We will begin by acquiring the interferograms of Sierra Negra from our Hyp3 subscription. <br> <i> THIS MAY NEED TO BE ALTERED. HOW WILL STUDENTS DL FROM HYP??? </i> <br> The first cell below acquires user credentials and creates a .netrc file in order to access the Hyp3 subscription. </font>\n",
    "<hr>\n",
    "<font face=\"Calibri\" size=\"3\"> To download data from ASF, you need to provide your <a href=\"https://www.asf.alaska.edu/get-data/get-started/free-earthdata-account/\" target=\"_blank\">NASA Earth Data</a> username to the system. Setup an EarthData account if you do not yet have one. <font color='rgba(200,0,0,0.2)'><b>Note that EarthData's ULA applies when accessing the Hyp3 API from this notebook. If you have not acknowleged the ULA in EarthData, you will need to navigate to EarthData's home page and complete that process.</b></font>\n",
    "<br><br>\n",
    "<b>Login to Earthdata:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = earthdata_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"> Before we download anything, let's <b>first create a working directory for this analysis and change into it:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/jovyan/notebooks/ASF/GEOS_657_Labs/lab_9_Hyp3_data\"\n",
    "new_directory(path)\n",
    "os.chdir(path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a folder in which to download your interferograms:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory(\"ingrams\")\n",
    "products_path = f\"{path}/ingrams\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Set a date range, flight direction, and path of products to download:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = [datetime.date(2017, 2, 1), datetime.date(2017, 10, 20)]\n",
    "\n",
    "########## NOTE: Currently filtering by path and flight_direction doesn't work for InSAR products #########\n",
    "\n",
    "\n",
    "# uncomment code below to download all products\n",
    "date_range = [None, None]\n",
    "direction = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Download the products:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = download_hyp3_products(\n",
    "    api, products_path)#, start_date=date_range[0], end_date=date_range[1], flight_direction=direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Grab the paths of the amplitude, interferogram, and coherence imagery:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amp_paths = !ls ingrams/*/*_amp.tif | sort\n",
    "print(f\"amp_paths[0]: {amp_paths[0]}\")\n",
    "ingram_paths = !ls ingrams/*/*_unw_phase.tif | sort\n",
    "print(f\"ingram_paths[0]: {ingram_paths[0]}\")\n",
    "cohr_paths = !ls ingrams/*/*_corr.tif | sort\n",
    "print(f\"cohr_paths[0]: {cohr_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Write a function to sort through the amplitude, interferogram, and coherence paths, removing those that occur either before or after the eruption:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return entries of '*_paths' that occur before or after a desired date. \n",
    "def sortDates(datesToSort, condition, eruptionDate):\n",
    "    bad = []\n",
    "    if condition.lower() in 'before':\n",
    "        # add to list 'bad' if either date is after the the eruptionDate (i.e., before our desired timeframe). \n",
    "        for i in range(len(datesToSort)):\n",
    "            mDate, sDate = datesToSort[i][13:21], datesToSort[i][29:37]\n",
    "            if (int(mDate) > eruptionDate) or (int(sDate) > eruptionDate):\n",
    "                bad.append(int(i))\n",
    "    elif condition.lower() in 'after':\n",
    "        # add to list 'bad' if either date is before the the eruptionDate (i.e., after our desired timeframe). \n",
    "        for i in range(len(datesToSort)):\n",
    "            mDate, sDate = datesToSort[i][13:21], datesToSort[i][29:37]\n",
    "            if (int(mDate) < eruptionDate) or (int(sDate) < eruptionDate):\n",
    "                bad.append(int(i))\n",
    "    else:\n",
    "        print(f'Input \\'condition\\' does not conform to expected input.\\ncondition: {condition}\\nExpected: \\'before\\' or \\'after\\'')\n",
    "    # loop through the indices of 'bad' in reverse order and delete undesired entries. \n",
    "    for index in sorted(bad, reverse=True):\n",
    "        del datesToSort[index]\n",
    "    return datesToSort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Set some variables determining whether and how to filter the paths of the images:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eruptionDate = 20180729\n",
    "condition = 'before'\n",
    "filter_dates = False # if True, separate data according to details above; if anything else, run the whole dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Filter the image paths by date if filter_dates = True:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the paths to include only those that are before or after ('condition') 'eruptionDate'.\n",
    "if filter_dates:\n",
    "    amp_paths = sortDates(amp_paths, condition, eruptionDate)\n",
    "    ingram_paths = sortDates(ingram_paths, condition, eruptionDate)\n",
    "    cohr_paths = sortDates(cohr_paths, condition, eruptionDate)\n",
    "\n",
    "print(amp_paths[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='5'> <b> 1. Identify Area of Interest</b> </font>\n",
    "    <br>\n",
    "    <font size='3'> Here we use an interactive Bokeh plot to identify our region of interest. Our region of interest must contain all of the expected deformation and a surrounding region of little to no deformation. Following our selection of this region, we will subset our data to this region. This helps reduce computation time. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Now we <b>select the bounding box for our area of interest.</b> Our area of interest should be large enough to include regions that have little to no deformation.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google maps, get rough bounding box for the area of interest\n",
    "ulx = -91.34 # Upper Left X; western most longitude\n",
    "lrx = -90.9 # Lower Right X; eastern most longitude\n",
    "lry = -0.97 # Lower Right Y; southern most latitude\n",
    "uly = -0.67 # Upper Left Y; northern most latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"ingrams/S1AA-20180712T002627-20180724T002627-DV-POEORB-12d-20x4-int-gamma/20180712T002627_20180724T002627_amp.tif\"\n",
    "pth2 = \"ingrams/S1AA-20180712T002627-20180724T002627-DV-POEORB-12d-20x4-int-gamma/20180712T002627_20180724T002627_amp_r.tif\"\n",
    "!gdalwarp -overwrite $pth $pth2 -s_srs EPSG:3857 -t_srs EPSG:32606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure, show, output_file, gmap\n",
    "import numpy as np\n",
    "import gdal\n",
    "import matplotlib.pylab as plt\n",
    "from bokeh.tile_providers import get_provider, Vendors\n",
    "from bokeh.models import ColumnDataSource, GMapOptions, HoverTool\n",
    "\n",
    "output_notebook()\n",
    "output_file(\"tile.html\")\n",
    "p = gmap\n",
    "tile_provider = get_provider(Vendors.STAMEN_TERRAIN)\n",
    "\n",
    "t = \"pan, wheel_zoom, tap, crosshair, reset, save, hover\"\n",
    "\n",
    "# range bounds supplied in web mercator coordinates\n",
    "p = figure(title=\"Title\", x_range=(-10167922, -10118941), y_range=(-107985, -74585),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\", tools=t)\n",
    "\n",
    "\n",
    "\n",
    "p.add_tools(HoverTool(tooltips = [\n",
    "    (\"index\", \"$index\"),\n",
    "    (\"(x,y)\", \"($x, $y)\")\n",
    "]))\n",
    "\n",
    "\n",
    "p.add_tile(tile_provider)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Use an interactive Bokeh graph to select the bounding box for the subset. \n",
    "# DOES THIS AFTER YOU HAVE GIAnT WORKING!!!!\n",
    "from PIL import Image\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "import numpy as np\n",
    "import gdal\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "output_notebook() # has to be run in order to make Bokeh display in the notebook. Otherwise, the code will run without an output. \n",
    "x_range,y_range = (ulx,lrx),(uly,lry)\n",
    "#TOOLS = \"pan, wheel_zoom, reset, hover, save\"\n",
    "x_axis_type,y_axis_type = \"mercator\",\"mercator\"\n",
    "\n",
    "im=gdal.Open(amp_paths[0])\n",
    "raster_1 = im.GetRasterBand(1).ReadAsArray()\n",
    "fig = plt.figure(figsize=(18,10)) # Initialize figure with a size\n",
    "##########ax1 = fig.add_subplot(111) # 221 determines: 2 rows, 2 plots, first plot\n",
    "###########ax1.imshow(raster_1,cmap='gray',vmin=-0.75,vmax=0.75) #,vmin=2000,vmax=10000)\n",
    "\n",
    "#im = Image.open(tiff_paths[0])\n",
    "#im = im.convert(\"RGBA\")\n",
    "#im.show()\n",
    "#imarray = np.array(im)\n",
    "XSize, YSize = im.RasterXSize, im.RasterYSize\n",
    "print(XSize) # Number of Pixels\n",
    "print(YSize) # Number of Lines\n",
    "\n",
    "# Takes a really long time for Bokeh to load the image. I may need to downsample it somehow. \n",
    "# Bokeh also shows my map flipped. \n",
    "p1 = figure(x_range=(0,im.RasterXSize), y_range=(0,im.RasterYSize), \n",
    "            x_axis_type=x_axis_type, y_axis_type=y_axis_type,\n",
    "            width=round(XSize/10), height=round(YSize/10))\n",
    "p1.image_rgba(image=[raster_1], x=0, y=0, dw=XSize, dh=YSize)\n",
    "show(p1)\n",
    "\n",
    "#data = np.empty((20,20), dtype=np.uint32)\n",
    "#print(f\"\\ndata = \",data[0])\n",
    "\"\"\"\n",
    "p = figure(\n",
    "    title=\"Bounding Box Test\", tools=TOOLS, x_range=x_range, y_range=y_range, \n",
    "    tooltips=[(\"Long (x)\",\"$x\"),(\"Lat (y)\",\"$y\"),(\"value\",\"@image\")], \n",
    "    x_axis_type=x_axis_type, y_axis_type=y_axis_type)#, \n",
    "#    source=data)\n",
    "\n",
    "p.image(image=[img])\n",
    "\n",
    "p.xaxis.axis_label = \"Longitude\"\n",
    "p.yaxis.axis_label = \"Latitude\"\n",
    "\n",
    "\"\"\"\n",
    "#p = figure(tooltips=[(\"value\",\"@image\")],x_range=[0,20],y_range=[0,20])\n",
    "#p.image(image=[data],x=[0],y=[0],dw=[20],dh=[20])\n",
    "\n",
    "# must give a vector of image data for image parameter\n",
    "#bk.plotting.show(p)\n",
    "\n",
    "# plot the created object\n",
    "#show(p)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='5'> <b> 2. Subset (Crop) Data to Area of Interest </b> </font>\n",
    "    <br>\n",
    "    <font size='3'> We now crop our data to our area of interest. We must do this for both the interferograms and the coherence files. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a directory for and subset the interferograms\n",
    "try:\n",
    "    shutil.rmtree('ingram_subsets')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "!mkdir -p ingram_subsets\n",
    "\n",
    "for ingram_path in ingram_paths:\n",
    "    _, granule_name, ingram_name = ingram_path.split('/')\n",
    "    \n",
    "    #Using the GDAL subset service, get a small subset around the volcano\n",
    "    #!wget -O ingram_subsets/{granule_name}_unw_phase.tiff \"https://services.asf.alaska.edu/geospatial/subset?ulx={ulx}&lrx={lrx}&lry={lry}&uly={uly}&product={granule_name}.zip/{granule_name}/{tiff_name}\"\n",
    "    \n",
    "    # GDAL service is out of service. Pretend that it isn't when calling the following equivalent command\n",
    "    gdal_command = f\"gdal_translate -epo -eco -projwin {ulx} {uly} {lrx} {lry} -projwin_srs 'WGS84' -co \\\"COMPRESS=DEFLATE\\\" -co \\\"TILED=YES\\\" -co \\\"COPY_SRC_OVERVIEWS=YES\\\" {ingram_path} ingram_subsets/{granule_name}_unw_phase.tiff > /dev/null\"\n",
    "    #print(f\"\\nCalling the command: {gdal_command}\")\n",
    "    !{gdal_command}\n",
    "    # ' > /dev/null' sends the output of the GDAL command to a null file to suppress overly verbose output. \n",
    "    # If you suspect a problem, remove ' > /dev/null'\n",
    "print(\"Interferograms subsetted.\")\n",
    "# repeat subsetting for the coherence files\n",
    "for cohr_path in cohr_paths:\n",
    "    _, granule_name, cohr_name = cohr_path.split('/')\n",
    "    gdal_command = f\"gdal_translate -epo -eco -projwin {ulx} {uly} {lrx} {lry} -projwin_srs 'WGS84' -co \\\"COMPRESS=DEFLATE\\\" -co \\\"TILED=YES\\\" -co \\\"COPY_SRC_OVERVIEWS=YES\\\" {cohr_path} ingram_subsets/{granule_name}_corr.tiff > /dev/null\"\n",
    "    !{gdal_command}\n",
    "print(\"Coherence files subsetted.\")\n",
    "# Repeat subsetting for amplitude files for later data visualizations\n",
    "for amp_path in amp_paths:\n",
    "    _, granule_name, amp_name = amp_path.split('/')\n",
    "    gdal_command = f\"gdal_translate -epo -eco -projwin {ulx} {uly} {lrx} {lry} -projwin_srs 'WGS84' -co \\\"COMPRESS=DEFLATE\\\" -co \\\"TILED=YES\\\" -co \\\"COPY_SRC_OVERVIEWS=YES\\\" {amp_path} ingram_subsets/{granule_name}_amp.tiff > /dev/null\"\n",
    "    !{gdal_command}\n",
    "print(\"Amplitude files subsetted for data visualizations.\")\n",
    "print(\"Subsetting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.1 Check that the subsetted tiffs have pixels</b></font>\n",
    "<br>\n",
    "<font size='3'>Some of the subsetted geotiffs do not have pixels in our desired area of interest despite the -epo -eco options which should cause an error for all of these and skip them. Below, we will check which geotiffs actually have pixels in our area of interest and remove those that don't. \n",
    "<br><br>\n",
    "    <b>Write a function to create a list of the geotiff paths and another to print them:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_paths(paths):\n",
    "    tiff_paths = !ls $paths | sort -t_ -k5,5\n",
    "    return tiff_paths\n",
    "\n",
    "def print_tiff_paths(tiff_paths):\n",
    "    print(\"Tiff paths:\")\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>Call get_tiff_paths() to make a list the geotiffs:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = f\"ingram_subsets/*.tiff\"  \n",
    "tiff_paths = get_tiff_paths(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>Remove any empty or partial subsets:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_path = f\"{os.getcwd()}/ingram_subsets/\"\n",
    "remove_nan_subsets(t_path, tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>2.2 Make sure that the subset images have the same number of pixels </b> </font>\n",
    "<br>\n",
    "<font size='3'>In some instances, the 'gdal_translate' function will return subsetted imagery with slightly different extents; for example, one subset may be 1000 x 1000 pixels while another is 1001 x 1000. This is usually more of a problem when different different data sensors are used as these sensors will often have different pixel sizes or their pixel locations will be slightly offset from each other. Since all of our data comes from Sentinel1, this is generally not a problem, but it is still good to double check.</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Update tiff_paths after possibly removing some containing incomplete data:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiff_paths = get_tiff_paths(paths)\n",
    "print(len(tiff_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Check geotiffs for unique dimensions:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Go through each '.tiff' file and get the file sizes. \n",
    "Pixels,Lines = [], []\n",
    "for file in tiff_paths: \n",
    "    im = gdal.Open(file)\n",
    "    raster_1 = im.GetRasterBand(1).ReadAsArray()\n",
    "    XSize, YSize = im.RasterXSize, im.RasterYSize\n",
    "    Pixels.append(XSize)\n",
    "    Lines.append(YSize)\n",
    "\n",
    "# get unique values\n",
    "Pixel_set, Line_set = set(Pixels), set(Lines)\n",
    "if len(Pixel_set) >1 or len(Line_set) > 1:\n",
    "    print(\"Problem: More than 1 pixel or line value. This indicates two or more subsetted .tiff files have different sizes.\")\n",
    "else: \n",
    "    print(\"All subsetted .tiff files are the same size. Hurray!\")\n",
    "    print(\"Pixels, Lines = \", Pixels[0], Lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'><b>If the '.tiff' files are different sizes, use GDAL to make them uniform.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change value of one of the pixels to test\n",
    "#Pixels[12] = Pixels[12]-10\n",
    "# find the index of the smallest of the '.tiff' files. \n",
    "idx_Pixel = np.argmin(Pixels)\n",
    "idx_Line =  np.argmin(Lines)\n",
    "idx = max([idx_Pixel, idx_Line]) # Value 'idx' will be zero if all of the files are the same size. \n",
    "\n",
    "# clip the other files according to that smallest '.tiff' file. \n",
    "if idx > 0:\n",
    "    PSize,LSize = Pixels[idx],Lines[idx] # Pixel and Line size all of the rasters should be. \n",
    "    for file in files:\n",
    "        if file is not files[idx]:\n",
    "            gdal_command = f\"gdal_translate -of GTIFF -projwin {ulx} {uly} {lrx} {lry} ingram_subsets/{file} ingram_subsets/{file}\"\n",
    "            try: \n",
    "                !{gdal_command}\n",
    "                print(\"Raster sizes corrected.\")\n",
    "            except:\n",
    "                print(\"Drat\")\n",
    "else:\n",
    "    print(\"Nothing to do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size='5'> <b> 3. Prepare Data for GIAnT </b> </font>\n",
    "<br>\n",
    "<font size='3'> Our resultant files need to be adjusted for input into GIAnT. This involves:\n",
    "\n",
    "- Adjusting filenames to start with the date <br>\n",
    "- (Optional) Remove potentially disruptive default values <br>\n",
    "- Convert data format from '.tiff' to '.flt' <br>\n",
    "</font>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>3.1 Adjust File Names</b></font>\n",
    "    <font size='3'><br>We will adjust the files to a simple format of &lt;masterdate&gt;-&lt;slavedate&gt;_&lt;unwrapped or coherence designation&gt;.tiff. This assumes that the files all come from Sentinel-1 and that every interferogram has a unique master and slave date pair. <br>This is entirely true; there are two interferograms with the same master and slave date pair. Because of this, one of these interferograms is lost during processing. To make use of every unique interferogram will require a more complex approach. To keep this exercise relatively simple, we will ignore the lost interferogram.</font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_dir = f\"{tiff_paths[0].split('/')[0]}/\"\n",
    "for i in range (0, len(tiff_paths)):\n",
    "    tiff_paths[i] = tiff_paths[i].split('/')[1]\n",
    "print_tiff_paths(tiff_paths)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <masterdate>-<slavedate>_<unwrapped or coherence designation>.tiff\n",
    "# Rename the interferogram, coherence, and amplitude (for plotting later) files. \n",
    "# Rename files. \n",
    "for file in tiff_paths:\n",
    "    if \"S1\" in file: # only affect those files that need to be renamed, which will have 'S1' at their start. \n",
    "        old_name, old_ext = os.path.splitext(file)\n",
    "        #print(f\"\\nCurrent Name: {oldname}\\nCurrent Extension: {oldExt}\")\n",
    "        master, slave = old_name[5:13], old_name[21:29]\n",
    "        if \"_unw\" in file:\n",
    "            new_name = f\"{master}-{slave}_unw_phase{old_ext}\"\n",
    "        elif \"_corr\" in file:\n",
    "            new_name = f\"{master}-{slave}_corr{old_ext}\"\n",
    "        elif \"_amp\" in file:\n",
    "            new_name = f\"{master}-{slave}_amp{old_ext}\"\n",
    "        exists = os.path.isfile(tiff_dir+new_name)\n",
    "        if exists:\n",
    "            print(f\"This one already exists: {new_name}\")\n",
    "        #print(f\"New Name: {newname}\")\n",
    "        os.rename(tiff_dir+file, tiff_dir+new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'><b>3.2 Remove disruptive default values</b></font> \n",
    "    <br>\n",
    "    <font size='3'>Now we can remove the potentially disruptive default values. For this lab, this step is optional. This is because the Sentinel-1 data does not have any of disruptive default values. However, for other datasources, this may be required.<br>This works by creating an entirely new geotiff with the text '_no_default' added to the file name.<br><b>IF YOU SKIP THIS STEP</b>, you will need to edit the code to exclude the use of '_no_default' when identifying which files to manipulate.</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unw_phase_paths = glob.glob(f\"{tiff_dir}*_unw_phase.tiff\")\n",
    "print(*unw_phase_paths, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in unw_phase_paths:\n",
    "    cmd = 'gdal_calc.py -A %s --outfile=%s.tiff --calc=\"(A>-1000)*A\" --NoDataValue=0 --format=GTiff' % (file, file.replace('.tiff','_no_default'))\n",
    "    os.system(cmd)\n",
    "print(\"Disruptive default values removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='3'> Delete *_unw_phase.tiff files that have potentially disruptive default values.<br><b>SKIP THIS STEP IF YOU HAVE NOT RUN THE CODE DIRECTLY ABOVE</b>.</font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in unw_phase_paths:\n",
    "    try:\n",
    "        os.remove(p)\n",
    "    except:\n",
    "        print(f\"Error: failed to remove {p}\")\n",
    "if not glob.glob(f\"{tiff_dir}*_unw_phase.tiff\"):\n",
    "    print(f\"There are no *_unw_phase.tiff files containing potentially disruptive default values.\")\n",
    "else:\n",
    "    print(\"WARNING: Potentially disruptive value files still present!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>3.3 Convert data format from '.tiff' to '.flt'</b><br></font>\n",
    "    <font size='3'>Now we go through the interferograms and coherence files and alter their format.<br>First, we convert the interferograms.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tiff_2_flt(paths):\n",
    "    new_ext = '.flt'\n",
    "    for file in paths: \n",
    "        new_name = file.split('.')[0]\n",
    "        # create system command that will convert the file from a .tiff to a .flt\n",
    "        cmd = f\"gdal_translate -of ENVI {file} {new_name}{new_ext}\"\n",
    "        #print(f\"cmd = {cmd}\") # display what the command looks like. \n",
    "        try:\n",
    "            # pass command to the system\n",
    "            os.system(cmd)\n",
    "        except: \n",
    "            print(\"Problem\")\n",
    "    print(\"Conversion from '.tiff' to '.flt' complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the unwrapped phase files from '.tiff' to '.flt'\n",
    "no_default_paths = glob.glob('ingram_subsets/*unw_phase_no_default.tiff')\n",
    "# Check that the number of files and the names appear correct\n",
    "print(len(no_default_paths))\n",
    "print(*no_default_paths, sep=\"\\n\")\n",
    "# file format should appear as below\n",
    "# YYYYMMDD-YYYYMMDD_unw_phase_no_default.flt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tiff_2_flt(no_default_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'>Now we repeat the '.tiff' to '.flt' conversion for the coherence files. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_paths = glob.glob(f\"{tiff_dir}*_corr.tiff\")\n",
    "print(len(corr_paths))\n",
    "print(*corr_paths, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tiff_2_flt(corr_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corr_paths))\n",
    "print(*corr_paths, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Let's set up some path information and a date file that will be used later. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>This next step is optional.<br>Here, we can clean up our data folder by removing the subsetted geotiff files which are now duplicates. This is primarily for saving space. This step can be skipped without making modification to the code below. <br>For now, let's leave the code commented out (using the quotation marks) so we can use the subsetted geotiffs for data visualization after we use GIAnT.<br><b>If you run the code to remove all of the geotiffs, they will not be available for visualization later.</b> <i>It may be prudent to change this code to do all except the first clipped tiff.</i></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove extra files that we no longer need (the '.tiff' files)\n",
    "# Might be useful to skip this and keep the .tiff files for visualization later. \n",
    "\"\"\"\n",
    "files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase_no_default'+file_ext) or f.endswith('_corr'+file_ext)] \n",
    "for file in files:\n",
    "    #if '_no_default' not in file:\n",
    "    try:\n",
    "        os.remove(datadirectory+file)\n",
    "        print(f\"File Removed: {file}\")\n",
    "    except:\n",
    "        print(f\"File Not Found: {file}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>4. Create Input Files And Code for GIAnT</b></font>\n",
    "    <br>\n",
    "    <font size ='3'>Let's create the input files and specialty code that GIAnT requires. These are listed below. \n",
    "        <br>\n",
    "        \n",
    "- ifg.list\n",
    "    - List of the interferogram properties including master and slave date, perpendicular baseline, and sensor. \n",
    "- date.mli.par\n",
    "    - File from which GIAnT pulls requisite information about the sensor. \n",
    "    - This is specifically for GAMMA files. When using other interferogram processing techniques, an alternate file is required. \n",
    "- prepxml_SBAS.py\n",
    "    - Python function to create an xml file that specifies the processing options to GIAnT. \n",
    "    - This must be modified by the user for their particular application. \n",
    "- userfn.py\n",
    "    - Python function to map the interferogram dates to a phyiscal file on disk. \n",
    "    - This must be modified by the user for their particular application. \n",
    "    </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>4.1 Create 'ifg.list' File </b> </font> </font>\n",
    "<br>\n",
    "<font size='3'> This will simple 4 column text file will communicate network information to GIAnT. It will be created within the <b>GIAnT</b> folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_dir = './GIAnT' # directory where we will perform GIAnT analysis\n",
    "new_directory(giant_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_paths = glob.glob(f\"{tiff_dir}*_amp.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Now we copy a clipped geotiff into our working directory for later data visualization.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(amp_paths[0], giant_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_default_flt_paths = glob.glob(f\"{tiff_dir}*_unw_phase_no_default.flt\")\n",
    "print(len(no_default_flt_paths))\n",
    "print(*no_default_flt_paths, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_default_flt_paths[0].split('/')[1][:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get all of the primary and secondary dates. \n",
    "masterdates, slavedates = [], []\n",
    "for p in no_default_flt_paths:\n",
    "    masterdates.append(p.split('/')[1][:8])\n",
    "    slavedates.append(p.split('/')[1][9:17])\n",
    "# Sort the dates according to the master dates. \n",
    "p_dates, s_dates = (list(t) for t in zip(*sorted(zip(masterdates, slavedates))))\n",
    "\n",
    "with open( os.path.join('GIAnT', 'ifg.list'), 'w') as fid:\n",
    "    for i in range(len(p_dates)):\n",
    "        masterdate = p_dates[i] # pull out master Date (first set of numbers)\n",
    "        slavedate = s_dates[i] # pull out slave Date (second set of numbers)\n",
    "        bperp = '0.0' # according to JPL notebooks\n",
    "        sensor = 'S1' # according to JPL notebooks\n",
    "        fid.write(f'{masterdate}  {slavedate}  {bperp}  {sensor}\\n') # write values to the 'ifg.list' file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>You may notice that the code above sets the perpendicular baseline to a value of 0.0 m. This is not the true perpendicular baseline. That value can be found in metadata file (titled '$<$primary timestamp$>$_$<$secondary timestamp$>$.txt') that comes with the original interferogram. Generally, we would want the true baseline for each interferogram. However, since Sentinel-1 has such a short baseline, a value of 0.0 m is sufficient for our purposes. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>4.2 Create 'date.mli.par' File </b> </font> \n",
    "<br>\n",
    "<font size='3'> As we are using GAMMA products, we must create a 'date.mli.par' file from which GIAnT will pull necessary information. If another processing technique is used to create the interferograms, an alterante file name and file inputs are required. </font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file 'date.mli.par'\n",
    "# Get WIDTH (xsize) and FILE_LENGTH (ysize) information\n",
    "ds = gdal.Open(no_default_flt_paths[0], gdal.GA_ReadOnly)\n",
    "n_lines = ds.RasterYSize\n",
    "n_pixels = ds.RasterXSize\n",
    "trans = ds.GetGeoTransform()\n",
    "ds = None\n",
    "\n",
    "# Get the center line UTC time stamp; can also be found inside <date>_<date>.txt file and hard coded\n",
    "dir_name = os.listdir('ingrams')[0] # get original file name (any file can be used; the timestamps are different by a few seconds)\n",
    "vals = dir_name.split('-') # break file name into parts using the separator '-'\n",
    "time_stamp = vals[2][9:16] # extract the time stamp from the 2nd datetime (could be the first)\n",
    "c_l_utc = int(time_stamp[0:2]) * 3600 + int(time_stamp[2:4])*60 + int(time_stamp[4:6])\n",
    "\n",
    "radar_freq = 299792548.0 / 0.055465763 # radar frequency; speed of light divided by radar wavelength of Sentinel1 in meters\n",
    "\n",
    "# write the 'date.mli.par' file\n",
    "with open(os.path.join(giant_dir, 'date.mli.par'), 'w') as fid:\n",
    "    # Method 1\n",
    "    fid.write(f'radar_frequency: {radar_freq} \\n') # when using GAMMA products, GIAnT requires the radar frequency. Everything else is in wavelength (m) \n",
    "    fid.write(f'center_time: {c_l_utc} \\n') # Method from Tom Logan's prepGIAnT code; can also be found inside <date>_<date>.txt file and hard coded\n",
    "    fid.write( 'heading: -11.9617913 \\n') # inside <date>_<date>.txt file; can be hardcoded or set up so code finds it. \n",
    "    fid.write(f'azimuth_lines: {n_lines} \\n') # number of lines in direction of the satellite's flight path\n",
    "    fid.write(f'range_samples: {n_pixels} \\n') # number of pixels in direction perpendicular to satellite's flight path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>4.3 Make prepxml_SBAS.py</b> </font>\n",
    "<br>\n",
    "<font size='3'>We will create a prepxml_SBAS.py function and put it into our GIAnT working directory.</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='3'><b>4.3.1 Necessary prepxml_SBAS.py edits</b></font>\n",
    "<br>\n",
    "<font size='3'> GIAnT comes with an example prepxml_SBAS.py, but requries significant edits for our purposes. These alterations have already been made, so we don't have to do anything now, but it is good to know the kinds of things that have to be altered. The details of some of these options can be found in the GIAnT documentation. The rest must be found in the GIAnT processing files themselves, most notably the tsxml.py and tsio.py functions. <br>The following alterations were made:\n",
    "<br>\n",
    "- Changed 'example' &#9658; 'date.mli.par'\n",
    "- Removed 'xlim', 'ylim', 'rxlim', and 'rylim'\n",
    "    - These are used for clipping the files in GIAnT. As we have already done this, it is not necessary. \n",
    "- Removed latfile='lat.map' and lonfile='lon.map'\n",
    "    - These are optional inputs for the latitude and longitude maps. \n",
    "- Removed hgtfile='hgt.map'\n",
    "    - This is an optional altitude file for the sensor. \n",
    "- Removed inc=21.\n",
    "    - This is the optional incidence angle information. \n",
    "    - It can be a constant float value or incidence angle file. \n",
    "    - For Sentinel1, it varies from 29.1-46.0&deg;.\n",
    "- Removed masktype='f4'\n",
    "    - This is the mask designation. \n",
    "    - We are not using any masks for this. \n",
    "- Changed unwfmt='RMG' &#9658; unwfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Removed demfmt='RMG'\n",
    "- Changed corfmt='RMG' &#9658; corfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Changed nvalid=30 -> nvalid=1\n",
    "    - This is the minimum number of interferograms in which a pixel must be coherent. A particular pixel will be included only if its coherence is above the coherence threshold, cohth, in more than nvalid number of interferograms. \n",
    "- Removed atmos='ECMWF'\n",
    "    - This is an amtospheric correction command. It depends on a library called 'pyaps' developed for GIAnT. This library has not been installed yet. \n",
    "- Changed masterdate='19920604' &#9658; masterdate='20161119'\n",
    "    - Use our actual masterdate. \n",
    "    - I simply selected the earliest date as the masterdate. \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Defining a reference region is a potentially important step. This is a region at which there should be no deformation. For a volcano, this should be some significant distance away from the volcano. GIAnT has the ability to automatically select a reference region which we will use for this exercise. <b>If you wish to set your own reference region, do so in the code below and remove the '#' from before 'rxlim=[{1},...' below.</b><br>Below is an example of how the reference region would be defined. If we look at the prepxml_SBAS.py code below, rxlim and rylim, the pixel based location of the reference region, is within the code, but has been commented out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reference region\n",
    "ref_region_size = [5, 5]   # Reference region size in Lines and Pixels\n",
    "ref_region_center = [-0.9, -91.3] # Center of reference region in lat, lon coordinates\n",
    "rxlim,rylim = [0, 10], [95, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_date = min([no_default_flt_paths[i].split('/')[1][:8] for i in range(len(no_default_flt_paths))], key=int)\n",
    "filtr = 1.0 / 6 # temporal filter in length of years. \n",
    "\n",
    "prepxml_SBAS_Template = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Example script for creating XML files for use with the SBAS processing chain. This script is supposed to be copied to the working directory and modified as needed.\"\"\"\n",
    "\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    'date.mli.par', proc='GAMMA', \n",
    "    #rxlim = [{1},{2}], rylim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt={6})\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''\n",
    "\n",
    "with open(os.path.join(giant_dir,'prepxml_SBAS.py'), 'w') as fid:\n",
    "    fid.write(prepxml_SBAS_Template.format(giant_dir,rxlim[0],rxlim[1],rylim[0],rylim[1],m_date,filtr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>4.4 Run prepxml_SBAS.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> Here we run <b>prepxml_SBAS.py</b> to create the 2 needed files\n",
    "\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "\n",
    "To use MinTS, we would run <b>prepxml_MinTS.py</b> to create\n",
    "\n",
    "- data.xml\n",
    "- mints.xml\n",
    "        \n",
    "These files are needed by <b>PrepIgramStack.py</b>. \n",
    "<br>\n",
    "We must first switch to the GIAnT folder in which <b>prepxml_SBAS.py</b> is contained, then call it. Otherwise, <b>prepxml_SBAS.py</b> will not be able to find the file 'date.mli.par', which holds necessary processing information. \n",
    "\n",
    "</font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General path to the GIAnT code. This is a temporary necessity; in the future, the path to GIAnT will be unnecessary. \n",
    "giant_path = \"/usr/local/GIAnT/SCR\" # only for the code that we will not modify. #\n",
    "\n",
    "\n",
    "#os.chdir(giant_path)\n",
    "#!ls\n",
    "#!cat PrepIgramStack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into your working directory\n",
    "if os.getcwd() != f\"{path}/GIAnT\":\n",
    "    os.chdir(f\"{path}/GIAnT\")\n",
    "print(f\"{os.getcwd()}:\")\n",
    "print(*glob.glob('*.*'), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python2.7 prepxml_SBAS.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data.xml # display the contents of 'data.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat sbas.xml # display the contents of 'sbas.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>After running <b>prepxml_SBAS.py</b>, output will appear in the Juupyter Notebook in which some your input values can be checked. Make sure the two requisite xml files were produced. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>4.5 Create userfn.py</b></font>\n",
    "<br>\n",
    "<font size='3'>Before running the next piece of code, <b>PrepIgramStack.py</b>, we must create a python file called <b>userfn.py</b>. This file maps the interferogram dates to a physical file on disk. This python file must be in our working directory, <b>/GIAnT</b>. We can create this file from within the notebook using python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_dir = './ingram_subsets'\n",
    "rel_dir = os.path.relpath(aligned_dir, giant_dir)\n",
    "userfn_template = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "import os \n",
    "\n",
    "def makefnames(dates1, dates2, sensor):\n",
    "    dirname = '{0}'\n",
    "    root = os.path.join(dirname, dates1+'-'+dates2)\n",
    "    #unwname = root+'_unw_phase.flt' # for potentially disruptive default values kept. \n",
    "    unwname = root+'_unw_phase_no_default.flt' # for potentially disruptive default values removed. \n",
    "    corname = root+'_corr.flt'\n",
    "    return unwname, corname\n",
    "\"\"\"\n",
    "\n",
    "with open('userfn.py', 'w') as fid:\n",
    "    fid.write(userfn_template.format(rel_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>5. Run GIAnT</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We have now created all of the necessary files to run GIAnT. The full GIAnT process requires 3 function calls.\n",
    "- PrepIgramStack.py\n",
    "- ProcessStack.py\n",
    "- SBASInvert.py\n",
    "<br>We will make a 4th function call that is not necessary, but provides some error estimation that can be useful.\n",
    "- SBASxval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>5.1 Run PrepIgramStack.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> Here we run <b>PrepIgramStack.py</b> to create the files for GIAnT. This will read in the input data and the files we previously created. This will output HDF5 files. As we did not have to modify <b>PrepIgramStack.py</b>, we will call from the installed GIAnT library. <br>\n",
    "Inputs:       \n",
    "- ifg.list\n",
    "- data.xml\n",
    "- sbas.xml        \n",
    "\n",
    "Outputs:\n",
    "- RAW-STACK.h5\n",
    "- PNG previews under 'Igrams' folder \n",
    "    \n",
    "    </font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some help information\n",
    "!python2.7 $giant_path/PrepIgramStack.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call PrepIgramStack.py\n",
    "!python2.7 $giant_path/PrepIgramStack.py -i ifg.list\n",
    "# The '-i ifg.list' is technically unnecessary as that is the default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the files really are HDF5 files as they need to be. \n",
    "#files = [f for f in os.listdir(datadirectory) if os.path.isfile(os.path.join(datadirectory,f))]\n",
    "file = os.path.join('Stack','RAW-STACK.h5')\n",
    "#print(files)\n",
    "if not h5py.is_hdf5(file):\n",
    "    print(f'Not an HDF5 file:{file}')\n",
    "else:\n",
    "    print(\"It's an HDF5 file! Hurray!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>5.2 Run ProcessStack.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> This seems to be an optional step. Does atmospheric corrections and estimation of orbit residuals. <br>\n",
    "Inputs:\n",
    "\n",
    "- HDF5 files from PrepIgramStack.py, RAW-STACK.h5\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "- GPS Data (optional; we don't have this)\n",
    "- Weather models (downloaded automatically)\n",
    "\n",
    "Outputs: \n",
    "\n",
    "- HDF5 files, PROC-STACK.h5\n",
    "        \n",
    "These files are then fed into SBAS. \n",
    "</font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some help information\n",
    "!python2.7 $giant_path/ProcessStack.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python2.7 $giant_path/ProcessStack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join('Stack','PROC-STACK.h5')\n",
    "#print(files)\n",
    "if not h5py.is_hdf5(file):\n",
    "    print(f'Not an HDF5 file:{file}')\n",
    "else:\n",
    "    print(\"It's an HDF5 file! Hurray!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>5.3 Run SBASInvert.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> Actually do the time series. \n",
    " \n",
    "Inputs\n",
    "\n",
    "- HDF5 file, PROC-STACK.h5\n",
    "- data.xml\n",
    "- sbas.xml\n",
    "\n",
    "Outputs\n",
    "\n",
    "- HDF5 file: LS-PARAMS.h5\n",
    "\n",
    "</font> </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some help information\n",
    "!python2.7 $giant_path/SBASInvert.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python2.7 $giant_path/SBASInvert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>5.4 Run SBASxval.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> Get an uncertainty estimate for each pixel and epoch using a Jacknife test. \n",
    " \n",
    "Inputs: \n",
    "\n",
    "- HDF5 files, PROC-STACK.h5\n",
    "- data.xml\n",
    "- sbas.xml\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- HDF5 file, LS-xval.h5\n",
    "\n",
    "</font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some help information\n",
    "!python2.7 $giant_path/SBASxval.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python2.7 $giant_path/SBASxval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>6. Data Visualization</b></font>\n",
    "<br>\n",
    "    <font size='3'>Now we visualize the data. This is largely copied from Lab4B. </font></font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>First, we have to load the stack produced by GIAnT and read it into an array so we can manipulate and display it.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Stack/LS-PARAMS.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "# List all groups\n",
    "print(\"Keys: %s\" %f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Details on what each of these keys means can be found in the GIAnT documentation. For now, the only keys with which we are concerned are 'recons' (the filtered time series of each pixel) and 'dates' (the dates of acquisition). It is important to note that the dates are given in a type of Julian Day number called Rata Die number. This will have to be converted later, but this can easily be done via one of several different methods in Python. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our data from the stack\n",
    "data_cube = f[('recons')]\n",
    "# Get the dates for each raster from the stack\n",
    "dates = list(f['dates']) # these dates appear to be given in Rata Die style: floor(Julian Day Number - 1721424.5). \n",
    "if data_cube.shape[0] is not len(dates):\n",
    "    print('Problem')\n",
    "    print('Number of rasters in data_cube: ', data_cube.shape[0])\n",
    "    print('Number of dates: ', len(dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Display and save an amplitude image of the volcano. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot amplitude image with transparency determined by alpha. \n",
    "plt.rcParams.update({'font.size': 14})\n",
    "radar_tiff = '20180724-20180805_amp.tiff'\n",
    "radar=gdal.Open(radar_tiff)\n",
    "im_radar = radar.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Remove zeros is numpy array to avoid divide-by-zero errors when calling np.log10():</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy requires use of a range-based for loop\n",
    "for i in range (0, len(im_radar)):\n",
    "    for x in range (0, len(im_radar[i])):\n",
    "        if im_radar[i][x] == 0.:\n",
    "            im_radar[i][x] = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbplot = np.log10(im_radar)\n",
    "vmin=np.percentile(dbplot,3)\n",
    "vmax=np.percentile(dbplot,97)\n",
    "fig = plt.figure(figsize=(18,10)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(111) # 221 determines: 2 rows, 2 plots, first plot\n",
    "ax1.imshow(dbplot, cmap='gray',vmin=vmin,vmax=vmax,alpha=1);\n",
    "plt.title('Example dB-scaled SAR Image for Ifgrm 20161119-20170106')\n",
    "plt.grid()\n",
    "plt.savefig('SierraNegra-dBScaled-AmplitudeImage.png',dpi=200,transparent='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Display an overlay of the clipped deformation map and amplitude image. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a short function that can plot an overaly of our radar image and deformation map. \n",
    "def defNradar_plot(deformation,radar):\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    vmin=np.percentile(radar,3)\n",
    "    vmax=np.percentile(radar,97)\n",
    "    ax.imshow(radar,cmap='gray',vmin=vmin,vmax=vmax)\n",
    "    fin_plot = ax.imshow(deformation,cmap='RdBu',vmin=-50.0,vmax=50.0,alpha=0.75)\n",
    "    fig.colorbar(fin_plot,fraction=0.24,pad=0.02)\n",
    "    ax.set(title=\"Integrated Defo [mm] Overlain on Clipped db-Scaled Amplitude Image\")\n",
    "    plt.grid()\n",
    "    return None\n",
    "# Get deformation map and radar image we wish to plot\n",
    "deformation = data_cube[data_cube.shape[0]-1]\n",
    "# Call function to plot an overlay of our deformation map and radar image.\n",
    "defNradar_plot(deformation,dbplot)\n",
    "plt.savefig('SierraNegra-DeformationComposite.png',dpi=200,transparent='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Create an animation of the deformation</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from Rata Die number (similar to Julian Day number) contained in 'dates' to Gregorian date. \n",
    "tindex = []\n",
    "for d in dates:\n",
    "    tindex.append(date.fromordinal(int(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "vmin=np.percentile(data_cube.flatten(),5)\n",
    "vmax=np.percentile(data_cube.flatten(),95)\n",
    "\n",
    "\n",
    "im = ax.imshow(data_cube[0],cmap='RdBu',vmin=-50.0,vmax=50.0)\n",
    "ax.set_title(\"Animation of Deformation Time Series - Sierra Negra, Galapagos\")\n",
    "fig.colorbar(im)\n",
    "plt.grid()\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"Date: {}\".format(tindex[i]))\n",
    "    im.set_data(data_cube[i])\n",
    "    \n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=data_cube.shape[0], interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('animation', embed_limit=10.0**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the animation\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the animation as a 'gif' file. \n",
    "ani.save('SierraNegraDeformationTS_filt={0}.gif'.format(filtr), writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>7. Alter the time filter parameter</b></font><br>\n",
    "    <font size='3'>Looking at the video above, you may notice that the deformation has a very smoothed appearance. This may be because of our time filter which is currently set to 1 year ('filt=1.0' in the prepxml_SBAS.py code). Let's repeat the lab from there with 2 different time filters. <br>First, using no time filter ('filt=0.0') and then using a 1 month time filter ('filt=0.082'). Change the output file name for anything you want saved (e.g., 'SierraNegraDeformationTS.gif' to 'YourDesiredFileName.gif'). Otherwise, it will be overwritten. <br><br>How did these changes affect the output time series?<br>How might we figure out the right filter length?<br>What does this say about the parameters we select? </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>8. Clear data from the Notebook (optional)</b></font>\n",
    "    <br>\n",
    "    <font size='3'>This lab has produced a large quantity of data. If you look at this notebook in your home directory, it should now be ~80 MB. This can take a long time to load in a Jupyter Notebook. It may be useful to clear the cell outputs, which will restore the Notebook to its original size. <br>To clear the cell outputs, go Cell->All Output->Clear. This will clear the outputs of the Jupyter Notebook and restore it to its original size of ~60 kB. This will not delete any of the files we have created. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657 Microwave Remote Sensing - Version 1.0 - Feb 2019 </i>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
