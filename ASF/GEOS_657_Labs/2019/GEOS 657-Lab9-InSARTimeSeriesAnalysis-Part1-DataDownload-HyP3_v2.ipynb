{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.png\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 9: InSAR Time Series Analysis using GIAnT within Jupyter Notebooks<br>Part 1: Data Download & Preprocessing from a SARVIEWS Import <font color='rgba(200,0,0,0.2)'> -- [## Points] </font> </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer, Joshua J C Knicely, Alex Lewandowski, Rowan Biessel; University of Alaska Fairbanks</b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /><font color='rgba(200,0,0,0.2)'> <b>Due Date: </b>NONE</font>\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. This lab is divided into 3 parts: 1) data download and preprocessing, 2) GIAnT time series, and 3) a simple Mogi source inversion. The primary goal of this lab is to demonstrate how to download the requisite data, specifically interferograms, and preprocess them for use with the Generic InSAR Analysis Toolbox (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">GIAnT</a>) in the framework of *Jupyter Notebooks*.<br>\n",
    "\n",
    "<b>Our specific objectives for this lab are to:</b>\n",
    "\n",
    "- Download data using ASF tools: \n",
    "    - From a SARVIEWS subscription. \n",
    "- Pre-process data: \n",
    "    - Subset (crop) the data to an Area of Interest (AOI). \n",
    "    - Verify the quality of the data.\n",
    "    - Cull data selection based on a timeframe and orbital characteristics. \n",
    "    - Reproject interferograms to a uniform UTM zone. \n",
    "    - Use TRAIN to remove static atmospheric effects related to surface elevation.\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> Target Description </b> </font>\n",
    "\n",
    "<font size=\"3\"> In this lab, we will download interferograms covering a SARVIEWS area of interest.  </font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>THIS NOTEBOOK INCLUDES NO HOMEWORK ASSIGNMENTS.</b></font> <br>\n",
    "\n",
    "Contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%%javascript\n",
    "var kernel = Jupyter.notebook.kernel;\n",
    "var command = [\"notebookUrl = \",\n",
    "               \"'\", window.location, \"'\" ].join('')\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/train':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"train\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select \"train\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"train\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>Overview</b></font>\n",
    "<br>\n",
    "<font size='3'><b>About TRAIN</b>\n",
    "<br>\n",
    "The tropospheric correction is one of the most significant challanges in InSAR. Without this correction, surface deformation signals can go completely unnoticed, or, perhaps worse, a false signal caused by the atmosphere can be taken as an accurate representation of surface deformation. This can often occur with volcanoes due to the characteristics of the atmosphere as well as the surface elevation (i.e., a taller point on the volcano means the InSAR signal passed through less atmosphere and will be affected differently from a point lower on the volcano). <br>We will use the Toolbox for Reducing Atmospheric InSAR Noise (TRAIN) today. The purpose of TRAIN is to add state of the art tropospheric correction methods to the InSAR processing chain. It can include corrections that are phase-based, using spectrometers, using weather models, and even data from balloon soundings. \n",
    "<br><br>\n",
    "<b>Limitations</b>\n",
    "<br>\n",
    "The particular version we are using was created by the Alaska Satellite Facility. ASF took the original MATLAB code developed by David Bekaert created a Python2.7 wrapper (and more recently, a Python3.7 wrapper)for it. Currently, it only allows processing using MERRA2 data. Including other data types can be done relatively easily, though it does require modification of the existing python code. \n",
    "<br>\n",
    "Each of the correction methods included in TRAIN is ideal for different locations and conditions. Spectrometers provide the best correction, but can only be used in cloud-free and daylight conditions. Phase-based and weather model correction methods capture regional signals well, but fail to capture turbulent tropospheric signals. \n",
    "<br><br>\n",
    "More information about TRAIN, its capabilities, and its limitations can be found in <a href=\"http://www.sciencedirect.com/science/article/pii/S0034425715301231\">Bekaert et al. [2015]</a>, at David Dekaert's <a href=\"http://davidbekaert.com/#links\">webpage</a>, or at the <a href=\"https://github.com/asfadmin/hyp3-TRAIN\" target=\"_blank\">ASF Github</a>. \n",
    "<br><br>\n",
    "<b>Steps to use TRAIN</b><br>\n",
    "\n",
    "- System Setup\n",
    "    - Import Python Packages\n",
    "    - Set User Inputs\n",
    "- Download and Preprocess Data\n",
    "    - Access SARVIEWs Subscriptions\n",
    "    - Download and unzip the Data\n",
    "    - Project all Geotiffs to the Same UTM Zone\n",
    "    - Mosaic Geotiffs with Partial Coverage\n",
    "- Identify Area of Interest\n",
    "- Subset (Crop) Data to Area of Interest\n",
    "    - Subset the Data\n",
    "    - Check that Subsetted Geotiffs have Pixels\n",
    "    - Check the Dimensions of Subsetted Geotiffs\n",
    "- Create Input Files and Code for TRAIN\n",
    "    - Create <font face='Courier New'>parms_aps.txt</font> file\n",
    "    - Create <font face='Courier New'>ifgday.mat</font> file\n",
    "    - Convert Subsetted Tiffs to GCS Coordinates\n",
    "    - Adjust file names\n",
    "- Run TRAIN\n",
    "    - Minor Set Up\n",
    "    - Steps 0-3\n",
    "    - Step 4\n",
    "    - Comparison of Corrected and Uncorrected Unwrapped Phase\n",
    "    - Convert back to the original coordinate system\n",
    "\n",
    "<br><br>\n",
    "When you use TRAIN, cite the creator's work using:<br>\n",
    "Bekaert et al., RSE 2015, \"Statistical comparison of InSAR tropospheric correction techniques.\" <br>&emsp;Open access: http://www.sciencedirect.com/science/article/pii/S0034425715301231\n",
    "<!-- <br><br><b><i>DO WE NEED TO ALSO GIVE ASF A CITATION???</i></b> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>0. Setup</b></font><br>\n",
    "    <font size='3'>We will first import requisite Python libraries and modules, create or define an analysis directory, and define all of our user inputs. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size=\"4\"> <b> 0.0 Import Python Packages and Enable Extensions</b></font>    <br>\n",
    "    <font size='3'>First, we will import all needed libraries and modules. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "from getpass import getpass\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import pathlib\n",
    "from datetime import datetime, date\n",
    "from copy import copy\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "from osgeo import gdal\n",
    "import pyproj\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Markdown\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from hyp3_sdk import Batch, HyP3\n",
    "\n",
    "import asf_notebook as asfn\n",
    "asfn.jupytertheme_matplotlib_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size=\"4\"> <b> 0.1 Define an Analysis Directory</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    analysis_directory = asfn.input_path(\n",
    "        f\"\\nPlease enter the name of a directory in which to store your data for this analysis.\")\n",
    "    if os.path.exists(analysis_directory):\n",
    "        contents = glob.glob(f'{analysis_directory}/*')\n",
    "        if len(contents) > 0:\n",
    "            choice = asfn.handle_old_data(analysis_directory, contents)\n",
    "            if choice == 1:\n",
    "                shutil.rmtree(analysis_directory)\n",
    "                os.mkdir(analysis_directory)\n",
    "                break\n",
    "            if choice == 2:\n",
    "                break\n",
    "            else:\n",
    "                clear_output()\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        os.mkdir(analysis_directory)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size=\"4\"> <b> 0.2 Set Paths and TRAIN Parameters </b></font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>0.2.0 Create directories we will need later in the notebook</b><br></font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(analysis_directory)\n",
    "analysis_directory = os.getcwd()\n",
    "print(f\"Current working directory: {analysis_directory}\")\n",
    "\n",
    "# Create a dictonary to hold values we wish to pickle and use in the Part 2 notebook\n",
    "to_pickle = dict()\n",
    "\n",
    "# Create the folder that will hold the full interferograms and their associated files. \n",
    "ingram_folder = 'ingrams'\n",
    "pathlib.Path(ingram_folder).mkdir(parents=True, exist_ok=True)\n",
    "to_pickle.update({'ingram_folder': ingram_folder})\n",
    "\n",
    "# Create the folder in which we wish to store our interferogram subsets. \n",
    "subset_folder = 'ingram_subsets'\n",
    "pathlib.Path(subset_folder).mkdir(parents=True, exist_ok=True)\n",
    "to_pickle.update({'subset_folder': subset_folder})\n",
    "\n",
    "            \n",
    "# Create the folder in which we wish to store our converted interferogram subsets. \n",
    "# This is important later in the notebook when we convert our subsets from a \n",
    "# local geographic coordinate system to decimal degrees. \n",
    "corrected_folder = 'ingram_subsets_converted'\n",
    "pathlib.Path(corrected_folder).mkdir(parents=True, exist_ok=True)\n",
    "to_pickle.update({'corrected_folder': corrected_folder})\n",
    "\n",
    "\n",
    "train_dir = '' # temporary blank directory\n",
    "merra2_datapath = './MERRA2'\n",
    "dem_path = os.path.join(train_dir,'myDEM.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>0.2.1 Designate TRAIN Input Parameters</b><br></font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_data_type = 'ECMWF'\n",
    "ifgday_file = 'ifgday.mat'\n",
    "lambda_meters = 0.055465763 \n",
    "incidence_angle = 38.5/180*np.pi # This needs to be in radians. \n",
    "extra = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Create a HyP3 object and authenticate</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3 = HyP3(prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 1. Download and Preprocess Data</b> </font>\n",
    "    <br><br>\n",
    "<font size=\"3\"><b>1.0.1 List your projects and select one containing INSAR_GAMMA products:</b> </font>\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = asfn.get_RTC_projects(hyp3)\n",
    "\n",
    "if len(projects) > 0:\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a project, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Project:')\n",
    "    project_select = asfn.select_parameter(projects)\n",
    "\n",
    "project_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Select a date range of products to download:</b>\n",
    "    <br><br>\n",
    "Note: There may not yet be MERRA2 weather data for recently acquired scenes. Visit the <a href=\"https://disc.gsfc.nasa.gov/datasets?keywords=%22MERRA-2%20inst6_3d_ana_Np%22&page=1\" target=\"_blank\">MERRA2 inst6_3d_ana_Np dataset page</a> to confirm that you download HyP3 products that are old enough to have weather data. Avoid downloading products that are newer than the listed \"End Date.\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = project_select.value\n",
    "jobs = hyp3.find_jobs(name=project, job_type='INSAR_GAMMA')\n",
    "jobs = jobs.filter_jobs(running=False, include_expired=False)\n",
    "jobs = Batch([job for job in jobs])\n",
    "\n",
    "if len(jobs) < 1:\n",
    "    raise ValueError(\"There are no unexpired INSAR_GAMMA products for this project.\\nSelect a different project or rerun your jobs in Vertex.\")\n",
    "\n",
    "display(Markdown(\"<text style='color:darkred;'>Note: After selecting a date range, you should select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "display(Markdown(\"<text style='color:darkred;'>Otherwise, you may simply rerun this code cell.</text>\"))\n",
    "print('\\nSelect a Date Range:')\n",
    "dates = asfn.get_job_dates(jobs)\n",
    "date_picker = asfn.gui_date_picker(dates)\n",
    "date_picker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Save the selected date range and remove products falling outside of it:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = asfn.get_slider_vals(date_picker)\n",
    "date_range[0] = date_range[0].date()\n",
    "date_range[1] = date_range[1].date()\n",
    "print(f\"Date Range: {str(date_range[0])} to {str(date_range[1])}\")\n",
    "project = asfn.filter_jobs_by_date(jobs, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Gather the available orbit directions for the remaining products:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"<text style='color:darkred;'><text style='font-size:150%;'>This may take some time for projects containing many jobs...</text></text>\"))\n",
    "project = asfn.get_paths_orbits(project)\n",
    "orbit_directions = set()\n",
    "for p in project:\n",
    "    orbit_directions.add(p.orbit_direction)\n",
    "display(Markdown(f\"<text style=color:blue><text style='font-size:175%;'>Done.</text></text>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Select an orbit direction:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(orbit_directions) > 1:\n",
    "    display(Markdown(\"<text style='color:red;'>Note: After selecting a flight direction, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:red;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "print('\\nSelect a Flight Direction:')\n",
    "direction_choice = asfn.select_parameter(orbit_directions, 'Direction:')\n",
    "direction_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Filter jobs by path and orbit direction:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = direction_choice.value\n",
    "project = asfn.filter_jobs_by_orbit(project, direction)\n",
    "print(f\"There are {len(project)} products to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Download the products, unzip them into the rtc_products directory, and delete the zip files:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_zips = jobs.download_files(ingram_folder)\n",
    "for z in project_zips:\n",
    "    asfn.asf_unzip(ingram_folder, str(z))\n",
    "    z.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\">\n",
    "    <font size=\"5\"> <b> 2. Prepare the Tiffs for Processing</b> </font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>2.0 Gather Amplitude, Coherence, and Interferogram Paths</b></font><br><br>\n",
    "    <font size='3'><b>2.0.0 Write functions to grab and print the path information for the amplitude, unwrapped phase, and the coherence files.</b></font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_paths(paths):\n",
    "    tiff_paths = !ls $paths | sort -t_ -k5,5\n",
    "    return tiff_paths\n",
    "\n",
    "def print_tiff_paths(tiff_paths):\n",
    "    print(\"Tiff paths:\")\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.0.1 Call the functions we just wrote to gather the file path information.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_wild_path    = f\"{ingram_folder}/*/*_amp.tif\"\n",
    "ingram_wild_path = f\"{ingram_folder}/*/*_unw_phase.tif\"\n",
    "corr_wild_path   = f\"{ingram_folder}/*/*_corr.tif\"\n",
    "amp_paths    = get_tiff_paths(amp_wild_path)\n",
    "ingram_paths = get_tiff_paths(ingram_wild_path)\n",
    "corr_paths    = get_tiff_paths(corr_wild_path)\n",
    "# print(f\"amp_path[0]: {amp_paths[0]}\")\n",
    "# print(f\"ingram_path[0]: {ingram_paths[0]}\")\n",
    "# print(f\"cohr_path[0]: {cohr_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.1.0 Write functions to filter product paths by orbit direction and file type</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interferogram_headings(metadata_paths: list) -> dict:\n",
    "    headings = {}\n",
    "    for path in metadata_paths:\n",
    "        with open(path, 'r') as metadata:\n",
    "            for line in metadata:\n",
    "                if 'Heading' in line:\n",
    "                    headings.update({os.path.basename(path): float(line.split(':')[1])})\n",
    "                    break\n",
    "    return headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.1.1 Use heading directions from the product metadata to find the average heading and pickle it. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of paths to interferogram metadata\n",
    "metadata_paths = []\n",
    "for path in amp_paths:\n",
    "    metadata_paths.append(path.replace('_amp.tif', '.txt'))\n",
    "    \n",
    "# Create a dictionary of interferogram headings \n",
    "# and find the average heading\n",
    "headings = get_interferogram_headings(metadata_paths)\n",
    "heading_avg = np.mean(list(headings.values()))\n",
    "print(f'Average Heading: {heading_avg}')\n",
    "to_pickle.update({'heading_avg': heading_avg})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.2 Project all data to the Same UTM Zone</b><br></font>\n",
    "<font size='3'><font face='Calibri'><font size='3'>Some of the geotiffs may use different UTM zones. In the code below, we will identify the predominate UTM zone and reproject the rest into that zone. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = amp_paths + ingram_paths + corr_paths\n",
    "print_tiff_paths(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Reproject all tiffs to the predominate UTM:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utm_zones_types(tiff_paths):\n",
    "    utm_zones = []\n",
    "    utm_types = []\n",
    "    print('Checking UTM Zones in the data stack ...\\n')\n",
    "    for i, path in enumerate(tiff_paths):\n",
    "        info = (gdal.Info(path, options = ['-json']))\n",
    "        info = json.dumps(info)\n",
    "        info = (json.loads(info))['coordinateSystem']['wkt']\n",
    "        zone = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "        utm_zones.append(zone)\n",
    "        typ = info.split('ID')[-1].split('\"')[1]\n",
    "        utm_types.append(typ)\n",
    "    return utm_zones, utm_types\n",
    "\n",
    "utm_zones, utm_types = get_utm_zones_types(tiff_paths)\n",
    "print(f\"Unique UTM Zones: {list(set(utm_zones))}\")\n",
    "print(f\"Unique UTM Types: {list(set(utm_types))}\\n\")\n",
    "\n",
    "utm_unique, counts = np.unique(utm_zones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "predominant_utm = utm_unique[a][0]\n",
    "print(f\"Predominant UTM Zone: {predominant_utm}\")\n",
    "print(f\"Number of UTM Zones:  {len(utm_unique)}\")\n",
    "to_pickle.update({'utm': predominant_utm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_indicies = [i for i, j in enumerate(utm_zones) if j != predominant_utm]\n",
    "print('--------------------------------------------')\n",
    "print('Reprojecting %4.1f files' %(len(reproject_indicies)))\n",
    "print('--------------------------------------------')\n",
    "for k in reproject_indicies:\n",
    "    temppath = tiff_paths[k].strip()\n",
    "    temppath = temppath.split('/')\n",
    "    product_name = temppath[-2]\n",
    "    tiff_name = temppath[-1]\n",
    "    cmd = f\"gdalwarp -overwrite {products_path}/{product_name}/{tiff_name}\"\\\n",
    "          f\" {products_path}/{product_name}/r{tiff_name} -s_srs {utm_types[k]}:\"\\\n",
    "          f\"{utm_zones[k]} -t_srs EPSG:{predominant_utm}\"\n",
    "    !$cmd\n",
    "    rm_cmd = f\"rm {tiff_paths[k].strip()}\"\n",
    "    !$rm_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.3 Mosaic Geotiffs with Partial Coverage</b><br></font>\n",
    "    <font size='3'>Merge multiple frames of the same type and from the same date into a single geotiff. This code makes the assumption that any imagery taken on the same day are frames that do not overlap the same areas. For Sentinel1 imagery, this holds true; for other data sources, it may not. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.3.0 Get the paths for the files to be merged.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: revisit merging after we determine the correct criteria fro safely merging InSAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update paths after reprojecting\n",
    "\n",
    "unw_wild_path  = f\"{ingram_folder}/*/*_unw_phase.tif\"\n",
    "\n",
    "amp_paths    = get_tiff_paths(amp_wild_path)\n",
    "corr_paths    = get_tiff_paths(corr_wild_path)\n",
    "unw_paths  = get_tiff_paths(unw_wild_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='5'> <b> 3. Subset Area of Interest</b> </font>\n",
    "    <br>\n",
    "    <font size='3'> Here we identify our area of interest (AOI). Our AOI must contain all of the expected deformation and a surrounding region of little to no deformation. Following our selection of this region, we will subset our data to this region. This helps reduce computation time. </font>\n",
    "    <br><br>\n",
    "    <font size='4'><b>3.0 Identify AOI</b></font> <br><br>\n",
    "    <font size='3'><b>3.0.0 Update the paths to the amplitude data after merging in the previous section</b></font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: grab updated paths when merging is working\n",
    "\n",
    "# amp_paths    = get_tiff_paths(amp_wild_path)\n",
    "# corr_paths    = get_tiff_paths(corr_wild_path)\n",
    "# unw_paths  = get_tiff_paths(unw_wild_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size=\"3\"> <b>3.0.1 Merge together one image from each area represented in the stack for display in the AOI selector:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = {}\n",
    "for pth in amp_paths:\n",
    "    info = (gdal.Info(pth, options = ['-json']))\n",
    "    info = json.dumps(info)\n",
    "    info = (json.loads(info))['wgs84Extent']['coordinates']\n",
    "    \n",
    "    coords = [info[0][0], info[0][3]]\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 2):\n",
    "            coords[i][j] = round(coords[i][j])\n",
    "    str_coords = f\"{str(coords[0])}{str(coords[1])}\"\n",
    "    if str_coords not in to_merge:\n",
    "        to_merge.update({str_coords: pth})\n",
    "merge_paths = \"\"\n",
    "for pth in to_merge:\n",
    "    merge_paths = f\"{merge_paths} {to_merge[pth]}\"  \n",
    "print(merge_paths)\n",
    "\n",
    "full_scene = f\"{analysis_directory}/full_scene.tif\"\n",
    "if os.path.exists(full_scene):\n",
    "    os.remove(full_scene)\n",
    "gdal_command = f\"gdal_merge.py -o {full_scene} {merge_paths}\"\n",
    "print(gdal_command)\n",
    "!{gdal_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.2 Create a VRT (virtual raster stack) from merged images and convert that into array:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = f\"{analysis_directory}/raster_stack.vrt\"\n",
    "!gdalbuildvrt -separate $image_file -overwrite $full_scene\n",
    "\n",
    "img = gdal.Open(image_file)\n",
    "rasterstack = img.ReadAsArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.3 Create an AOI selector from your raster stack:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig_xsize = 7.5\n",
    "fig_ysize = 7.5\n",
    "aoi = asfn.AOI_Selector(rasterstack, fig_xsize, fig_ysize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.4 Gather and define projection details:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotrans = img.GetGeoTransform()\n",
    "projlatlon = pyproj.Proj('EPSG:4326') # WGS84\n",
    "projimg = pyproj.Proj(f'EPSG:{predominant_utm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.5 Write a function to convert the pixel, line coordinates from the AOI selector into geographic coordinates in the stack's EPSG projection:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geolocation(x, y, geotrans,latlon=True):\n",
    "    ref_x = geotrans[0]+x*geotrans[1]\n",
    "    ref_y = geotrans[3]+y*geotrans[5]\n",
    "    if latlon:\n",
    "        ref_y, ref_x = pyproj.transform(projimg, projlatlon, ref_x, ref_y)\n",
    "    return [ref_x, ref_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.6 Call geolocation to gather the aoi_coords:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aoi_coords = [geolocation(aoi.x1, aoi.y1, geotrans, latlon=False), geolocation(aoi.x2, aoi.y2, geotrans, latlon=False)]\n",
    "    print(f\"aoi_coords in EPSG {predominant_utm}: {aoi_coords}\")\n",
    "except TypeError:\n",
    "    print('TypeError')\n",
    "    Markdown(f'<text style=color:red>This error occurs if an AOI was not selected.</text>')\n",
    "    Markdown(f'<text style=color:red>Note that the square tool icon in the AOI selector menu is <b>NOT</b> the selection tool. It is the zoom tool.</text>')\n",
    "    Markdown(f'<text style=color:red>Read the tips above the AOI selector carefully.</text>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b> 3.1 Subset (Crop) Data to Area of Interest </b> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'>We now subset our data to our AOI. We must do this for both the interferograms and the coherence files. In this lab, we will also subset the amplitude image files for later display purposes, though this is not necessary.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = amp_paths + unw_paths + corr_paths\n",
    "print(\"Subsetting amplitude, coherence, and interferogram files.\")\n",
    "for p in tiff_paths:\n",
    "    gdal_command = (f\"gdal_translate -epo -eco -projwin {aoi_coords[0][0]} \"\n",
    "                    f\"{aoi_coords[0][1]} {aoi_coords[1][0]} {aoi_coords[1][1]} \"\n",
    "                    f\"-projwin_srs 'EPSG:{predominant_utm}' -co \\\"COMPRESS=DEFLATE\\\" \"\n",
    "                    f\"-a_nodata 0 {p} {subset_folder}/{os.path.basename(p)} > /dev/null\")\n",
    "\n",
    "#     print(f\"\\nCalling the command: {gdal_command}\") \n",
    "    !{gdal_command} # Call the GDAL command. \n",
    "print(\"Subsetting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>3.2 Perform some checks on the subset data</b></font>\n",
    "<br>  \n",
    "<font size='3'><b>3.2.0 Check that the Subsetted Tiffs have Pixels</b></font>\n",
    "<br>\n",
    "<font size='3'>Some of the subsetted geotiffs do not have pixels in our AOI despite the \"-epo\" and \"-eco\" options which should cause an error for all of these and skip them. Below, we will <b>check which geotiffs actually have pixels in our AOI and remove those that don't.</b> This can be done with a simple NaN search or by checking the band statistics of the file. </font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_wild_path = f\"{subset_folder}/*.tif\"\n",
    "subset_paths = get_tiff_paths(subset_wild_path)\n",
    "asfn.remove_nan_filled_tifs(subset_folder, subset_paths) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.2.1 Check the dimensions of the subset tiffs</b>\n",
    "<br>\n",
    "In some instances, the gdal_translate function may return subset imagery with slightly different extents. They may differ in size by a single pixel width in the x or y dimension. This is usually only a problem when multiple data sensors are used. Sensors will often have different pixel sizes and/or their pixel locations will be slightly offset from each other. Since all of our data comes from Sentinel-1, we probably don't need to worry but it is still good to double check.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_paths = get_tiff_paths(subset_wild_path)\n",
    "subset_paths.sort()\n",
    "print(f\"Number of subset tifs: {len(subset_paths)}\")\n",
    "    \n",
    "def get_pixels_lines(geotiff_paths: list) -> list:\n",
    "    pixels = []\n",
    "    lines = []\n",
    "    for file in geotiff_paths:\n",
    "        img = gdal.Open(file)\n",
    "        pixels.append(img.RasterXSize)\n",
    "        lines.append(img.RasterYSize)\n",
    "    return {'pixels': set(pixels), 'lines': set(lines)}\n",
    "\n",
    "\n",
    "dimensions = get_pixels_lines(subset_paths)\n",
    "print(f\"Dimensions: {dimensions}\")\n",
    "if len(dimensions['pixels']) > 1 or len(dimensions['lines']) > 1:\n",
    "    print(\"Error: Not all subset images in stack have the same dimensions\")\n",
    "else:\n",
    "    print(\"Check successful. All subset images in stack have the same dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size='5'> <b> 4. Create Input Files and Code for TRAIN </b> </font>\n",
    "<br>\n",
    "<font size='3'> In this section, we will use TRAIN to remove static atmospheric effects that can cause decoherence of the interferograms. This primarily corrects for effects caused by elevation differences between locations. <br>If we think of the propogating radar wave as a set of discrete rays, each ray will follow a different path. Those that reflect from elevated locations will pass through less of the atmosphere and therefore be less altered than those rays that reflect from locations at lower elevations. Without this correction, interferograms often produce exaggerated deformation. This is especially important in hazard monitoring of active volcanoes as a false alarm can be extremely costly and cause the general public to ignore future warnings. \n",
    "</font>\n",
    "    \n",
    "</font>\n",
    "\n",
    "<font face='Calibri'>\n",
    "    <font size='3'>Let's create the input files and modify the files as required by TRAIN. The necessary items and actions are listed below. <br>\n",
    "        \n",
    "- parms_aps.txt\n",
    "    - List of parameters defining how TRAIN will run.\n",
    "- ifgday.txt\n",
    "    - Text file listing the primary and secondary date pairs. \n",
    "    - 2 column vector [primary secondary]\n",
    "    - Format: YYYYMMDD\n",
    "- Convert subsetted '.tif' Files to GCS Coordinates\n",
    "    - TRAIN requires the input '.tif' files to have a global coordinate system. \n",
    "    - We will convert them to EPSG:4326. \n",
    "- Adjust File Names\n",
    "    - Many SAR codes expect the input files to have a particular name format. \n",
    "    - For TRAIN, this is <font face='Courier New'>&lt;primary\\_date&gt;\\_&lt;secondary\\_date&gt;\\_&lt;unwrapped, amplitude, or coherence designation&gt;.tif</font>.\n",
    "<br></font>\n",
    "    </font>\n",
    "    \n",
    "<font face='Calibri'>\n",
    "    <font size='4'> <b>4.0 Create parms_aps.txt file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>We will need to create the file 'parms_aps.txt'. TRAIN will read parameters from this file. We will first need to extract some information from the satellite metadata files. We'll start with getting the UTC time of the satellite pass over our study area. <br>\n",
    "        \n",
    "</font>\n",
    "    </font>\n",
    "    \n",
    "    \n",
    "<font face='Calibri'>\n",
    "<font size='3'><b>4.0.0 Extract the UTC Time</b>\n",
    "<br>\n",
    "This information is contained in the file name.<br>\n",
    "Alternatively, it can be found in the metadata file named <font face='Courier New'>$<$primary timestamp$>$_$<$secondary timestamp$>$.txt</font> that comes with the interferogram.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUTC_sat(paths):\n",
    "    utc_hr = []\n",
    "    utc_min = []\n",
    "    utc_sec = []\n",
    "    for p in paths:\n",
    "        tstamp = asfn.date_from_product_name(p).split('_')[0].split('T')[1]\n",
    "        utc_hr.append(int(tstamp[0:2]))\n",
    "        utc_min.append(int(tstamp[2:4]))\n",
    "        utc_sec.append(int(tstamp[4:6]))    \n",
    "    \n",
    "    # UTC time as HH:MIN; we extract the median value and pad with up to 2 zeroes. \n",
    "    utc_sat = (f\"{str(int(np.median(utc_hr))).zfill(2)}:\"\n",
    "               f\"{str(int(np.median(utc_min))).zfill(2)}\")\n",
    "    # UTC time as an integer; Method from Tom Logan's prepGIAnT code\n",
    "    # Can also be found inside <date>_<date>.txt file and hard coded/extracted\n",
    "    c_l_utc = np.median(utc_hr)*3600 + np.median(utc_min)*60 + np.median(utc_sec) \n",
    "    return utc_sat, c_l_utc\n",
    "\n",
    "unw_wild_path  = f\"{subset_folder}/*_unw_phase.tif\"\n",
    "unw_paths = get_tiff_paths(unw_wild_path)\n",
    "\n",
    "print(\"Unwrapped Phase Tifs:\")\n",
    "for file in unw_paths: \n",
    "    print(file)\n",
    "    \n",
    "utc_sat, c_l_utc = getUTC_sat(unw_paths)\n",
    "print(f\"\\nUTC in int form: {c_l_utc}\")\n",
    "print(f\"UTC in human-readable form: {utc_sat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>4.0.1 Create parms_aps.txt file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>Make the parms_aps.txt file. This gives TRAIN information on how to process the data. </font>\n",
    "<br><br>\n",
    "<font color='red'>Note: Ignore the deprecation warnings generated by this code cell. This is happening under-the-hood in pyproj's geopandas calls and will go away with future updates. It is a known issue, not something we can control, and will not impact results.</font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = pyproj.Transformer.from_crs(f\"epsg:{predominant_utm}\", \"epsg:4326\")\n",
    "llx, lly = transformer.transform(coords[0][0],coords[0][1])\n",
    "urx, ury = transformer.transform(coords[1][0],coords[1][1])\n",
    "region_lats = abs(np.diff([lly,ury])[0]) + extra\n",
    "region_lons = abs(np.diff([llx,urx])[0]) + extra\n",
    "\n",
    "!mkdir -p {merra2_datapath} # create the directory\n",
    "\n",
    "parms_aps_Template = '''\n",
    "# Input parameters for TRAIN\n",
    "\n",
    "crop_flag: n\n",
    "date_origin: file\n",
    "dem_null: -32768\n",
    "DEM_origin: asf\n",
    "DEM_file: {7}\n",
    "era_data_type: {1}\n",
    "ifgday_file: {2}\n",
    "incidence_angle: {9}\n",
    "lambda: {8}\n",
    "look_angle: 21\n",
    "meric_perc_coverage: 80\n",
    "merra2_datapath: {4}\n",
    "non_defo_flag: n\n",
    "region_lat_range: {5}\n",
    "region_lon_range: {6}\n",
    "region_res: 0.008333000000000\n",
    "save_folder_name: aps_estimation\n",
    "small_baseline_flag: n\n",
    "stamps_processed: n\n",
    "UTC_sat: {3}\n",
    "\n",
    "'''\n",
    "\n",
    "with open(os.path.join(train_dir,'parms_aps.txt'), 'w') as fid:\n",
    "    fid.write(parms_aps_Template.format(train_dir, era_data_type,\n",
    "                                        ifgday_file, utc_sat,\n",
    "                                        merra2_datapath, region_lats,\n",
    "                                        region_lons, dem_path,\n",
    "                                        lambda_meters, incidence_angle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>You may notice that the <font face='Courier New'>parms_aps.txt</font> file we created has very little geographic information; we've only included the width and height of the subsetted interferograms in decimal degrees (the variables <font face='Courier New'>region_lats</font> and <font face='Courier New'>region_lons</font>). This is because we will use one of our subsetted and converted tiffs as a geographic reference file. Otherwise, we would need to create a text file that contains the latitude and longitude of each pixel in the interferogram.</font>\n",
    "<br><br>  \n",
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.1 Create ifgday.mat file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>This gives TRAIN the primary and secondary dates of the interferograms as 2 column vectors. The dates must be separated by a single space.<br>\n",
    "        \n",
    "- Interferogram dates stored as a matrix with name ifgday and size [n_ifgs 2]. \n",
    "- Primary image is in the first and secondary is in the second column. \n",
    "- Specify dates as a numeric value in YYYYMMDD format.\n",
    "\n",
    "    </font>\n",
    "    </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_from_product_name(product_name: str) -> str:\n",
    "    regex = \"[0-9]{8}T[0-9]{6}_[0-9]{8}T[0-9]{6}\"\n",
    "    results = re.search(regex, product_name)\n",
    "    if results:\n",
    "        return results.group(0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the reference and secondary dates. \n",
    "ref_dates = []\n",
    "sec_dates = []\n",
    "for p in unw_paths:\n",
    "    tstamps = dates_from_product_name(p) \n",
    "    ref_dates.append(tstamps.split('_')[0].split('T')[0])\n",
    "    sec_dates.append(tstamps.split('_')[1].split('T')[0])\n",
    "    \n",
    "# Sort the dates according to the primary dates. \n",
    "ref_dates, sec_dates = (list(t) for t in zip(*sorted(zip(ref_dates, sec_dates))))\n",
    "\n",
    "# write values to the 'ifgday_file'; make sure there is only 1 space between the dates.\n",
    "with open( os.path.join(train_dir, ifgday_file), 'w') as ifg:\n",
    "    for i, r_date in enumerate(ref_dates):\n",
    "        ifg.write(f'{r_date} {sec_dates[i]}\\n') \n",
    "\n",
    "# print the contents of the file we just wrote\n",
    "with open(os.path.join(train_dir, ifgday_file),'r') as ifg:\n",
    "    ifg_contents = ifg.read()\n",
    "print(ifg_contents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.2 Convert Subsetted '.tif' Files to GCS Coordinates</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>The ASF version of TRAIN requries our subsets to be in GCS coordinates (i.e., for every pixel in the subset to be designated by latitude and longitude). Currently, they are in a UTM coordinate system, which gives pixel location in meters based on a local coordinate system. We now will reproject our subsets to GCS.\n",
    "</font>\n",
    "<br><br>\n",
    "<font face='Calibri' size='3'><b>4.2.0 Define the projection to which we will be reprojecting the subsets. This is typically designated as EPSG:4326.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_TRAIN = '4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.2.1 Create the converted subsets.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in unw_paths:\n",
    "    out_file = os.path.join(corrected_folder, os.path.basename(p))\n",
    "    cmd = f\"gdalwarp -t_srs EPSG:{coord_TRAIN} {p} {out_file}\"\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.3 Adjust File Names</b></font>\n",
    "    <font size='3'><br>TRAIN expects the file naming format, <font face='Courier New'>&lt;reference_date&gt;_&lt;secondary_date&gt;_&lt;unwrapped, amplitude, or coherence designation&gt;.tif</font>. We will adjust the files to match this name convention. The code below assumes that the files all come from Sentinel-1 and that every interferogram has a unique primary and secondary date pair. <br><b>This is not always true; some interferograms will have identical primary/secondary date pairs, but have been taken at different times.</b> This is a relatively rare occurrence, but it is good to keep in mind. To keep this exercise relatively simple, we assume each interferogram has a unique primary/secondary date pair.</font>\n",
    "<br><br>\n",
    "<b>4.3.0 Write a function to rename the files</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_for_train(data_dir, paths): \n",
    "    for p in paths:\n",
    "        tstamps = dates_from_product_name(p) \n",
    "        ref_date = tstamps.split('_')[0].split('T')[0]\n",
    "        sec_date = tstamps.split('_')[1].split('T')[0]\n",
    "        if \"_unw\" in p:\n",
    "            new_name = f\"{ref_date}_{sec_date}_unw_phase.tif\"\n",
    "        elif \"_corr\" in p:\n",
    "            new_name = f\"{ref_date}_{sec_date}_corr.tif\"\n",
    "        elif \"_amp\" in p:\n",
    "            new_name = f\"{ref_date}_{sec_date}_amp.tif\"\n",
    "        \n",
    "        exists = os.path.isfile(os.path.join(data_dir, new_name))\n",
    "        if exists:\n",
    "            print(f\"File already exists: {new_name}\")\n",
    "        else:\n",
    "            os.rename(os.path.join(data_dir, p),\n",
    "                      os.path.join(data_dir, new_name))\n",
    "    print(\"Files renamed.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.3.1 Rename the subset files and save a list of the new filenames.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(subset_folder) if f.endswith('.tif')] \n",
    "files.sort()\n",
    "\n",
    "rename_files_for_train(subset_folder, files)\n",
    "files_subset = [ f for f in os.listdir(subset_folder) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.3.2 Rename the converted files and save a list of the new filenames.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(corrected_folder) if f.endswith('.tif')] \n",
    "files.sort()\n",
    "\n",
    "rename_files_for_train(corrected_folder, files)\n",
    "files_converted = [ f for f in os.listdir(corrected_folder) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>5. Run TRAIN</b></font>\n",
    "<br>\n",
    "<font size='3'>We have now created all of the necessary files to run TRAIN. \n",
    "<br><br>\n",
    "<font face='Calibri'><font size='4'><b>5.0 Minor Set Up</b></font>\n",
    "<br>\n",
    "<font size='3'>We have to set up some path information in order to run TRAIN. <i>Eventually, this will be modified so we don't have to include the full path to TRAIN.</i> Additionally, we show multiple ways in which to call TRAIN.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/jovyan/.local/TRAIN/src\"\n",
    "aps_weather_model_path = os.path.join(train_path,'aps_weather_model.py')\n",
    "print(f\"aps_weather_model.py path: {aps_weather_model_path}\")\n",
    "georef_path = os.path.join(analysis_directory, corrected_folder, files_converted[0])\n",
    "print(f\"\\nPath to a georeference file: {georef_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$train_path/aps_weather_model.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.1 TRAIN Steps 0-3</b></font>\n",
    "    <br>\n",
    "    <font size='3'>Now we run steps 0 through 3. Step 4 requires a few extra actions. \n",
    "    <br><br>\n",
    "        <b>Step 0: Enter your Earthdata credentials</b>\n",
    "        <br>\n",
    "        TRAIN requires a .netrc holding your Earthdata credentials. To avoid permanently storing user secrets in OpenSARlab, we will create a .netrc, make a TRAIN call, and delete the .netrc, all in the same code cell.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = input(\"Enter your Earthdata username: \")\n",
    "password = getpass(\"Enter your Earthdata password: \")\n",
    "netrc_path = '/home/jovyan/.netrc'\n",
    "\n",
    "def write_dot_netrc(path, username, password):\n",
    "    with open(netrc_path, 'w+') as netrc:\n",
    "        netrc.write(f'machine urs.earthdata.nasa.gov login {username} password {password}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Step 0: Identify weather data files to download.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dot_netrc(netrc_path, username, password)\n",
    "\n",
    "!$train_path/aps_weather_model.py -g {georef_path} 0 0\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Step 1: Download weather data.</b>\n",
    "<br>\n",
    "This will download a series of '*.nc4' files for each primary.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any existing MERRA2 downloads to prevent old files from being used. \n",
    "try:\n",
    "    shutil.rmtree(merra2_datapath)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "write_dot_netrc(netrc_path, username, password)\n",
    "\n",
    "!$train_path/aps_weather_model.py -g {georef_path} 1 1\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Step 2: Calculate wet and hydrostatic zenith delays</b>\n",
    "<br>\n",
    "This will download a set of '*.xyz' files and use those \n",
    "to calculate the necessary delays.\n",
    "<br><br>\n",
    "Note: This step will error if the subset area being processed is too small. Unfortunately, the minimum area size does not appear to be documented. If you run into this issue, a little trial and error with subset sizes may be required. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dot_netrc(netrc_path, username, password)\n",
    "\n",
    "!$train_path/aps_weather_model.py -g {georef_path} 2 2\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Step 3 - Calculate the SAR delays</b>\n",
    "<br>\n",
    "This produces *_*_{hydro_correction, wet_correction, and correction}.bin files\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dot_netrc(netrc_path, username, password)\n",
    "\n",
    "!$train_path/aps_weather_model.py -g {georef_path} 3 3\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Step 3 may give a strange message: \"No correction for &lt;insert date&gt;\". If this occurs, it is most likely because TRAIN wasn't able to access the MERRA2 files due to missing permissions in your Earthdata user account. This can be done by going to your <a href=\"https://urs.earthdata.nasa.gov/profile\" target=\"_blank\">EarthData's profile page</a>, clicking <b>Applications</b> and selecting <b>Approved Applications</b> from the drop down menu. Select <b>Approve More Applications</b> at the bottom left, search for <b>NASA GESDISC DATA ARCHIVE</b>, select it, and agree to the terms and conditions. Once that is complete, restart the kernel in the kernel menu and rerun the notebook.</font></font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.2 TRAIN Step 4</b></font>\n",
    "<br>\n",
    "<font size='3'>In step 4, we apply the correction to our <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase.tif&gt;</font> files. \n",
    "<br><br>\n",
    "For this step, we need to do 2 things first:\n",
    "<ol>\n",
    "<li>Move all of the '*.bin' files into the same directory as our converted geotiffs.</li>\n",
    "<li>Move into the converted geotiffs directory.</li>\n",
    "</ol>\n",
    "<b>5.2.0 Create a list of our .bin files</b></font>     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bin_paths = glob.glob(\"*.bin\")\n",
    "bin_paths.sort()\n",
    "print(bin_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.2.1 Move the .bin files into the converted geotiffs directory and then move into that directory</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in bin_paths:\n",
    "    try:\n",
    "        shutil.move(p, os.path.join(corrected_folder, p))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError, {p} does not exist.\")\n",
    "        print(\"Have you already moved your .bin files?\")\n",
    "        break\n",
    "\n",
    "#TRAIN does not allow you to pass a path to the .bin files, so we must change directories\n",
    "try:        \n",
    "    os.chdir(f\"{analysis_directory}/{corrected_folder}\")\n",
    "except FileNotFoundError:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.2.2 Step 4 - Subtract calculated delay from interferograms</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dot_netrc(netrc_path, username, password)\n",
    "\n",
    "!$train_path/aps_weather_model.py -g {georef_path} 4 4\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.2.3 Return to the home directory for the remainder of this analysis</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(analysis_directory)\n",
    "except FileNotFoundError:\n",
    "    raise\n",
    "    \n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>If we check our <font face='Courier New'>corrected_folder</font> directory, we will find new files with the naming convention <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase\\_corrected.tif&gt;</font>. The uncorrected files, <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase.tif&gt;</font>, are now technically superfluous and can be deleted. However, we will keep these files for the purpose of comparing the corrected and uncorrected times series in Part 2.\n",
    "</font></font>\n",
    "<br><br>\n",
    "<font face='Calibri'><font size='4'><b>5.3 Comparison of Corrected and Uncorrected Unwrapped Phase</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We will make a quick and simple comparison between the corrected and uncorrected unwrapped phase geotiffs. This is meant to highlight the importance of these atmospheric corrections.\n",
    "<br><br>\n",
    "    <b>5.3.0 Gather the paths to the corrected and uncorrected unwrapped phase tifs</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_cor = f\"{analysis_directory}/{corrected_folder}/*_unw_phase_corrected.tif\"\n",
    "paths_unc = f\"{analysis_directory}/{corrected_folder}/*_unw_phase.tif\"\n",
    "cor_paths = get_tiff_paths(paths_cor)\n",
    "unc_paths = get_tiff_paths(paths_unc)\n",
    "\n",
    "# Uncomment to view paths\n",
    "# print_tiff_paths(cor_paths)\n",
    "# print_tiff_paths(unc_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.3.1 Plot a corrected and uncorrected unwrapped phase tif for comparison</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "corrected = gdal.Open(cor_paths[0])\n",
    "uncorrected = gdal.Open(unc_paths[0])\n",
    "im_c = corrected.GetRasterBand(1).ReadAsArray()\n",
    "im_u = uncorrected.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax1.imshow(im_c, cmap='gray')\n",
    "ax2.imshow(im_u, cmap='gray')\n",
    "plt.title('Corrected and Uncorrected Unwrapped Phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.3.2 Plot a difference map of the two images</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = np.subtract(im_c, im_u)\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "fig_plot = ax1.imshow(difference, cmap='RdBu')\n",
    "cbar = fig.colorbar(fig_plot, fraction=0.24, pad=0.02)\n",
    "cbar.ax.set_xlabel('mm')\n",
    "cbar.ax.set_ylabel('Correction Difference', rotation=270, labelpad=20)\n",
    "ax1.set(title='TRAIN Correction Difference Map [mm]')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>In looking at the Correction Difference Map, we can see that size of the correction correlates with surface elevation. Without this correction, a volcanologist would see the extra elevation difference as an indication of magma injection and possible eruptive activity.</font></font>\n",
    "<br>\n",
    "\n",
    "<font face='Calibri'><font size='4'><b>5.4 Convert back to original coordinate system</b></font>\n",
    "<br>\n",
    "<font size='3'>GIAnT requires the interferograms to be in a particular coordinate system. The original coordinate system is one of those accepted, so we will convert our interferograms back to that.\n",
    "<br><br>\n",
    "<b>5.4.0 Display our EPSGs</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"original coordiante system = EPSG:{predominant_utm}\")\n",
    "print(f\"TRAIN coordinate system =    EPSG:{coord_TRAIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.1 Grab the paths to all the corrected tiffs</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = f\"{corrected_folder}/*.tif\"\n",
    "tiff_paths = get_tiff_paths(paths)\n",
    "\n",
    "# Uncomment to view paths\n",
    "print_tiff_paths(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.2 check that the current coordinate system of the files is different from the desired.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones, utm_types = get_utm_zones_types(tiff_paths)\n",
    "print(f\"Current UTM Types & Zones = EPSG:{list(set(utm_zones))}\")\n",
    "print(f\"Expected current system   = EPSG:{coord_TRAIN}\")\n",
    "print(f\"Desired coordinate system = EPSG:{predominant_utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.3 Reproject the corrected tiffs.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tiff_paths:\n",
    "    # Designate the output file and its path; ideally these are the same. \n",
    "    # GDAL can't do that (it'll overwrite data sometimes), so for each  \n",
    "    # we create new file with the correct projection, delete the old file, \n",
    "    # and then rename the newly created file. \n",
    "    base_path ,filename = os.path.split(file)\n",
    "    desig = 'TEMP_'\n",
    "    outfile = os.path.join(base_path, f\"{desig}{filename}\")\n",
    "    # create the convert command\n",
    "    cmd = f\"gdalwarp -t_srs EPSG:{predominant_utm} {file} {outfile}\"\n",
    "    !{cmd} # convert the file\n",
    "    \n",
    "    # delete the file in the EPSG:4326 coordinate system\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # rename the file in the utm coordinate system to our original name of 'inFile'. \n",
    "    try:\n",
    "        os.rename(outfile, file)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.4 Confirm that the files are now in the correct projection</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones, utm_types = get_utm_zones_types(tiff_paths)\n",
    "print(f\"Current UTM Types & Zones = EPSG:{list(set(utm_zones))}\")\n",
    "print(f\"Expected current system   = EPSG:{predominant_utm}\")\n",
    "print(f\"Desired coordinate system = EPSG:{predominant_utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.5 Take care of some final details</b></font>\n",
    "    <br><br>\n",
    "<font size='3'><b>5.5.0 Do another pixel check</b></font>\n",
    "<br>\n",
    "<font size='3'>Check the pixel sizes again, and then do the pixel correction if necessary.\n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels_lines = get_pixels_lines(tiff_paths)\n",
    "print(pixels_lines)\n",
    "if len(pixels_lines['pixels']) == 1 and \\\n",
    "len(pixels_lines['lines']) == 1:\n",
    "    print(\"All images in the stack have the same dimensions\")\n",
    "else:\n",
    "    print(\"Error: Images with different dimensions exist in the stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.5.1 Pickle the information we need to access in the Part 2 notebook</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_pickle)\n",
    "filename = 'part1_pickle'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(to_pickle, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>\n",
    "You have now corrected the interferograms for atmospheric conditions and can proceed to Part 2: GIAnT. \n",
    "</font></font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'><b>Print the path to the analysis directory, which you can copy/paste into Part 2</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657-Lab9-InSARTimeSeriesAnalysis-Part1-DataDownload-HyP3_v2.ipynb - Version 0.0.1 - May 2021\n",
    "    </i>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
