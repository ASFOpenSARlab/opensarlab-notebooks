{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OpenSARlab notebook banner](NotebookAddons/blackboard-banner.png)\n",
    "\n",
    "# Lab 9: InSAR Time Series Analysis using GIAnT within Jupyter Notebooks\n",
    "\n",
    "### Franz J Meyer & Joshua J C Knicely; University of Alaska Fairbanks\n",
    "\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" />\n",
    "\n",
    "This Lab is part of the UAF course [GEOS 657: Microwave Remote Sensing](https://radar.community.uaf.edu/). The primary goal of this lab is to demonstrate how to process InSAR data, specifically interferograms, using the Generic InSAR Analysis Toolbox [GIAnT](http://earthdef.caltech.edu/projects/giant/wiki) in the framework of *Jupyter Notebooks*.\n",
    "\n",
    "**Our specific objectives for this lab are to:**\n",
    "\n",
    "- Learn how to prepare data for GIAnT. \n",
    "- Use GIAnT to create maps of surface deformation. \n",
    "    -  Understand its capabilities. \n",
    "    -  Understand its limitations. \n",
    "\n",
    "## Target Description\n",
    "\n",
    "In this lab, we will analyze the volcano Sierra Negra. This is a highly active volcano on the Galapagos hotpsot. The most recent eruption occurred from 29 June to 23 August 2018. The previous eruption occurred in October 2005, prior to the launch of the Sentinel-1 satellites, which will be the source of data we use for this lab. We will be looking at the deformation that occurred prior to the volcano's 2018 eruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%javascript\n",
    "var kernel = Jupyter.notebook.kernel;\n",
    "var command = [\"notebookUrl = \",\n",
    "               \"'\", window.location, \"'\" ].join('')\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/insar_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"insar_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select \"insar_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"insar_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**About GIAnT**\n",
    "\n",
    "GIAnT is a Python framework that allows rapid time series analysis of low amplitude deformation signals. It allows users to use multiple time series analysis technqiues: Small Baseline Subset (SBAS), New Small Baseline Subset (N-SBAS), and Multiscale InSAR Time-Series (MInTS). As a part of this, it includes the ability to correct for atmospheric delays by assuming a spatially uniform stratified atmosphere. \n",
    "\n",
    "**Limitations**\n",
    "\n",
    "GIAnT has a number of limitations that are important to keep in mind as these can affect its effectiveness for certain applications. It implements the simplest time-series inversion methods. Its single coherence threshold is very conservative in terms of pixel selection. It does not include any consistency checks for unwrapping errors. It has a limited dictionary of temporal model functions. It cannot correct for atmospheric effects due to differing surface elevations. \n",
    "\n",
    "**Steps to use GIAnT**\n",
    "Although GIAnT is an incredibly powerful tool, it requires very specific input. Because of the input requirements, the majority of one's effort goes to getting the data into a form that GIAnT can manipulate and to creating files that tell GIAnT what to do. The general steps to use GIAnT are below. \n",
    "\n",
    "- Download Data\n",
    "- Identify Area of Interest\n",
    "- Subset (Crop) Data to Area of Interest\n",
    "- Prepare Data for GIAnT\n",
    "    - Adjust file names\n",
    "    - Remove potentially disruptive default values (optional)\n",
    "    - Convert data from '.tiff' to '.flt' format\n",
    "- Create Input Files for GIAnT\n",
    "    - Create 'ifg.list'\n",
    "    - Create 'date.mli.par'\n",
    "    - Make prepxml_SBAS.py\n",
    "    - Run prepxml_SBAS.py\n",
    "    - Make userfn.py\n",
    "- Run GIAnT\n",
    "    - PrepIgramStack.py*\n",
    "    - ProcessStack.py\n",
    "    - SBASInvert.py\n",
    "    - SBASxval.py\n",
    "- Data Visualization\n",
    "\n",
    "The steps from PrepIgramStack.py and above have been completed for you in order to save disk space and computation time. This allows us to concentrate on the usage of GIAnT and data visualization. Some of the code to create the prepatory files (e.g., 'ifg.list', 'date.mli.par', etc.) have been incldued for your potential use. More information about GIAnT can be found [here](http://earthdef.caltech.edu/projects/giant/wiki)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note about JupyterHub**\n",
    "\n",
    "Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Python Libraries:\n",
    "\n",
    "**Import the Python libraries and modules we will need to run this lab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py # for is_hdf5\n",
    "import os # need os for context switch\n",
    "import shutil\n",
    "\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import opensarlab_lib as asfn\n",
    "asfn.jupytertheme_matplotlib_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transfer data to a local directory\n",
    "\n",
    "The data cube (referred to as a stack in the GIAnT documentation and code) and several other needed files have been created and stored in the GIAnT server. We will download this data to a local directory and unzip it.\n",
    "\n",
    "Before we download anything, **create a working directory for this analysis and change into it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_path = Path.cwd()/'lab_9_data'\n",
    "if not work_path.exists():\n",
    "    work_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to find the zip file and download it to a local directory. This zip file has been placed in the S3 bucket for this class.\n",
    "\n",
    "**Display the contents of the S3 bucket:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --region=us-east-1 --no-sign-request s3://asf-jupyter-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy the desired file ('Lab9Files.zip') to your data directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --region=us-east-1 --no-sign-request s3://asf-jupyter-data/Lab9Files.zip $work_path/Lab9Files.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the directories where we will perform the GIAnT analysis and store the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_path = work_path/'Stack'\n",
    "\n",
    "if not stack_path.exists():\n",
    "    stack_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the zipped file to path and delete it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_path = work_path/'Lab9Files.zip'\n",
    "asfn.asf_unzip(str(work_path), str(zipped_path))\n",
    "\n",
    "if zipped_path.is_file:\n",
    "    zipped_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files have been extracted and placed in a folder called 'Lab9Files'. \n",
    "\n",
    "**Move the amplitude image, data.xml, date.mli.par, and sbas.xml files to path and RAW-STACK.h5 to stack_path:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(temp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = work_path/'Lab9Files'\n",
    "\n",
    "# Move 'RAW-STACK.h5' to 'Stack' directory\n",
    "if not Path(f'{stack_path}/RAW-STACK.h5').exists():\n",
    "    shutil.move(f'{temp_dir}/RAW-STACK.h5', stack_path)\n",
    "    (temp_dir/'RAW-STACK.h5').unlink()\n",
    "\n",
    "# Move rest of content into directory above\n",
    "files = list(temp_dir.rglob(\"*.*\"))\n",
    "for file in files:\n",
    "    if file.exists():\n",
    "        try:\n",
    "            file.move(work_path/f\"{file.name}\")\n",
    "        except:\n",
    "            file.unlink()\n",
    "        \n",
    "# Remove empty 'Lab9Files' directory\n",
    "if temp_dir.exists():\n",
    "    temp_dir.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Input Files And Code for GIAnT\n",
    "\n",
    "The code below shows how to create the input files and specialty code that GIAnT requires. For this lab, 'ifg.list' is not needed, 'date.mli.par' has already been provided, 'prepxml_SBAS.py' is not needed as the 'sbas.xml' and 'data.xml' files it would create have already been provided, and 'userfn.py' is not needed as we are skipping the step in which it would be used. <br>The files that would be created are listed below. \n",
    "   \n",
    "- ifg.list\n",
    "    - List of the interferogram properties including master and slave date, perpendicular baseline, and sensor. \n",
    "- date.mli.par\n",
    "    - File from which GIAnT pulls requisite information about the sensor. \n",
    "    - This is specifically for GAMMA files. When using other interferogram processing techniques, an alternate file is required. \n",
    "- prepxml_SBAS.py\n",
    "    - Python function to create an xml file that specifies the processing options to GIAnT. \n",
    "    - This must be modified by the user for their particular application. \n",
    "- userfn.py\n",
    "    - Python function to map the interferogram dates to a phyiscal file on disk. \n",
    "    - This must be modified by the user for their particular application. \n",
    "    \n",
    "### 2.1 Create 'ifg.list' File\n",
    "\n",
    "This will create simple 4 column text file will communicate network information to GIAnT. It will be created within the **GIAnT** folder.\n",
    "\n",
    "**This step has already been done, so we will not actually create the 'ifg.list' file. This code is displayed for your potential future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Get one of each file name. This assumes the unwrapped phase geotiff has been converted to a '.flt' file\n",
    "files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')] \n",
    "\n",
    "# Get all of the master and slave dates. \n",
    "masterDates,slaveDates = [],[]\n",
    "for file in files:\n",
    "    masterDates.append(file[0:8])\n",
    "    slaveDates.append(file[9:17])\n",
    "# Sort the dates according to the master dates. \n",
    "master_dates,sDates = (list(t) for t in zip(*sorted(zip(masterDates,slaveDates))))\n",
    "\n",
    "with open( os.path.join('GIAnT', 'ifg.list'), 'w') as fid:\n",
    "    for i in range(len(master_dates)):\n",
    "        masterDate = master_dates[i] # pull out master Date (first set of numbers)\n",
    "        slaveDate = sDates[i] # pull out slave Date (second set of numbers)\n",
    "        bperp = '0.0' # according to JPL notebooks\n",
    "        sensor = 'S1' # according to JPL notebooks\n",
    "        fid.write(f'{masterDate}  {slaveDate}  {bperp}  {sensor}\\n') # write values to the 'ifg.list' file. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the code above sets the perpendicular baseline to a value of 0.0 m. This is not the true perpendicular baseline. That value can be found in metadata file (titled '$<$master timestamp$>$_$<$slave timestamp$>$.txt') that comes with the original interferogram. Generally, we would want the true baseline for each interferogram. However, since Sentinel-1 has such a short baseline, a value of 0.0 m is sufficient for our purposes.\n",
    "\n",
    "### 2.2 Create 'date.mli.par' File\n",
    "\n",
    "As we are using GAMMA products, we must create a 'date.mli.par' file from which GIAnT will pull necessary information. If another processing technique is used to create the interferograms, an alterante file name and file inputs are required.\n",
    "\n",
    "**Again, this step has already been completed and the code is only displayed for your potential future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create file 'date.mli.par'\n",
    "\n",
    "# Get file names\n",
    "files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')]\n",
    "\n",
    "# Get WIDTH (xsize) and FILE_LENGTH (ysize) information\n",
    "ds = gdal.Open(datadirectory+files[0], gdal.GA_ReadOnly)\n",
    "type(ds)\n",
    "\n",
    "nLines = ds.RasterYSize\n",
    "nPixels = ds.RasterXSize\n",
    "\n",
    "trans = ds.GetGeoTransform()\n",
    "ds = None\n",
    "\n",
    "# Get the center line UTC time stamp; can also be found inside <date>_<date>.txt file and hard coded\n",
    "dirName = os.listdir('ingrams')[0] # get original file name (any file can be used; the timestamps are different by a few seconds)\n",
    "vals = dirName.split('-') # break file name into parts using the separator '-'\n",
    "tstamp = vals[2][9:16] # extract the time stamp from the 2nd datetime (could be the first)\n",
    "c_l_utc = int(tstamp[0:2])*3600 + int(tstamp[2:4])*60 + int(tstamp[4:6])\n",
    "\n",
    "rfreq = 299792548.0 / 0.055465763 # radar frequency; speed of light divided by radar wavelength of Sentinel1 in meters\n",
    "\n",
    "# write the 'date.mli.par' file\n",
    "with open(os.path.join(path, 'date.mli.par'), 'w') as fid:\n",
    "    # Method 1\n",
    "    fid.write(f'radar_frequency: {rfreq} \\n') # when using GAMMA products, GIAnT requires the radar frequency. Everything else is in wavelength (m) \n",
    "    fid.write(f'center_time: {c_l_utc} \\n') # Method from Tom Logan's prepGIAnT code; can also be found inside <date>_<date>.txt file and hard coded\n",
    "    fid.write( 'heading: -11.9617913 \\n') # inside <date>_<date>.txt file; can be hardcoded or set up so code finds it. \n",
    "    fid.write(f'azimuth_lines: {nLines} \\n') # number of lines in direction of the satellite's flight path\n",
    "    fid.write(f'range_samples: {nPixels} \\n') # number of pixels in direction perpendicular to satellite's flight path\n",
    "    fid.close() # close the file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Make prepxml_SBAS.py\n",
    "\n",
    "We will create a prepxml_SBAS.py function and put it into our GIAnT working directory. Again, this is shown for anyone that may want to use GIAnT on their own.\n",
    "\n",
    "If we do wish to change `sbas.xml` or `data.xml`, this can be done by creating and running a new 'prepxml_SBAS.py'.\n",
    "\n",
    "#### 2.3.1 Necessary prepxml_SBAS.py edits\n",
    "\n",
    "GIAnT comes with an example prepxml_SBAS.py, but requries significant edits for our purposes. These alterations have already been made, so we don't have to do anything now, but it is good to know the kinds of things that have to be altered. The details of some of these options can be found in the GIAnT documentation. The rest must be found in the GIAnT processing files themselves, most notably the tsxml.py and tsio.py functions.\n",
    "\n",
    "**The following alterations were made:**\n",
    "\n",
    "- Changed 'example' &#9658; 'date.mli.par'\n",
    "- Removed 'xlim', 'ylim', 'ref_x_lim', and 'ref_y_lim'\n",
    "    - These are used for clipping the files in GIAnT. As we have already done this, it is not necessary. \n",
    "- Removed latfile='lat.map' and lonfile='lon.map'\n",
    "    - These are optional inputs for the latitude and longitude maps. \n",
    "- Removed hgtfile='hgt.map'\n",
    "    - This is an optional altitude file for the sensor. \n",
    "- Removed inc=21.\n",
    "    - This is the optional incidence angle information. \n",
    "    - It can be a constant float value or incidence angle file. \n",
    "    - For Sentinel1, it varies from 29.1-46.0&deg;.\n",
    "- Removed masktype='f4'\n",
    "    - This is the mask designation. \n",
    "    - We are not using any masks for this. \n",
    "- Changed unwfmt='RMG' &#9658; unwfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Removed demfmt='RMG'\n",
    "- Changed corfmt='RMG' &#9658; corfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Changed nvalid=30 -> nvalid=1\n",
    "    - This is the minimum number of interferograms in which a pixel must be coherent. A particular pixel will be included only if its coherence is above the coherence threshold, cohth, in more than nvalid number of interferograms. \n",
    "- Removed atmos='ECMWF'\n",
    "    - This is an amtospheric correction command. It depends on a library called 'pyaps' developed for GIAnT. This library has not been installed yet. \n",
    "- Changed masterdate='19920604' &#9658; masterdate='20161119'\n",
    "    - Use our actual masterdate. \n",
    "    - I simply selected the earliest date as the masterdate. \n",
    "    \n",
    "---\n",
    "\n",
    "Defining a reference region is a potentially important step. This is a region at which there should be no deformation. For a volcano, this should be some significant distance away from the volcano. GIAnT has the ability to automatically select a reference region which we will use for this exercise. \n",
    "\n",
    "Below is an example of how the reference region would be defined. If we look at the prepxml_SBAS.py code below, ref_x_lim and ref_y_lim, the pixel based location of the reference region, is within the code, but has been commented out. \n",
    "\n",
    "**Define reference region:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_x_lim, ref_y_lim = [0, 10], [95, 105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the reference region would be defined. Look at the prepxml_SBAS.py code below. Note that ref_x_lim and ref_y_lim (the pixel based location of the reference region) are within the code.\n",
    "\n",
    "**This has already been completed but the code is here as an example script for creating XML files for use with the SBAS processing chain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    'date.mli.par', proc='GAMMA', \n",
    "    #ref_x_lim = [{1},{2}], ref_y_lim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt=1.0)\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the master date and create a script for creating XML files for use with the SBAS processing chain:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')]\n",
    "#master_date = min([files[i][0:8] for i in range(len(files))], key=int)\n",
    "\n",
    "master_date = '20161119'\n",
    "\n",
    "prepxml_SBAS_Template = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Example script for creating XML files for use with the SBAS processing chain. This script is supposed to be copied to the working directory and modified as needed.\"\"\"\n",
    "\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    'date.mli.par', proc='GAMMA', \n",
    "    #ref_x_lim = [{1},{2}], ref_y_lim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt=1.0)\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''\n",
    "\n",
    "# with open(work_path.joinpath('prepxml_SBAS.py'), 'w') as fid:\n",
    "with open(work_path/'prepxml_SBAS.py', 'w') as fid:\n",
    "    fid.write(prepxml_SBAS_Template.format(work_path,ref_x_lim[0],ref_x_lim[1],ref_y_lim[0],ref_y_lim[1],master_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new 'sbas.xml' and 'data.xml' file, we would modify the above code to give new parameters and to write to the appropriate folder (e.g., to change the time filter from 1 year to none and to write to the directory in which we are working; 'filt=1.0' -> 'filt=0.0'; and 'os.path.join(path,'prepxml_SBAS.py') -> 'prepxml_SBAS.py' OR '%cd ~' into your home directory). Then we would run it below.\n",
    "\n",
    "### 2.4 Run prepxml_SBAS.py\n",
    "\n",
    "Here we run `prepxml_SBAS.py` to create the 2 needed files:\n",
    "\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "\n",
    "To use MinTS, we would run `prepxml_MinTS.py` to create:\n",
    "\n",
    "- data.xml\n",
    "- mints.xml\n",
    "        \n",
    "These files are needed by `PrepIgramStack.py`. \n",
    "\n",
    "We must first switch to the GIAnT folder in which `prepxml_SBAS.py` is contained, then call it. Otherwise, `prepxml_SBAS.py` will not be able to find the file 'date.mli.par', which holds necessary processing information. \n",
    "\n",
    "**Create a variable holding the general path to the GIAnT code base and download GIAnT from the asf-jupyter-data S3 bucket, if not present.**\n",
    "\n",
    "GIAnT is no longer supported (Python 2). This unofficial version of GIAnT has been partially ported to Python 3 to run this notebook. Only the portions of GIAnT used in this notebook have been tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_path = \"/home/jovyan/.local/GIAnT/SCR\"\n",
    "\n",
    "if not Path('/home/jovyan/.local/GIAnT').exists():\n",
    "    download_path = 's3://asf-jupyter-data/GIAnT_5_21.zip'\n",
    "    output_path = f'/home/jovyan/.local/{Path(download_path).name}'\n",
    "\n",
    "    !aws --region=us-east-1 --no-sign-request s3 cp $download_path $output_path\n",
    "    \n",
    "    if Path(output_path).is_file():        \n",
    "        !unzip $output_path -d /home/jovyan/.local/\n",
    "        Path(output_path).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run prepxml_SBAS.py and check the output to confirm that your input values are correct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python $giant_path/prepxml_SBAS.py # this has already been done. data.xml and sbas.xml already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure the two requisite xml files (data.xml and sbas.xml) were produced after running prepxml_SBAS.py.**\n",
    "\n",
    "**Display the contents of data.xml:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xml_path = work_path/'data.xml'\n",
    "\n",
    "if data_xml_path.exists():\n",
    "    !cat $data_xml_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the contents of sbas.xml:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbas_xml_path = work_path/'sbas.xml'\n",
    "\n",
    "if sbas_xml_path.exists():\n",
    "    ! cat $sbas_xml_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create userfn.py\n",
    "\n",
    "Before running the next piece of code, `PrepIgramStack.py`, we must create a python file called `userfn.py`. This file maps the interferogram dates to a physical file on disk. This python file must be in our working directory, `/GIAnT/SCR`. We can create this file from within the notebook using python. \n",
    "\n",
    "**Again, this step has already been preformed and is unnecessary, but the code is provided as an example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userfnTemplate = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "# import os \n",
    "\"\"\"\n",
    "def makefnames(dates1, dates2, sensor):\n",
    "    dirname = '{0}'\n",
    "    root = os.path.join(dirname, dates1+'-'+dates2)\n",
    "    #unwname = root+'_unw_phase.flt' # for potentially disruptive default values kept. \n",
    "    unwname = root+'_unw_phase_no_default.flt' # for potentially disruptive default values removed. \n",
    "    corname = root+'_corr.flt'\n",
    "    return unwname, corname\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# with open(f'{giant_path}/userfn.py', 'w') as fid:\n",
    "#     fid.write(userfnTemplate.format(work_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(giant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run GIAnT\n",
    "\n",
    "We have now created all of the necessary files to run GIAnT. The full GIAnT process requires 3 function calls.\n",
    "\n",
    "- PrepIgramStack.py\n",
    "    - After PrepIgramStack.py, we will actually start running GIAnT. \n",
    "- ProcessStack.py\n",
    "- SBASInvert.py\n",
    "- SBASxval.py\n",
    "    - This 4th function call is not necessary and we will skip it, but provides some error estimation that can be useful.\n",
    "    \n",
    "### 3.1 Run PrepIgramStack.py\n",
    "\n",
    "Here we would run `PrepIgramStack.py` to create the files for GIAnT. This would read in the input data and the files we previously created and output an HDF5 file. As we do not actually need to call this, it is currently set up to display some help information.\n",
    "\n",
    "Inputs:       \n",
    "- ifg.list\n",
    "- data.xml\n",
    "- sbas.xml  \n",
    "- interferograms\n",
    "- coherence files        \n",
    "\n",
    "Outputs:\n",
    "- RAW-STACK.h5\n",
    "- PNG previews under 'GIAnT/Figs/Igrams'\n",
    "\n",
    "**Display some help information for PrepIgramStack.py:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/PrepIgramStack.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run PrepIgramStack.py (in our case, this has already been done):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with asfn.work_dir(work_path):\n",
    "#     !python $giant_path/PrepIgramStack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "PrepIgramStack.py creates a file called 'RAW-STACK.h5'.\n",
    "\n",
    "**Verify that RAW-STACK.h5 is an HDF5 file as required by the rest of GIAnT.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_h5 = f\"{stack_path}/RAW-STACK.h5\"\n",
    "if not h5py.is_hdf5(raw_h5):\n",
    "    print(f\"Not an HDF5 file: {raw_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {raw_h5} is an HDF5 file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run ProcessStack.py\n",
    "\n",
    "This seems to be an optional step. Does atmospheric corrections and estimation of orbit residuals.\n",
    "\n",
    "Inputs:\n",
    "\n",
    "- HDF5 files from PrepIgramStack.py, RAW-STACK.h5\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "- GPS Data (optional; we don't have this)\n",
    "- Weather models (downloaded automatically)\n",
    "\n",
    "Outputs: \n",
    "\n",
    "- HDF5 files, PROC-STACK.h5\n",
    "        \n",
    "These files are then fed into SBAS. \n",
    "\n",
    "**Display the help information for ProcessStack.py:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/ProcessStack.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run ProcessStack.py:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with asfn.work_dir(work_path):\n",
    "    !python $giant_path/ProcessStack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ProcessStack.py creates a file called 'PROC-STACK.h5'.\n",
    "\n",
    "**Verify that PROC-STACK.h5 is an HDF5 file as required by the rest of GIAnT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_h5 = f\"{stack_path}/PROC-STACK.h5\"\n",
    "if not h5py.is_hdf5(proc_h5):\n",
    "    print(f\"Not an HDF5 file: {proc_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {proc_h5} is an HDF5 file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run SBASInvert.py\n",
    "\n",
    "Invert the time series. \n",
    " \n",
    "Inputs\n",
    "\n",
    "- HDF5 file, PROC-STACK.h5\n",
    "- data.xml\n",
    "- sbas.xml\n",
    "\n",
    "Outputs\n",
    "\n",
    "- HDF5 file: LS-PARAMS.h5\n",
    "\n",
    "**Display the help information for SBASInvert.py:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/SBASInvert.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run SBASInvert.py:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with asfn.work_dir(work_path):\n",
    "    !python $giant_path/SBASInvert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "SBASInvert.py creates a file called 'LS-PARAMS.h5'.\n",
    "\n",
    "**Verify that LS-PARAMS.h5 is an HDF5 file as required by the rest of GIAnT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_h5 = f\"{stack_path}/LS-PARAMS.h5\"\n",
    "if not h5py.is_hdf5(params_h5):\n",
    "    print(f\"Not an HDF5 file: {params_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {params_h5} is an HDF5 file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Run SBASxval.py\n",
    "\n",
    "Get an uncertainty estimate for each pixel and epoch using a Jacknife test. We are skipping this function as we won't be doing anything with its output and it takes a significant amount of time to run relative to the other GIAnT functions.\n",
    " \n",
    "Inputs: \n",
    "\n",
    "- HDF5 files, PROC-STACK.h5\n",
    "- data.xml\n",
    "- sbas.xml\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- HDF5 file, LS-xval.h5\n",
    "\n",
    "**Display the help information for SBASxval.py:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python $giant_path/SBASxval.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run SBASxval.py:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with asfn.work_dir(work_path):\n",
    "#     !python $giant_path/SBASxval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "SBASxval.py  creates a file called 'LS-xval.h5'.\n",
    "\n",
    "**Verify that LS-xval.h5 is an HDF5 file as required by the rest of GIAnT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "xval_h5 = f\"{stack_path}/LS-xval.h5\"\n",
    "if not h5py.is_hdf5(xval_h5):\n",
    "    print(f\"Not an HDF5 file: {xval_h5}\")\n",
    "else:\n",
    "    print(f\"Confirmed: {xval_h5} is an HDF5 file.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "Now we visualize the data. This is largely copied from Lab 4.\n",
    "\n",
    "**Create a directory in which to store our plots and move into it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = work_path/'plots'\n",
    "\n",
    "if not plot_dir.exists():\n",
    "    plot_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the stack produced by GIAnT and read it into an array so we can manipulate and display it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(params_h5, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List all groups ('key's) within the HDF5 file that has been loaded into the object 'f'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys: %s\" %f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details on what each of these keys means can be found in the GIAnT documentation. For now, the only keys with which we are concerned are `recons` (the filtered time series of each pixel) and `dates` (the dates of acquisition). It is important to note that the dates are given in a type of Julian Day number called Rata Die number. This will have to be converted later, but this can easily be done via one of several different methods in Python.\n",
    "\n",
    "**Get our data from the stack:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cube = f['recons'][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the dates for each raster from the stack:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(f['dates']) # these dates appear to be given in Rata Die style: floor(Julian Day Number - 1721424.5). \n",
    "if data_cube.shape[0] is not len(dates):\n",
    "    print('Problem:')\n",
    "    print('Number of rasters in data_cube: ',data_cube.shape[0])\n",
    "    print('Number of dates: ',len(dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot and save amplitude image with transparency determined by alpha (SierraNegra-dBScaled-AmplitudeImage.png):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "radar_tiff = f\"{work_path}/20161119-20170106_amp.tiff\"\n",
    "radar=gdal.Open(radar_tiff)\n",
    "im_radar = radar.GetRasterBand(1).ReadAsArray()\n",
    "radar = None\n",
    "dbplot = np.ma.log10(im_radar)\n",
    "vmin=np.percentile(dbplot,3)\n",
    "vmax=np.percentile(dbplot,97)\n",
    "fig = plt.figure(figsize=(18,10)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(111) # 221 determines: 2 rows, 2 plots, first plot\n",
    "ax1.imshow(dbplot, cmap='gray',vmin=vmin,vmax=vmax,alpha=1);\n",
    "plt.title('Example dB-scaled SAR Image for IfgrmÂ 20161119-20170106')\n",
    "plt.grid()\n",
    "plt.savefig(f'{plot_dir}/SierraNegra-dBScaled-AmplitudeImage.png',dpi=200,transparent='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display and save an overlay of the clipped deformation map and amplitude image (SierraNegra-DeformationComposite.png):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a short function that can plot an overaly of our radar image and deformation map. \n",
    "def defNradar_plot(deformation, radar):\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    vmin = np.percentile(radar, 3)\n",
    "    vmax = np.percentile(radar, 97)\n",
    "    ax.imshow(radar, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    fin_plot = ax.imshow(deformation, cmap='RdBu', vmin=-50.0, vmax=50.0, alpha=0.75)\n",
    "    fig.colorbar(fin_plot, fraction=0.24, pad=0.02)\n",
    "    ax.set(title=\"Integrated Defo [mm] Overlain on Clipped db-Scaled Amplitude Image\")\n",
    "    plt.grid()\n",
    "    \n",
    "# Get deformation map and radar image we wish to plot\n",
    "deformation = data_cube[data_cube.shape[0]-1]\n",
    "\n",
    "# Call function to plot an overlay of our deformation map and radar image.\n",
    "defNradar_plot(deformation, dbplot)\n",
    "plt.savefig(f'{plot_dir}/SierraNegra-DeformationComposite.png', dpi=200, transparent='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert from Rata Die number (similar to Julian Day number) contained in 'dates' to Gregorian date:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tindex = []\n",
    "for d in dates:\n",
    "    tindex.append(date.fromordinal(int(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an animation of the deformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis('off')\n",
    "vmin=np.percentile(data_cube.flatten(), 5)\n",
    "vmax=np.percentile(data_cube.flatten(), 95)\n",
    "\n",
    "\n",
    "im = ax.imshow(data_cube[0], cmap='RdBu', vmin=-50.0, vmax=50.0)\n",
    "ax.set_title(\"Animation of Deformation Time Series - Sierra Negra, Galapagos\")\n",
    "fig.colorbar(im)\n",
    "plt.grid()\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"Date: {}\".format(tindex[i]))\n",
    "    im.set_data(data_cube[i])\n",
    "    \n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=data_cube.shape[0], interval=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure matplotlib's RC settings for the animation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('animation', embed_limit=10.0**9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a javascript animation of the time-series running inline in the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the animation as a 'gif' file (SierraNegraDeformationTS.gif):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(f'{plot_dir}/SierraNegraDeformationTS.gif', writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alter the time filter parameter\n",
    "\n",
    "Looking at the video above, you may notice that the deformation has a very smoothed appearance. This may be because of our time filter which is currently set to 1 year ('filt=1.0' in the prepxml_SBAS.py code). Let's repeat the lab from there with 2 different time filters.\n",
    "\n",
    "First, using no time filter ('filt=0.0') and then using a 1 month time filter ('filt=0.082'). Change the output file name for anything you want saved (e.g., 'SierraNegraDeformationTS.gif' to 'YourDesiredFileName.gif'). Otherwise, it will be overwritten.\n",
    "\n",
    "- How did these changes affect the output time series?\n",
    "- How might we figure out the right filter length?\n",
    "- What does this say about the parameters we select?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clear data (optional)\n",
    "\n",
    "This lab has produced a large quantity of data. If you look at this notebook in your home directory, it should now be ~13 MB. This can take a long time to load in a Jupyter Notebook. It may be useful to clear the cell outputs. \n",
    "\n",
    "To clear the cell outputs, go Cell->All Output->Clear. This will clear the outputs of the Jupyter Notebook and restore it to its original size of ~60 kB. This will not delete any of the files we have created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GEOS 657-Lab9-InSARTimeSeriesAnalysis.ipynb - Version 1.3.3 - November 2021*\n",
    "\n",
    "*Version Changes:*\n",
    "\n",
    "- *asf_notebook -> opensarlab_lib*\n",
    "- *html -> markdown*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar_analysis",
   "language": "python",
   "name": "conda-env-.local-insar_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
