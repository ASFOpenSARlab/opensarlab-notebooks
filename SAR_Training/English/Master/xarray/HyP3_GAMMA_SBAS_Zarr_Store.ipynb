{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OpenSARlab notebook banner](NotebookAddons/blackboard-banner.png)\n",
    "\n",
    "# Prepare an SBAS stack as a Zarr Store for a MintPy SBAS Analysis\n",
    "### Alex Lewandowski; Alaska Satellite Facility\n",
    "\n",
    "<img style=\"padding: 7px\" src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\"/></font>\n",
    "\n",
    "**This notebook:**\n",
    "1. Creates 2 xarray.Datasets\n",
    "    1. sbas (x, y, insar-pairs)\n",
    "    1. geometry (x, y)\n",
    "2. Saves them to a Zarr store in an S3 bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing Relevant Python Packages\n",
    "\n",
    "In this notebook we will use the following scientific libraries:\n",
    "\n",
    "1. [GDAL](https://www.gdal.org/) is a software library for reading and writing raster and vector geospatial data formats. It includes a collection of programs tailored for geospatial data processing. Most modern GIS systems (such as ArcGIS or QGIS) use GDAL in the background.\n",
    "1. [NumPy](http://www.numpy.org/) is one of the principal packages for scientific applications of Python. It is intended for processing large multidimensional arrays and matrices, and an extensive collection of high-level mathematical functions and implemented methods makes it possible to perform various operations with these objects.\n",
    "\n",
    "**Our first step is to import them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import copy\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json # for loads\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from tqdm.auto import tqdm \n",
    "from typing import Union\n",
    "import warnings\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import pycrs\n",
    "import s3fs\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import yaml\n",
    "import zarr\n",
    "\n",
    "from mintpy.constants import SPEED_OF_LIGHT\n",
    "from mintpy.objects import sensor\n",
    "from mintpy.utils import readfile\n",
    "\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "import asf_search\n",
    "from hyp3_sdk import Batch, HyP3\n",
    "import opensarlab_lib as osl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Your HyP3 GAMMA SBAS Stack\n",
    "\n",
    "This notebook assumes that you've created an SBAS stack using the [Alaska Satellite Facility's](https://www.asf.alaska.edu/) value-added product system HyP3, available via [ASF Data Search (Vertex)](https://search.asf.alaska.edu/#/). HyP3 is an ASF service used to prototype value added products and provide them to users to collect feedback.\n",
    "\n",
    "We will retrieve HyP3 data via the hyp3_sdk or work with previously downloaded data. As both HyP3 and the Notebook environment sit in the [Amazon Web Services (AWS)](https://aws.amazon.com/) cloud, data transfer is quick and cost effective.\n",
    "\n",
    "---\n",
    "\n",
    "If downloading data, create a data directory in which to download interferograms in your SBAS stack.\n",
    "\n",
    "<!-- If working with a previously downloaded SBAS stack, each product should contain VH and VV  or HH and HV data, the HyP3 log file, and the HyP3 product README in subdirectories of the data directory: -->\n",
    "\n",
    "```\n",
    "data_directory   \n",
    "│\n",
    "└───interferogram_1_directory\n",
    "│   │   *_unw_phase.tif\n",
    "│   │   *_corr.tif\n",
    "│   │   *_amp.tif\n",
    "│   │   *_lv_theta.tif\n",
    "│   │   *_lv_phi.tif\n",
    "│   │   *_dem.tif\n",
    "│   │   *.README.md.txt\n",
    "│   │   *.txt\n",
    "│   ...\n",
    "│   \n",
    "└───interferogram_2_directory\n",
    "│   │   *_unw_phase.tif\n",
    "│   │   *_corr.tif\n",
    "│   │   *_amp.tif\n",
    "│   │   *_lv_theta.tif\n",
    "│   │   *_lv_phi.tif\n",
    "│   │   *_dem.tif\n",
    "│   │   *.README.md.txt\n",
    "│   │   *.txt\n",
    "│   ...\n",
    "│ \n",
    "...\n",
    "```\n",
    "\n",
    "**Select the directory holding your data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Select your data directory\")\n",
    "fc = FileChooser(Path.cwd())\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = Path(fc.selected_path)\n",
    "\n",
    "print(f\"data_directory: {data_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather the paths to the DEMs, unwrapped-phase, amplitude, look vector, and spatial coherence rasters, plus the README and log files for each interferogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems = list(data_directory.glob('*/*_dem.tif'))\n",
    "dems.sort()\n",
    "\n",
    "unw_phases = list(data_directory.glob('*/*_unw_phase.tif'))\n",
    "unw_phases.sort()\n",
    "\n",
    "amps = list(data_directory.glob('*/*_amp.tif'))\n",
    "amps.sort()\n",
    "\n",
    "lv_thetas = list(data_directory.glob('*/*_lv_theta.tif'))\n",
    "lv_thetas.sort()\n",
    "\n",
    "lv_phis = list(data_directory.glob('*/*_lv_phi.tif'))\n",
    "lv_phis.sort()\n",
    "\n",
    "corrs = list(data_directory.glob('*/*_corr.tif'))\n",
    "corrs.sort()\n",
    "\n",
    "metadata = list(data_directory.glob('*/*.README.md.txt'))\n",
    "metadata.sort()\n",
    "\n",
    "logs = list(data_directory.glob('*/*.txt'))\n",
    "log_regex = re.compile('.*_\\w{4}.txt')\n",
    "logs = [Path(pth) for pth in filter(log_regex.match, [str(p) for p in logs])]\n",
    "logs.sort()\n",
    "\n",
    "s = [dems, unw_phases, amps, lv_thetas, lv_phis, corrs, metadata, logs]\n",
    "\n",
    "stack_paths = [{'dem': s[0][i],\n",
    "                'unw_phase': s[1][i], \n",
    "                'amp': s[2][i], \n",
    "                'lv_theta': s[3][i],\n",
    "                'lv_phi': s[4][i], \n",
    "                'corr': s[5][i], \n",
    "                'metadata': s[6][i], \n",
    "                'log': s[7][i]} for i in range(0, len(dems))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write some functions to:\n",
    "\n",
    "- gather metadata\n",
    "- calculate spatial extents\n",
    "- find the padding required for each raster to fit into an Xarray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corners_gdal(file):\n",
    "    ds=gdal.Open(str(file))\n",
    "    transform = ds.GetGeoTransform()\n",
    "    x = ds.RasterXSize\n",
    "    y = ds.RasterYSize\n",
    "    \n",
    "    ulx = transform[0]\n",
    "    uly = transform[3]\n",
    "    lrx = transform[0] + x * transform[1]\n",
    "    lry = transform[3] + y * transform[5]\n",
    "    return [ulx, uly, lrx, lry]\n",
    "\n",
    "def pad_array(coords, pixel_size, left_pad_pixels=0, right_pad_pixels=0):  \n",
    "    left_pad = left_pad_pixels * pixel_size\n",
    "    right_pad = right_pad_pixels * pixel_size\n",
    "    increasing = coords[0] < coords[-1]\n",
    "    \n",
    "    if increasing:\n",
    "        left_coords = np.arange(start=coords[0]-left_pad, stop=coords[0], step=pixel_size)\n",
    "        right_coords = np.arange(start=coords[-1]+pixel_size, stop=coords[-1]+pixel_size+right_pad, step=pixel_size)\n",
    "    else:\n",
    "        left_coords = np.arange(start=coords[0]+left_pad, stop=coords[0], step=-pixel_size)\n",
    "        right_coords = np.arange(start=coords[-1]-pixel_size, stop=coords[-1]-pixel_size-right_pad, step=-pixel_size)      \n",
    "        \n",
    "    coords = np.append(left_coords, coords)\n",
    "    return np.append(coords, right_coords)\n",
    "\n",
    "def get_padding_to_match_xyxy_bbox(tiff, bbox):\n",
    "        f = gdal.Open(str(tiff))\n",
    "        pixel_size = f.GetGeoTransform()[1]\n",
    "        corners = get_corners_gdal(tiff)\n",
    "        \n",
    "        left_x_pad_pixels = int(abs(bbox[0] - corners[0]) / pixel_size)\n",
    "        right_x_pad_pixels = int((abs(bbox[2] - corners[2]) + pixel_size) / pixel_size)\n",
    "        \n",
    "        upper_y_pad_pixels = int(abs(bbox[1] - corners[1]) / pixel_size)\n",
    "        lower_y_pad_pixels = int((abs(bbox[3] - corners[3]) + pixel_size) / pixel_size)\n",
    "        return {\n",
    "            \"x_pad\" : [left_x_pad_pixels, right_x_pad_pixels],\n",
    "            \"y_pad\": [upper_y_pad_pixels, lower_y_pad_pixels]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsg(path: Union[str, Path]) -> str:\n",
    "    \"\"\"\n",
    "    returns the EPSG of a geotiff\n",
    "    \"\"\"\n",
    "    info = gdal.Info(str(path), format='json')\n",
    "    return info['coordinateSystem']['wkt'].split('ID')[-1].split(',')[1][0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mintpy_attrs(attrs):\n",
    "    mintpy_attrs = {\n",
    "        # ROI_PAC/MintPy attributes\n",
    "        'ALOOKS'             : attrs['Azimuth_looks'],\n",
    "        'RLOOKS'             : attrs['Range_looks'],\n",
    "        'AZIMUTH_PIXEL_SIZE' : sensor.SEN['azimuth_pixel_size'] * int(attrs['Azimuth_looks']),\n",
    "        'RANGE_PIXEL_SIZE'   : sensor.SEN['range_pixel_size'] * int(attrs['Range_looks']),\n",
    "        'ANTENNA_SIDE'       : -1,\n",
    "        'CENTER_LINE_UTC'    : attrs['UTC_time'],\n",
    "        'DATA_TYPE'          : attrs['DATA_TYPE'],\n",
    "        'EARTH_RADIUS'       : attrs['Earth_radius_at_nadir'],\n",
    "        'HEADING'            : attrs['Heading'],\n",
    "        'HEIGHT'             : attrs['Spacecraft_height'],\n",
    "        'BANDS'              : attrs['BANDS'],\n",
    "        'INTERLEAVE'         : attrs['INTERLEAVE'],\n",
    "        'LENGTH'             : attrs['LENGTH'],\n",
    "        'ORBIT_DIRECTION'    : 'ASCENDING' if abs(float(attrs['Heading'])) < 90 else 'DESCENDING',\n",
    "        'NO_DATA_VALUE'      : attrs['NO_DATA_VALUE'],\n",
    "        'PLATFORM'           : 'Sen',\n",
    "        'POLARIZATION'       : None,\n",
    "        'PRF'                : None,\n",
    "        'Reference_Granule'  : attrs['Reference_Granule'],\n",
    "        'Secondary_Granule'  : attrs['Secondary_Granule'],\n",
    "        'STARTING_RANGE'     : None,\n",
    "        'UNIT'               : 'radian',\n",
    "        'WAVELENGTH'         : SPEED_OF_LIGHT / sensor.SEN['carrier_frequency'],\n",
    "        'WIDTH'              : attrs['WIDTH'],\n",
    "        # # from PySAR [MintPy<=1.1.1]\n",
    "        # 'REF_DATE'           : ['ref_date'],\n",
    "        # 'REF_LAT'            : ['ref_lat'],\n",
    "        # 'REF_LON'            : ['ref_lon'],\n",
    "        # 'REF_X'              : ['ref_x'],\n",
    "        # 'REF_Y'              : ['ref_y'],\n",
    "        # 'SUBSET_XMIN'        : ['subset_x0'],\n",
    "        # 'SUBSET_XMAX'        : ['subset_x1'],\n",
    "        # 'SUBSET_YMIN'        : ['subset_y0'],\n",
    "        # 'SUBSET_YMAX'        : ['subset_y1'],\n",
    "        \n",
    "        # from Gamma geo-coordinates - degree / meter\n",
    "        'X_FIRST'            : attrs['X_FIRST'],\n",
    "        'Y_FIRST'            : attrs['Y_FIRST'],\n",
    "        'X_STEP'             : attrs['X_STEP'],\n",
    "        'Y_STEP'             : attrs['Y_STEP'],\n",
    "\n",
    "        # # HDF-EOS5 attributes\n",
    "        # 'beam_swath'     : ['swathNumber'],\n",
    "        # 'first_frame'    : ['firstFrameNumber'],\n",
    "        # 'last_frame'     : ['lastFrameNumber'],\n",
    "        # 'relative_orbit' : ['trackNumber'],\n",
    "    }\n",
    "    \n",
    "    N = float(mintpy_attrs['Y_FIRST'])\n",
    "    W = float(mintpy_attrs['X_FIRST'])\n",
    "    S = N + float(mintpy_attrs['Y_STEP']) * int(mintpy_attrs['LENGTH'])\n",
    "    E = W + float(mintpy_attrs['X_STEP']) * int(mintpy_attrs['WIDTH'])\n",
    "\n",
    "    if mintpy_attrs['ORBIT_DIRECTION'] == 'ASCENDING':\n",
    "        mintpy_attrs['LAT_REF1'] = str(S)\n",
    "        mintpy_attrs['LAT_REF2'] = str(S)\n",
    "        mintpy_attrs['LAT_REF3'] = str(N)\n",
    "        mintpy_attrs['LAT_REF4'] = str(N)\n",
    "        mintpy_attrs['LON_REF1'] = str(W)\n",
    "        mintpy_attrs['LON_REF2'] = str(E)\n",
    "        mintpy_attrs['LON_REF3'] = str(W)\n",
    "        mintpy_attrs['LON_REF4'] = str(E)\n",
    "    else:\n",
    "        mintpy_attrs['LAT_REF1'] = str(N)\n",
    "        mintpy_attrs['LAT_REF2'] = str(N)\n",
    "        mintpy_attrs['LAT_REF3'] = str(S)\n",
    "        mintpy_attrs['LAT_REF4'] = str(S)\n",
    "        mintpy_attrs['LON_REF1'] = str(E)\n",
    "        mintpy_attrs['LON_REF2'] = str(W)\n",
    "        mintpy_attrs['LON_REF3'] = str(E)\n",
    "        mintpy_attrs['LON_REF4'] = str(W)\n",
    "        \n",
    "    timestamp_regex = \"(?<=_)\\d{8}T\\d{6}(?=_)\"\n",
    "    timestamps = re.findall(timestamp_regex, attrs['unw_phase_path'])\n",
    "    \n",
    "    date1 = datetime.strptime(timestamps[0],'%Y%m%dT%H%M%S')\n",
    "    date2 = datetime.strptime(timestamps[1],'%Y%m%dT%H%M%S')\n",
    "    mintpy_attrs['DATE12'] = f'{date1.strftime(\"%y%m%d\")}-{date2.strftime(\"%y%m%d\")}'\n",
    "    mintpy_attrs['P_BASELINE_TOP_HDR'] = attrs['Baseline']\n",
    "    mintpy_attrs['P_BASELINE_BOTTOM_HDR'] = attrs['Baseline']\n",
    "    \n",
    "    return mintpy_attrs\n",
    "        \n",
    "def get_insar_attrs(txt_path, unw_path):\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    attrs = {'_'.join((l.split(\":\")[0]).split(' ')):(l.split(': ')[1]).split('\\n')[0] for l in lines}\n",
    "    attrs['unw_phase_path'] = unw_path.name\n",
    "    attrs.update(readfile.read_gdal_vrt(unw_path))\n",
    "    return format_mintpy_attrs(attrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to turn a single interferogram into an Xarray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbas_to_xarray(insar_path, x_pad_pixels=[0,0], y_pad_pixels=[0,0]):   \n",
    "    '''\n",
    "    insar_path: dict of paths to insar rasters and metadata with keys:\n",
    "                'dem', 'unw_phase', 'amp', 'lv_theta', 'lv_phi', 'corr', 'metadata', 'log'\n",
    "    '''\n",
    "    \n",
    "    # get interferogram attrs\n",
    "    attrs = get_insar_attrs(insar_path['log'], insar_path['unw_phase'])\n",
    "    meta = readfile.read_gdal_vrt(insar_path['unw_phase'])\n",
    "    attrs.update(meta)\n",
    "    \n",
    "    ds=gdal.Open(str(insar_path['unw_phase']))\n",
    "    unw_phase_data = ds.GetRasterBand(1)\n",
    "    unw_phase_data = unw_phase_data.ReadAsArray()\n",
    "    unw_phase_data = np.ma.masked_invalid(unw_phase_data, copy=True)    \n",
    "    \n",
    "    ds=gdal.Open(str(insar_path['corr']))\n",
    "    corr_data = ds.GetRasterBand(1)\n",
    "    corr_data = corr_data.ReadAsArray()\n",
    "    corr_data = np.ma.masked_invalid(corr_data, copy=True)\n",
    "    \n",
    "    # get coordinate system projection\n",
    "    prj = ds.GetProjection()\n",
    "    crs = pycrs.parse.from_ogc_wkt(prj)\n",
    "    crs_proj = crs.proj.name.ogc_wkt.lower()  \n",
    "\n",
    "    # pixel resolution\n",
    "    geo_trans = ds.GetGeoTransform()\n",
    "    res_x = geo_trans[1]\n",
    "    res_y = geo_trans[5]\n",
    "    \n",
    "    #get corner coords and extents\n",
    "    corners = get_corners_gdal(str(insar_path['unw_phase']))\n",
    "    \n",
    "    \n",
    "    x_extent = [corners[0], corners[2]]\n",
    "    y_extent = [corners[1], corners[3]]\n",
    "    \n",
    "    # create x and y arrays based on extents, pixel resolution, and padding\n",
    "    x_coords = np.arange(x_extent[0], x_extent[1], res_x)\n",
    "    x_coords = pad_array(x_coords, res_x, left_pad_pixels=x_pad_pixels[0], right_pad_pixels=x_pad_pixels[1])\n",
    "                               \n",
    "    y_coords = np.arange(y_extent[0], y_extent[1], res_y)\n",
    "    y_coords = pad_array(y_coords, -res_y, left_pad_pixels=y_pad_pixels[0], right_pad_pixels=y_pad_pixels[1])\n",
    "      \n",
    "     \n",
    "    # create xarray dataset\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            'y': y_coords,\n",
    "            'x': x_coords,\n",
    "            'unw_phase': (\n",
    "                ('y', 'x'),\n",
    "                np.pad(unw_phase_data, pad_width=((y_pad_pixels[0],y_pad_pixels[1]), (x_pad_pixels[0],x_pad_pixels[1])), mode='constant',constant_values=(0.0)),\n",
    "            ),\n",
    "            'corr': (\n",
    "                ('y', 'x'),\n",
    "                np.pad(corr_data, pad_width=((y_pad_pixels[0],y_pad_pixels[1]), (x_pad_pixels[0],x_pad_pixels[1])), mode='constant',constant_values=(0.0)),\n",
    "            ),\n",
    "        },\n",
    "        attrs=None\n",
    "    )\n",
    "\n",
    "    # Set x and y coord attributes\n",
    "    attrs_x = {\n",
    "        'axis': 'X',\n",
    "        'units': 'dd',\n",
    "        'standard_name': 'projection_x_coordinate',\n",
    "        'long_name': 'longitude'\n",
    "    }\n",
    "    attrs_y = {\n",
    "        'axis': 'Y',\n",
    "        'units': 'dd',\n",
    "        'standard_name': 'projection_y_coordinate',\n",
    "        'long_name': 'latitude'\n",
    "    }\n",
    "    \n",
    "    for key in attrs_x:\n",
    "        ds.x.attrs[key] = attrs_x[key]\n",
    "    for key in attrs_y:\n",
    "        ds.y.attrs[key] = attrs_y[key]\n",
    "    \n",
    "    ds.rio.write_crs(attrs['EPSG'], inplace=True)\n",
    "    ds = ds.rio.reproject(\"EPSG:4326\", **{'nodata':0.0})\n",
    "    ds.rio.write_crs(4326, inplace=True)\n",
    "    del ds.unw_phase.attrs[\"_FillValue\"]\n",
    "    del ds.corr.attrs[\"_FillValue\"]\n",
    "    \n",
    "    attrs['X_FIRST'] = ds['x'][0]\n",
    "    attrs['Y_FIRST'] = ds['y'][0]\n",
    "    attrs['X_STEP'] = abs(ds['x'][-1] - ds['x'][0]) / len(ds['x'])\n",
    "    attrs['Y_STEP'] = abs(ds['y'][-1] - ds['y'][0]) / len(ds['y'])\n",
    "    attrs['X_UNIT'] = \"degrees\"\n",
    "    attrs['Y_UNIT'] = \"degrees\"\n",
    "    attrs['LENGTH'] = ds.y.shape[0]\n",
    "    attrs['WIDTH'] = ds.x.shape[0]\n",
    "    \n",
    "    # interferogram attrs\n",
    "    for k in attrs.keys():\n",
    "        if type(attrs[k]) == str:\n",
    "            ds[k] = attrs[k].ljust(50, ' ')\n",
    "        else:\n",
    "            ds[k] = attrs[k]\n",
    "            \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def geometry_to_xarray(insar_path, x_pad_pixels=[0,0], y_pad_pixels=[0,0]):   \n",
    "    '''\n",
    "    insar_path: dict of paths to insar rasters and metadata with keys:\n",
    "                'dem', 'unw_phase', 'amp', 'lv_theta', 'lv_phi', 'corr', 'metadata', 'log'\n",
    "    '''\n",
    "    \n",
    "    # get interferogram attrs\n",
    "    attrs = get_insar_attrs(insar_path['log'], insar_path['unw_phase'])\n",
    "    \n",
    "    meta = readfile.read_gdal_vrt(insar_path['unw_phase'])\n",
    "    attrs.update(meta)\n",
    "    # attrs = {k: str(v).ljust(80, ' ') for (k, v) in zip(attrs.keys(), attrs.values())}\n",
    "    \n",
    "    ds=gdal.Open(str(insar_path['dem']))\n",
    "    dem_data = ds.GetRasterBand(1)\n",
    "    dem_data = dem_data.ReadAsArray()\n",
    "    dem_data = np.ma.masked_invalid(dem_data, copy=True)   \n",
    "    \n",
    "    ds=gdal.Open(str(insar_path['lv_theta']))\n",
    "    lv_theta_data = ds.GetRasterBand(1)\n",
    "    lv_theta_data = lv_theta_data.ReadAsArray()\n",
    "    lv_theta_data = np.ma.masked_invalid(lv_theta_data, copy=True)\n",
    "    \n",
    "    ds=gdal.Open(str(insar_path['lv_phi']))\n",
    "    lv_phi_data = ds.GetRasterBand(1)\n",
    "    lv_phi_data = lv_phi_data.ReadAsArray()\n",
    "    lv_phi_data = np.ma.masked_invalid(lv_phi_data, copy=True)\n",
    "\n",
    "    # get coordinate system projection\n",
    "    prj = ds.GetProjection()\n",
    "    crs = pycrs.parse.from_ogc_wkt(prj)\n",
    "    crs_proj = crs.proj.name.ogc_wkt.lower()\n",
    "\n",
    "    # pixel resolution\n",
    "    geo_trans = ds.GetGeoTransform()\n",
    "    res_x = geo_trans[1]\n",
    "    res_y = geo_trans[5]\n",
    "    \n",
    "    #get corner coords and extents\n",
    "    corners = get_corners_gdal(str(insar_path['dem']))\n",
    "    x_extent = [corners[0], corners[2]]\n",
    "    y_extent = [corners[1], corners[3]]\n",
    "    \n",
    "    # create x and y arrays based on extents, pixel resolution, and padding\n",
    "    x_coords = np.arange(x_extent[0], x_extent[1], res_x)\n",
    "    x_coords = pad_array(x_coords, res_x, left_pad_pixels=x_pad_pixels[0], right_pad_pixels=x_pad_pixels[1])                         \n",
    "    y_coords = np.arange(y_extent[0], y_extent[1], res_y)\n",
    "    y_coords = pad_array(y_coords, -res_y, left_pad_pixels=y_pad_pixels[0], right_pad_pixels=y_pad_pixels[1])\n",
    "    \n",
    "    # create xarray dataset\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            'y': y_coords,\n",
    "            'x': x_coords,\n",
    "            'dem': (\n",
    "                ('y', 'x'),\n",
    "                np.pad(dem_data, pad_width=((y_pad_pixels[0],y_pad_pixels[1]), (x_pad_pixels[0],x_pad_pixels[1])), mode='constant',constant_values=(0.0)),\n",
    "            ),\n",
    "            'lv_theta': (\n",
    "                ('y', 'x'),\n",
    "                np.pad(lv_theta_data, pad_width=((y_pad_pixels[0],y_pad_pixels[1]), (x_pad_pixels[0],x_pad_pixels[1])), mode='constant',constant_values=(0.0)),\n",
    "            ),\n",
    "            'lv_phi': (\n",
    "                ('y', 'x'),\n",
    "                np.pad(lv_phi_data, pad_width=((y_pad_pixels[0],y_pad_pixels[1]), (x_pad_pixels[0],x_pad_pixels[1])), mode='constant',constant_values=(0.0)),\n",
    "            ),\n",
    "        },\n",
    "        attrs=None\n",
    "    )\n",
    "\n",
    "    # Set x and y coord attributes\n",
    "    attrs_x = {\n",
    "        'axis': 'X',\n",
    "        'units': 'dd',\n",
    "        'standard_name': 'projection_x_coordinate',\n",
    "        'long_name': 'longitude'\n",
    "    }\n",
    "    attrs_y = {\n",
    "        'axis': 'Y',\n",
    "        'units': 'dd',\n",
    "        'standard_name': 'projection_y_coordinate',\n",
    "        'long_name': 'latitude'\n",
    "    }\n",
    "    for key in attrs_x:\n",
    "        ds.x.attrs[key] = attrs_x[key]\n",
    "    for key in attrs_y:\n",
    "        ds.y.attrs[key] = attrs_y[key]\n",
    "    \n",
    "    ds.rio.write_crs(attrs['EPSG'], inplace=True)\n",
    "    ds = ds.rio.reproject(\"EPSG:4326\", **{'nodata':0.0})\n",
    "    ds.rio.write_crs(4326, inplace=True)\n",
    "    # ds = ds.drop_vars('spatial_ref')\n",
    "    del ds.dem.attrs[\"_FillValue\"]\n",
    "    del ds.lv_theta.attrs[\"_FillValue\"]\n",
    "    del ds.lv_phi.attrs[\"_FillValue\"]\n",
    "    \n",
    "    attrs['X_FIRST'] = ds['x'][0]\n",
    "    attrs['Y_FIRST'] = ds['y'][0]\n",
    "    attrs['X_STEP'] = abs(ds['x'][-1] - ds['x'][0]) / len(ds['x'])\n",
    "    attrs['Y_STEP'] = abs(ds['y'][-1] - ds['y'][0]) / len(ds['y'])\n",
    "    attrs['X_UNIT'] = \"degrees\"\n",
    "    attrs['Y_UNIT'] = \"degrees\"\n",
    "    attrs['LENGTH'] = ds.y.shape[0]\n",
    "    attrs['WIDTH'] = ds.x.shape[0]\n",
    "    \n",
    "    # interferogram attrs\n",
    "    for k in attrs.keys():\n",
    "        if type(attrs[k]) == str:\n",
    "            ds[k] = attrs[k].ljust(50, ' ')\n",
    "        else:\n",
    "            ds[k] = attrs[k]\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the spatial extents needed for an xarray SBAS stack\n",
    "\n",
    "- find the smallest bounding box that will cover the complete stack\n",
    "- (optional) add some padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=gdal.Open(str(stack_paths[0]['unw_phase']))\n",
    "geo_trans = ds.GetGeoTransform()\n",
    "res_x = geo_trans[1]\n",
    "res_y = geo_trans[5]\n",
    "res_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_pixels = 50\n",
    "bboxes = [get_corners_gdal(s['unw_phase']) for s in stack_paths]\n",
    "\n",
    "max_xyxy_bbox = [min([i[0] for i in bboxes]) - (res_x * padding_pixels),\n",
    "                 max([i[1] for i in bboxes]) + (res_y * padding_pixels),\n",
    "                 max([i[2] for i in bboxes]) + (res_x * padding_pixels),\n",
    "                 min([i[3] for i in bboxes]) - (res_y * padding_pixels)]\n",
    "max_xyxy_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of InSAR xarray.Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(xarray_insar):\n",
    "    pairs = []\n",
    "    for insar in xarray_insar:\n",
    "        ref = insar.Reference_Granule.to_numpy()\n",
    "        ref = ref.tolist()\n",
    "        sec = insar.Secondary_Granule.to_numpy()\n",
    "        sec = sec.tolist()\n",
    "        pairs.append(f\"{ref[17:25]}-{sec[17:25]}\")\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chunks(stack: xr.Dataset, bits_per_pixel, chunk_size=100, depth_dim=None, raster_count=1):\n",
    "    \"\"\"\n",
    "    stack: the xr.Dataset for which to determine chunks\n",
    "    chunk_size: int in MB\n",
    "    raster_count: number of rasters in each insar pair\n",
    "    \"\"\"\n",
    "    chunks = list()\n",
    "    bits_per_mb = 8000000\n",
    "    bits_per_chunk = bits_per_mb * chunk_size\n",
    "    \n",
    "\n",
    "    pixels_per_chunk = bits_per_chunk / bits_per_pixel\n",
    "    if depth_dim:\n",
    "        depth = len(stack[depth_dim])\n",
    "    else:\n",
    "        depth = 1\n",
    "    \n",
    "    temp_op_xy_pixels = pixels_per_chunk // (depth * raster_count)\n",
    "    spatial_op_xy_pixels = pixels_per_chunk // raster_count\n",
    "    \n",
    "    temp_x_y_side = math.floor(math.sqrt(temp_op_xy_pixels))\n",
    "    spatial_x_y_side = math.floor(math.sqrt(spatial_op_xy_pixels))\n",
    "    \n",
    "    return {\n",
    "        'temporal': (depth, temp_x_y_side, temp_x_y_side),\n",
    "        'spatial': (1, spatial_x_y_side, spatial_x_y_side)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri = \"s3://alex-asf-zarr-play/Salt_Lake\"\n",
    "s3 = s3fs.S3FileSystem(profile='zarr')\n",
    "store = s3fs.S3Map(root=s3_uri, s3=s3, check=False)\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full:\n",
    "[AZ_Stack](https://search.asf.alaska.edu/#/?zoom=6.583&center=-112.132,32.356&start=2014-06-14T08:00:00Z&end=2023-04-18T07:59:59Z&resultsLoaded=true&granule=S1A_IW_SLC__1SSV_20141206T132734_20141206T132802_003599_00440A_FB1C-SLC&searchType=SBAS%20Search&master=S1A_IW_SLC__1SSV_20141206T132734_20141206T132802_003599_00440A_FB1C&selectedPair=S1A_IW_SLC__1SSV_20141206T132734_20141206T132802_003599_00440A_FB1C-SLC,S1A_IW_SLC__1SSV_20141230T132734_20141230T132801_003949_004BFB_9ADE-SLC&perp=300to&temporal=1to48&pairs=S1A_IW_SLC__1SSV_20161113T132750_20161113T132817_013924_0166A5_21A4-SLC,S1A_IW_SLC__1SDV_20170313T132749_20170313T132816_015674_019C9E_D813-SLC$S1A_IW_SLC__1SSV_20161113T132750_20161113T132817_013924_0166A5_21A4-SLC,S1A_IW_SLC__1SDV_20170325T132749_20170325T132816_015849_01A1CF_3FB5-SLC$S1A_IW_SLC__1SSV_20161113T132750_20161113T132817_013924_0166A5_21A4-SLC,S1A_IW_SLC__1SDV_20170217T132749_20170217T132816_015324_0191F6_84A3-SLC$S1A_IW_SLC__1SSV_20161020T132750_20161020T132817_013574_015BBE_208C-SLC,S1A_IW_SLC__1SDV_20170217T132749_20170217T132816_015324_0191F6_84A3-SLC$S1A_IW_SLC__1SSV_20161020T132750_20161020T132817_013574_015BBE_208C-SLC,S1A_IW_SLC__1SDV_20170301T132749_20170301T132816_015499_019750_5210-SLC$S1A_IW_SLC__1SDV_20180730T132802_20180730T132829_023024_027FD1_B0E7-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20180718T132802_20180718T132829_022849_027A4C_6A57-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20180624T132800_20180624T132827_022499_026FD7_32D8-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20180612T132800_20180612T132826_022324_026A9D_E736-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190526T132805_20190526T132831_027399_03173A_4267-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190701T132807_20190701T132834_027924_03271E_46BA-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190607T132805_20190607T132832_027574_031C9E_B733-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190619T132806_20190619T132833_027749_0321E2_A30A-SLC)\n",
    "\n",
    "[Part 1 (998 pairs)](https://search.asf.alaska.edu/#/?zoom=7.804&center=-111.958,30.606&start=2014-06-14T08:00:00Z&end=2023-01-11T08:59:59Z&resultsLoaded=true&granule=S1A_IW_SLC__1SSV_20141206T132734_20141206T132802_003599_00440A_FB1C-SLC&searchType=SBAS%20Search&master=S1A_IW_SLC__1SSV_20141206T132734_20141206T132802_003599_00440A_FB1C&selectedPair=S1A_IW_SLC__1SSV_20141206T132734_20141206T132802_003599_00440A_FB1C-SLC,S1A_IW_SLC__1SSV_20141230T132734_20141230T132801_003949_004BFB_9ADE-SLC&perp=300to&temporal=1to48&pairs=S1A_IW_SLC__1SSV_20161113T132750_20161113T132817_013924_0166A5_21A4-SLC,S1A_IW_SLC__1SDV_20170313T132749_20170313T132816_015674_019C9E_D813-SLC$S1A_IW_SLC__1SSV_20161113T132750_20161113T132817_013924_0166A5_21A4-SLC,S1A_IW_SLC__1SDV_20170325T132749_20170325T132816_015849_01A1CF_3FB5-SLC$S1A_IW_SLC__1SSV_20161113T132750_20161113T132817_013924_0166A5_21A4-SLC,S1A_IW_SLC__1SDV_20170217T132749_20170217T132816_015324_0191F6_84A3-SLC$S1A_IW_SLC__1SSV_20161020T132750_20161020T132817_013574_015BBE_208C-SLC,S1A_IW_SLC__1SDV_20170217T132749_20170217T132816_015324_0191F6_84A3-SLC$S1A_IW_SLC__1SSV_20161020T132750_20161020T132817_013574_015BBE_208C-SLC,S1A_IW_SLC__1SDV_20170301T132749_20170301T132816_015499_019750_5210-SLC$S1A_IW_SLC__1SDV_20180730T132802_20180730T132829_023024_027FD1_B0E7-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20180718T132802_20180718T132829_022849_027A4C_6A57-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20180624T132800_20180624T132827_022499_026FD7_32D8-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20180612T132800_20180612T132826_022324_026A9D_E736-SLC,S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190526T132805_20190526T132831_027399_03173A_4267-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190701T132807_20190701T132834_027924_03271E_46BA-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190607T132805_20190607T132832_027574_031C9E_B733-SLC$S1A_IW_SLC__1SDV_20181115T132802_20181115T132829_024599_02B38C_EA96-SLC,S1A_IW_SLC__1SDV_20190619T132806_20190619T132833_027749_0321E2_A30A-SLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the full compliment of logfile attributes for the entire stack\n",
    "\n",
    "- some HyP3 interferograms make come with variations in these attributes\n",
    "- We add insar attributes as variables for each interferogram\n",
    "    - all interferograms must contain the same set of variables in order to concatenate xarray.Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sbas_group = 'format/sbas'\n",
    "geo_group = 'format/geometry'\n",
    "\n",
    "# We may not have enough memory to build a large SBAS Zarr store all at once, but we can work in batches.\n",
    "# Calculate number of batches in which to convert.\n",
    "batch_size = 10\n",
    "n = np.ceil(len(stack_paths) / batch_size)\n",
    "\n",
    "# iterate through each batch\n",
    "for i, batch in tqdm(enumerate(np.array_split(stack_paths, n))):   \n",
    "    xarray_sbas = []\n",
    "    # create a list of xarray.Datasets, one for each interferogram in the batch\n",
    "    for insar in tqdm(batch):\n",
    "        pad = get_padding_to_match_xyxy_bbox(insar['unw_phase'], max_xyxy_bbox)\n",
    "        xarray_sbas.append(sbas_to_xarray(insar, x_pad_pixels=pad['x_pad'] , y_pad_pixels=pad['y_pad']))\n",
    "    \n",
    "    # create an insar date pair dimension for the batch\n",
    "    pairs = get_pairs(xarray_sbas)\n",
    "    \n",
    "    # create an empty xarray.Dataset with the spatial coordinates needed for the stack\n",
    "    stack = xarray_sbas[0]\n",
    "    variables = list(xarray_sbas[0].variables)\n",
    "    for k in variables:\n",
    "        stack = stack.drop_vars(k)\n",
    "        \n",
    "    # add pair coordinates to the empty stack\n",
    "    stack = stack.assign_coords(pairs=pairs)\n",
    "\n",
    "    # concatenate all interferograms in the batch\n",
    "    for k in xarray_sbas[0].keys():\n",
    "        var = xr.concat([d[k] for d in xarray_sbas], dim=stack.pairs)\n",
    "        stack[k] = var\n",
    "        for da in xarray_sbas:\n",
    "            da = da.drop_vars(k)\n",
    "    \n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        # geometry\n",
    "        pad = get_padding_to_match_xyxy_bbox(batch[i]['dem'], max_xyxy_bbox)\n",
    "        xarray_geometry = geometry_to_xarray(batch[i], x_pad_pixels=pad['x_pad'] , y_pad_pixels=pad['y_pad'])\n",
    "        bits_per_pixel = sys.getsizeof(xarray_geometry.dem[0][0])\n",
    "        chunks = calc_chunks(xarray_geometry, bits_per_pixel, raster_count=3)\n",
    "        encoding = {vname: {'compressor': compressor, 'chunks': chunks['spatial']} for vname in xarray_geometry.data_vars}\n",
    "        xarray_geometry.to_zarr(store=store, encoding=encoding, consolidated=True, group=geo_group, mode='w')\n",
    "        \n",
    "        #sbas\n",
    "        bits_per_pixel = sys.getsizeof(stack.unw_phase.isel(pairs=1)[0][0])\n",
    "        chunks = calc_chunks(stack, bits_per_pixel, depth_dim=\"pairs\", raster_count=2)\n",
    "        encoding = {vname: {'compressor': compressor, 'chunks': chunks['temporal']} for vname in stack.data_vars}\n",
    "        \n",
    "        \n",
    "        stack.to_zarr(store=store, encoding=encoding, consolidated=True, group=sbas_group, mode='w')\n",
    "    else:\n",
    "        # zarr store append\n",
    "        stack.to_zarr(store=store, encoding=None, consolidated=True, group=sbas_group, mode='a', append_dim='pairs')\n",
    "\n",
    "    # if i == 0:\n",
    "    #     break\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "- creating large data cubes will overrun memory resources. We should append each layer in a cube to a zarr store in s3 instead of trying to build the whole thing as we go. \n",
    "- How do we want to handle DEMs. Should we keep a copy for every interferogram? They're identical.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prepare_Hyp3_RTC_TimeSeries_NetCDF_Zarr.ipynb - Version 0.1.0 - March 2021*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar_analysis_1 [conda env:.local-insar_analysis_1]",
   "language": "python",
   "name": "conda-env-.local-insar_analysis_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
