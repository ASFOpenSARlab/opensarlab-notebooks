{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "tags": []
   },
   "source": [
    "![SAR, InSAR, PolSAR, and banner](https://opensarlab-docs.asf.alaska.edu/opensarlab-notebook-assets/notebook_images/blackboard-banner.png)\n",
    "\n",
    "# InSAR Time Series Analysis using MintPy and HyP3 products\n",
    "\n",
    "**Author:** Alex Lewandowski; University of Alaska Fairbanks\n",
    "\n",
    "Based on the [LosAngeles_time_series](https://github.com/ASFOpenSARlab/opensarlab-notebooks/blob/master/SAR_Training/English/Hazards/LosAngeles_time_series.ipynb) notebook by Eric Fielding, David Bekaert, Heresh Fattahi and Yunjun Zhang, which uses an example ARIA dataset.\n",
    "\n",
    "\n",
    "\n",
    "## Mapping surface deformation with InSAR time series\n",
    "\n",
    "This notebook demonstrates how to create an InSAR time series with MintPy using an ASF HyP3 InSAR data stack, which can be ordered on [ASF Data Search/Vertex](https://search.asf.alaska.edu/#/) and prepared using the [Prepare_HyP3_InSAR_Stack_for_MintPy](https://github.com/ASFOpenSARlab/opensarlab-notebooks/blob/master/SAR_Training/English/Master/Prepare_HyP3_InSAR_Stack_for_MintPy.ipynb) notebook.\n",
    "\n",
    "<img src=\"https://opensarlab-docs.asf.alaska.edu/opensarlab-notebook-assets/notebook_images/UAFLogo_A_647.png\" width=\"170\" align=\"right\" />\n",
    "\n",
    "It also explores how to assess the quality of the stack inversion, temporal coherence, and velocity errors.\n",
    "\n",
    "**Important Note about JupyterHub**\n",
    "Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/insar_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"insar_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select \"insar_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"insar_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0: Notebook setup\n",
    "\n",
    "**Import necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from getpass import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Union\n",
    "import zipfile\n",
    "\n",
    "import h5py\n",
    "from ipyfilechooser import FileChooser\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr\n",
    "from pandas.core.frame import DataFrame\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import transform_bounds, transform\n",
    "import rioxarray as rxr\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib\n",
    "import xarray as xr\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file, ColumnDataSource, output_notebook\n",
    "from bokeh.models import LabelSet\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "from mintpy.cli import view, tsview, plot_network, plot_transection, plot_coherence_matrix\n",
    "import mintpy.plot_coherence_matrix\n",
    "import mintpy.objects.insar_vs_gps\n",
    "import mintpy.utils\n",
    "\n",
    "import opensarlab_lib as asfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the directory holding your MintPy-ready HyP3 data stack and/or MintPy directory from a previously loaded MintPy SBAS stack**\n",
    "- Click the `Select` button\n",
    "- Navigate to your data directory\n",
    "- Click the `Select` button\n",
    "- Confirm that the desired path appears in green text\n",
    "- Click the `Change` button to alter your selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd()\n",
    "fc = FileChooser(path)\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a project name and create a MintPy directory in which to store files output during our analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the work directory\n",
    "work_path = Path(fc.selected_path)\n",
    "print(f\"Work directory: {work_path}\")\n",
    "\n",
    "# define a project name\n",
    "proj_name = input(\"Enter a project name: \")\n",
    "\n",
    "# define the MintPy time-series directory\n",
    "mint_path = work_path/'MintPy'\n",
    "mint_path.mkdir(exist_ok=True)\n",
    "print(f\"MintPy directory: {mint_path}\")\n",
    "\n",
    "#create a directory in which to store plots\n",
    "plot_path = mint_path/\"plots\"\n",
    "plot_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Add Your Climate Data Store (CDS) UID & API Key to the Pyaps3 Config \n",
    "\n",
    "### This step only needs to be completed once and may be skipped if you have already updated the CDS config \n",
    "\n",
    "- Running the MintPy smallbaselineApp's `correct_troposphere` step requires downloading atmospheric pressure data from the CDS\n",
    "- If don't yet have a CDS API key:\n",
    "    - Proceed to [CDS](https://cds.climate.copernicus.eu/cdsapp#!/home) and create an account\n",
    "    - Open the [Datasets page](https://cds.climate.copernicus.eu/cdsapp#!/search?type=dataset)\n",
    "    - Search for \"ERA5\"\n",
    "    - Select any of ERA5 datasets that appear\n",
    "    - Select the `Download data` tab\n",
    "    - Scroll to the bottom of the screen\n",
    "    - Accept the `Terms of use`\n",
    "    - Click on your name at the top right of the screen to access your profile page\n",
    "    - Your `UID` and `API Key` will be displayed here \n",
    "- Run the following 2 code cells to update the pyaps config from this notebook **OR** open an OpenSARlab terminal and complete the following steps:\n",
    "    - Use vim or another text editor to open the `~/.cdsapirc` config file\n",
    "        - Add the CDS url to the first line of the config and your CDS `UID` and CDS`API Key` to the 2nd line of the config\n",
    "            - This should be formatted like:\n",
    "                - url: https://cds.climate.copernicus.eu/api/v2\\\n",
    "                - key: your_UID: your_API_Key\n",
    "        - Save the config and exit the text editor\n",
    "\n",
    "**If you do not add your CDS credentials to `~/.cdsapirc`, the `correct_troposphere` step will fail**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyaps_cfg = Path(\"/home/jovyan/.cdsapirc\")\n",
    "try:\n",
    "    with open(pyaps_cfg, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) == 2 and 'url' in lines[0]:\n",
    "            print(\"There was a CDS UID and API Key found in the pyaps3 config: ~/.cdsapirc\")\n",
    "            print(\"Would you like to update them?\")\n",
    "            update_cds_cfg = asfn.select_parameter([\"Do not update CDS UID and API Key\", \n",
    "                                                    \"Update CDS UID and API Key\"])\n",
    "            display(update_cds_cfg)\n",
    "        else:\n",
    "            update_cds_cfg = None\n",
    "except FileNotFoundError:\n",
    "    update_cds_cfg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not update_cds_cfg or \"Update\" in update_cds_cfg.value:\n",
    "    with open(pyaps_cfg, 'w') as f:\n",
    "            uid = input(\"Enter your CDS UID\")\n",
    "            key = getpass(\"Enter your CDS API Key\")\n",
    "            lines = ['', '']\n",
    "            lines[0] = f\"url: https://cds.climate.copernicus.eu/api/v2\\n\"\n",
    "            lines[1] = f\"key: {uid}:{key}\\n\"\n",
    "            f.seek(0)\n",
    "            f.writelines(lines)\n",
    "            f.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. smallbaselineApp.py overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This application provides a workflow which includes several steps to invert a stack of unwrapped interferograms and apply different corrections to obtain ground displacement timeseries.  \n",
    "The workflow consists of two main blocks:\n",
    "\n",
    "* correcting unwrapping errors and inverting for the raw phase time-series (blue ovals),\n",
    "* correcting for noise from different sources to obtain the displacement time-series (green ovals).\n",
    "\n",
    "Some steps are optional, which are switched off by default (marked by dashed boundaries). Configuration parameters for each step are initiated with default values in a customizable text file: [smallbaselineApp.cfg](https://github.com/insarlab/MintPy/blob/master/mintpy/defaults/smallbaselineApp.cfg). In this notebook, we will walk through some of these steps, for a complete example see the [MintPy repository](https://github.com/insarlab/MintPy).\n",
    "\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img width=\"600\" src=\"https://opensarlab-docs.asf.alaska.edu/opensarlab-notebook-assets/notebook_images/MintPyWorkflow.jpg\">\n",
    "</p>     \n",
    "<p style=\"text-align: center;\">\n",
    "    (Figure from Yunjun et al., 2019)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Processing steps of smallbaselineApp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MintPy **smallbaselineApp.py** application provides a workflow to invert a stack of unwrapped interferograms and apply different (often optional) corrections to obtain ground displacement timeseries. A detailed overview of the options can be retrieved by involking the help option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Configuring processing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing parameters for the **smallbaselineApp.py** are controlled through a configuration file. If no file is provided the default [smallbaselineApp.cfg](https://github.com/insarlab/MintPy/blob/master/mintpy/defaults/smallbaselineApp.cfg) configuration is used. We will create a custom config file with modified configuration parameters for this time-series analysis. Any options added to the custom config will override options set in the default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do you wish to skip the template creation and data loading steps to work with a previously loaded dataset?\")\n",
    "\n",
    "skip_load_choice = asfn.select_parameter(['create a MintPy template and load data', 'skip template creation and data loading steps'])\n",
    "display(skip_load_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = 'skip' not in skip_load_choice.value\n",
    "\n",
    "if load:\n",
    "    config = f'''# vim: set filetype=cfg:\n",
    "    mintpy.load.processor        = hyp3\n",
    "    ##---------interferogram datasets:\n",
    "    mintpy.load.unwFile          = {work_path}/*/*unw_phase_clip.tif\n",
    "    mintpy.load.corFile          = {work_path}/*/*corr_clip.tif\n",
    "    ##---------geometry datasets:\n",
    "    mintpy.load.demFile          = {work_path}/*/*dem_clip.tif\n",
    "    mintpy.load.incAngleFile     = {work_path}/*/*lv_theta_clip.tif\n",
    "    mintpy.load.azAngleFile      = {work_path}/*/*lv_phi_clip.tif\n",
    "    mintpy.load.waterMaskFile    = {work_path}/*/*water_mask_clip.tif\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    ref_point_option = asfn.select_parameter([\"Allow MintPy to determine a reference point\", \n",
    "                                             \"Define a reference point\"])\n",
    "    display(ref_point_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    is_float = False\n",
    "    ref_pt_config = ''\n",
    "    while not is_float:\n",
    "        if 'Define' in ref_point_option.value:\n",
    "            try:\n",
    "                lat = float(input(\"Enter reference latitude\"))\n",
    "                lon = float(input(\"Enter reference longitude\"))\n",
    "                is_float = True\n",
    "            except ValueError:\n",
    "                print(\"Latitude and Longitude must be convertable to float\")\n",
    "                continue\n",
    "            ref_pt_config = f'\\nmintpy.reference.lalo        = {lat},{lon}'\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    ref_date_option = asfn.select_parameter([\"Allow MintPy to determine reference date\", \n",
    "                                             \"Reference time-series to earliest date in stack\"])\n",
    "\n",
    "    display(ref_date_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    ref_date_config = ''\n",
    "    if 'MintPy' in ref_date_option.value:\n",
    "        ref_date_config = f'\\nmintpy.reference.date       = auto'\n",
    "    else:\n",
    "        ref_date_config = f'\\nmintpy.reference.date       = no'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MintPy allows for multithreaded processing using Dask for interferogram inversion and topographic correction.**\n",
    "\n",
    "MintPy defaults to sequential processing. \n",
    "In general, multithreaded processing will be faster. \n",
    "\n",
    "If you opt to use multithreaded proccessing, be aware that Dask will use CPU cores on your compute instance, which may impact the processing speeds for scripts and other notebooks running at the same time.\n",
    "\n",
    "https://www.dask.org/\n",
    "\n",
    "Note: The following code cell may list more CPU cores than available if you are running this on a shared instance in a Jupyter Hub, such as OpenSARLab. In this case, if opting to use all available cores for Dask multi-threading, multiple Dask workers will be assigned to each CPU core. The code will still run and you will still see a speed improvement compared to sequential processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    cpu_count = os.cpu_count()\n",
    "\n",
    "    print(f\"You currently have access to {cpu_count} logical CPU cores\")\n",
    "    multithread_option = asfn.select_parameter([\"Do not use multithreaded processing\",\n",
    "                                         f\"Use all {cpu_count} available cores for multithreaded processing\",\n",
    "                                         \"Use some of my available cores for multithreaded processing\"])\n",
    "    display(multithread_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    if 'Use all' in multithread_option.value:\n",
    "        multithread_config = f'''\\nmintpy.compute.cluster      = local\n",
    "    mintpy.compute.numWorker    = {cpu_count}'''\n",
    "    elif 'Use some' in multithread_option.value:\n",
    "        valid_cpu_count = False\n",
    "        worker_cores = 9999999999\n",
    "        while not valid_cpu_count:\n",
    "            try:\n",
    "                worker_cores = int(input(f\"Enter the number of CPU cores to use for multithreading (must be {cpu_count} or less)\"))\n",
    "            except ValueError:\n",
    "                pass\n",
    "            valid_cpu_count = worker_cores <= cpu_count\n",
    "        multithread_config = f'''\\nmintpy.compute.cluster      = local\n",
    "    mintpy.compute.numWorker    = {worker_cores}'''\n",
    "    else:\n",
    "        multithread_config = ''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = mint_path/f'{proj_name}.txt'\n",
    "\n",
    "if load:\n",
    "    updated_config = f'{config}{ref_pt_config}{ref_date_config}{multithread_config}'\n",
    "\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(updated_config)\n",
    "        print(f\"config file path: {config_path}\\n\")\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Small Baseline Time Series Analysis\n",
    "\n",
    "**You can run every step in smallbaselineApp.py with one call, using the command in the cell below**\n",
    "\n",
    "**For the purposes of this tutorial, we will run each step separately**\n",
    "\n",
    "We will run the steps:\n",
    "- load_data\n",
    "- modify_network\n",
    "- reference_point\n",
    "- quick_overview\n",
    "- invert_network\n",
    "- correct_troposphere\n",
    "- correct_topography\n",
    "- residual_RMS\n",
    "- reference_date\n",
    "- velocity\n",
    "- google_earth\n",
    "\n",
    "Skipped steps include:\n",
    "- correct_unwrap_error\n",
    "- correct_LOD\n",
    "- correct_SET\n",
    "- deramp\n",
    "- hdfeos5\n",
    "\n",
    "Skipped steps will also be skipped if running the entire smallbaselineApp in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This runs every step\n",
    "\n",
    "# !smallbaselineApp.py --work-dir {mint_path}  {config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Load Data\n",
    "\n",
    "**Run the `load_data` step**\n",
    "\n",
    "- If you get a missing 'Height' attribute error, you are missing a DEM, which is an available option when ordering HyP3 InSAR products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    !smallbaselineApp.py $config_path --work-dir {mint_path} --dostep load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the loading step is an \"inputs\" directory containing two HDF5 files:\n",
    "- ifgramStack.h5: This file contains 6 dataset cubes (e.g. unwrapped phase, coherence, connected components etc.) and multiple metadata.  \n",
    "- geometryGeo.h5: This file contains geometrical datasets (e.g., incidence/azimuth angle, masks, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_path = mint_path/'inputs'\n",
    "!ls $inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>info.py :</b> \n",
    "To get general infomation about a MintPy product, run info.py on the file.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2. Modify the Network\n",
    "\n",
    "**Run the `modify_network` step**\n",
    "\n",
    "- Identifies and excludes interferograms (i.e. affected by remaining coherence phase-unwrapping errors) before the network inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep modify_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3. Plot the interferogram network\n",
    "\n",
    "Running **plot_network.py** gives an overview of the network and the average coherence of the stack. The program creates multiple files as follows:\n",
    "- `ifgramStack_coherence_spatialAvg.txt`: Contains interferogram dates, average coherence temporal and spatial baseline separation.\n",
    "- `Network.pdf`: Displays the network of interferograms on time-baseline coordinates, colorcoded by avergae coherence of the interferograms. \n",
    "- `CoherenceMatrix.pdf` shows the avergae coherence pairs between all available pairs in the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "with asfn.work_dir(mint_path):\n",
    "    plot_network.main([f'{inputs_path}/ifgramStack.h5'])\n",
    "    plots = ['bperpHistory.pdf', 'coherenceHistory.pdf', 'coherenceMatrix.pdf', 'network.pdf']\n",
    "    for p in plots:\n",
    "        if (mint_path/p).exists():\n",
    "            (mint_path/p).rename(f'{plot_path}/{p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Set the Reference Point\n",
    "\n",
    "**Run the `reference_point` step**\n",
    "\n",
    "The interferometric phase is a relative observation by nature. The phases of each unwrapped interferogram are relative with respect to an arbitrary pixel. Therfore, we need to reference all interferograms to a common reference pixel.\n",
    "\n",
    "The `reference_point` step selects a common reference pixel for the stack of interferograms. The default approach of MintPy is to choose a pixel with the highest spatial coherence in the stack. Other options include specifying the longitude and latitude of the desired reference pixel or the line and column number of the refence pixel.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep reference_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the \"reference_step\" adds additional attributes \"REF_X, REF_Y\" and \"REF_LON, REF_LAT\" to the `ifgramStack.h5` file. To see the attributes of the file run `info.py`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!info.py $inputs_path/ifgramStack.h5 | egrep 'REF_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Run a Quick Overview\n",
    "\n",
    "**Run the `quick_overview` step**\n",
    "\n",
    "- Assess possible groud deformation using the velocity from traditional interferogram stacking \n",
    "    - *reference: Zebker et al. (1997, JGR)*\n",
    "- Assess distribution of phase unwrapping error from the number of interferogram triplets with non-zero integer ambiguity of closure phase \n",
    "    - *reference: T_int in Yunjun et al. (2019, CAGEO). Related to section 3.2, equation (8-9) and Fig. 3d-e.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep quick_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Inverting the Small Baseline network\n",
    "\n",
    "**Run the `invert_network` step**\n",
    "\n",
    "- Invert the network of differential unwrapped interferograms to estimate the time-series of unwrapped phase with respect to a reference acquisition date\n",
    "- By default mintpy selects the first acquisition\n",
    "- The estimated time-series is converted to distance change from radar to target and is provided in meters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgram_path = mint_path/\"inputs/ifgramStack.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep invert_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Correct for Tropospheric Propagation Delays (Optional)\n",
    "\n",
    "**Run the `correct_troposphere` step**\n",
    "\n",
    "- Uses ECMWF [ERA5 climate reanalysis pressure data](https://cds.climate.copernicus.eu/cdsapp#!/search?type=dataset&keywords=((%20%22Product%20type:%20Reanalysis%22%20)%20AND%20(%20%22Provider:%20Copernicus%20C3S%22%20))&text=pressure)\n",
    "- CDS limits ECMWF archive requests to 50, so your requests may be queued until there is space.\n",
    "    - https://cds.climate.copernicus.eu/live/queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    tropo_choice = asfn.select_parameter([\"Perform Tropospheric Correction Step\",\n",
    "                                          \"Skip Tropospheric Correction Step\",\n",
    "                                          \"Delete Outputs of a Previous (possibly interrupted) Troposheric Correction and Rerun\",\n",
    "                                          \"Delete Outputs of a Previous Troposheric Correction and Skip Troposheric Correction Now\"])\n",
    "    display(tropo_choice)\n",
    "else:\n",
    "    tropo_files = list(mint_path.glob('*ERA5*.h5'))\n",
    "    if 0 < len(tropo_files) < 3:\n",
    "        # some but not all tropo corrected HDF5s are present and we should delete outputs rerun troposheric correction\n",
    "        tropo_choice = \"Delete and Rerun\"\n",
    "    elif len(tropo_files) == 0:\n",
    "        # tropospheric correction was not performed\n",
    "        tropo_choice = \"Skip\"\n",
    "    else:\n",
    "        # tropospheric correction was performed and all necessary HDF5 files are present\n",
    "        tropo_choice = \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_troposhperic_correction_mintpy(config_path, method):\n",
    "    config_path = Path(config_path)\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = f.readlines()    \n",
    "    config_update = config\n",
    "    \n",
    "    for i, l in enumerate(config):\n",
    "        no_comment = l.split(\"#\")[0]\n",
    "        present = False\n",
    "        if \"mintpy.troposphericDelay.method=\" in \"\".join(no_comment.split()):\n",
    "            config_update[i] = f\"mintpy.troposphericDelay.method = {method}\"\n",
    "            present = True\n",
    "            break     \n",
    "    if not present:\n",
    "        config_update.append(f\"\\nmintpy.troposphericDelay.method = {method}\")\n",
    "\n",
    "    config_str = \"\"\n",
    "    for l in config_update:\n",
    "        config_str = f\"{config_str}{l}\"\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if type(tropo_choice) != str:\n",
    "    tropo_choice = tropo_choice.value\n",
    "\n",
    "correct_tropo = \"Perform\" in tropo_choice or \"Rerun\" in tropo_choice or \"Done\" in tropo_choice\n",
    "\n",
    "era5_path = mint_path/\"ERA5\"\n",
    "timeseries_era5_path = mint_path/\"timeseries_ERA5.h5\"\n",
    "inputs_era5_path = mint_path/\"inputs/ERA5.h5\"\n",
    "\n",
    "if \"Delete\" in tropo_choice:\n",
    "    for f in [timeseries_era5_path, inputs_era5_path]:\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    try:\n",
    "        shutil.rmtree(era5_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "if correct_tropo and \"Done\" not in tropo_choice:\n",
    "    set_troposhperic_correction_mintpy(config_path, \"pyaps\")\n",
    "    !smallbaselineApp.py $config_path --work-dir {mint_path} --dostep load_data\n",
    "    !smallbaselineApp.py $config_path --work-dir {mint_path} --dostep correct_troposphere\n",
    "elif not correct_tropo:\n",
    "    set_troposhperic_correction_mintpy(config_path, \"no\")\n",
    "    !smallbaselineApp.py $config_path --work-dir {mint_path} --dostep load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Correct for DEM Errors\n",
    "\n",
    "**Run the `correct_topography` step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep correct_topography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.9. Calculate the Root Mean Square (RMS) of Residual Phase Time-Series for Each Acquisition\n",
    "\n",
    "**Run the `residual_RMS` step**\n",
    "\n",
    "- *reference: Yunjun et al. (2019, section 4.9 and 5.4)*\n",
    "- To remove the long wavelength component in space, a phase ramp is removed for each acquisition\n",
    "- Sets optimal reference date to date with min RMS\n",
    "- Sets exclude dates (outliers) to dates with RMS > cutoff * median RMS (Median Absolute Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep residual_RMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Reference the Entire Time-Series to One Date in Time\n",
    "\n",
    "**Run the `reference_date` step**\n",
    "\n",
    "- *reference: Yunjun et al. (2019, section 4.9)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep reference_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11. Estimate The Long-Term Velocity Rate\n",
    "\n",
    "**Run the `velocity` step**\n",
    "\n",
    "The timeseries file contains three datasets:\n",
    "- the `time-series` dataset, which is the interferometric range change for each acquisition relative to the reference acquisition\n",
    "- the `date` dataset, which contains the acquisition date for each acquisition\n",
    "- the `bperp` dataset, which contains the timeseries of the perpendicular baseline \n",
    "\n",
    "The ground deformation caused by many geophysical or anthropogenic processes are linear at first order approximation. Therefore it is common to estimate the rate of the ground deformation which is the slope of linear fit to the time-series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp_args = f'{mint_path}/velocity.h5 velocity -v -1 40 --dpi 600 --figsize 15 15 --outfile {plot_path}/velocity.png'\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12. Geocode velocity.h5 in Preparation for Creating a velocity.kmz\n",
    "\n",
    "**Run the `geocode` step**\n",
    "\n",
    "- This is unnecessary for geocoded HyP3 data but would be needed for non-geocoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep geocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13. Create a kmz File\n",
    "\n",
    "**Run the `google_earth` step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py $config_path --work-dir {mint_path} --dostep google_earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.14 Plot the unwrapped inverted timeseries\n",
    "\n",
    "**Create directories in which to store output we will create in upcoming steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_path = mint_path/'GeoTiffs'\n",
    "geotiff_path.mkdir(exist_ok=True)\n",
    "\n",
    "disp_path = geotiff_path/'displacement_maps'\n",
    "disp_path.mkdir(exist_ok=True)\n",
    "\n",
    "wrapped_path = disp_path/\"wrapped\"\n",
    "wrapped_path.mkdir(exist_ok=True)\n",
    "\n",
    "unwrapped_path = disp_path/\"unwrapped\"\n",
    "unwrapped_path.mkdir(exist_ok=True)\n",
    "\n",
    "demErr = 'timeseries_ERA5_demErr.h5' if correct_tropo else 'timeseries_demErr.h5'\n",
    "ts_demErr = mint_path/f'{demErr}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the unwrapped inverted time series steps (n, n+1, n+2, etc...)**\n",
    "\n",
    "- save a png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = f'{ts_demErr} --notitle --notick --noaxis --dpi 600 --figsize 15 15 --outfile {unwrapped_path}/unwrapped_inverted_ts.png'\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Error analysis (signal vs noise)\n",
    "\n",
    "Uncertainty of the ground displacement products derived from InSAR time-series, depends on the quality of the inversion of the stack of interferograms and the accuracy in separating the ground displacement from other components of the InSAR data. Therefore the definition of signal vs noise is different at the two main steps in MintPy:  \n",
    "\n",
    "1) During the inversion: \n",
    "    At this step all systematic components of the interferometric phase (e.g., ground displacement, propagation delay, geometrical residuals caused by DEM or platform's orbit inaccuracy) are considered signal, while the interferometric phase decorrelation, phase unwrapping error and phase inconsistency are considered noise. \n",
    "    \n",
    "2) After inversion: the ground displacement component of the time-serieses is signal, and everything else (including the propagation delay and geometrical residuals) are considered noise\n",
    "\n",
    "Therefore we first discuss the possible sources of error during the inversion and the existing ways in MintPy to evaluate the quality of inversion and to improve the uncertainty of the inversion. Afterwards we explain the different components of the time-series and the different processing steps in MintPy to separate them from ground displacement signal.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Quality of the inversion\n",
    "\n",
    "The main sources of noise during the time-series inversion includes decorrelation, phase unwrapping error and the inconsistency of triplets of interferofgrams. Here we mainly focus on the decorrelation and unwrapping errors. We first show the existing quantities in MintPy to evaluate decorrelation and unwrapping errors and then discuss the existing ways in MintPy to reduce the decorrelation and unwrapping errors on the time-series inversion.\n",
    "\n",
    "### 4.1.1. Average spatial coherence\n",
    "\n",
    "Mintpy computes temporal average of spatial coherence of the entire stack as a potential ancillary measure to choose reliable pixels after time-series inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scp_args = f\"{mint_path}/avgSpatialCoh.h5 --dpi 600 --figsize 15 15 --outfile {plot_path}/avg_spatial_coh.png\"\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Temporal coherence\n",
    "\n",
    "In addition to timeseries.h5 which contains the time-series dataset, invert_network produces other quantities, which contain metrics to evaluate the quality of the inversion including temporalCoherence.h5. Temporal coherence represents the consistency of the timeseries with the network of interferograms. \n",
    "\n",
    "Temporal coherence varies from 0 to 1. Pixels with values closer to 1 are considered reliable and pixels with values closer to zero are considered unreliable. For a dense network of interferograms, a threshold of 0.7 may be used (Yunjun et al, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scp_args = f\"{mint_path}/temporalCoherence.h5 --dpi 600 --figsize 15 15 --outfile {plot_path}/temporal_coh.png\"\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Velocity error analysis\n",
    "\n",
    "The estimated velocity also comes with an expression of unecrtainty which is simply based on the goodness of fit while fitting a linear model to the time-series. This quantity is saved in \"velocity.h5\" under the velocityStd dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scp_args = f'{mint_path}/velocity.h5 velocityStd -v 0 1 --dpi 600 --figsize 15 15 --outfile {plot_path}/velocity_err.png'\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the plot above is the velocity error, not the velocity. The errors generally increases with distance from the reference point and can also increase for points with elevations different from the reference point if topographically correlated water vapor variations are especially strong in the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Compare InSAR time-series with GPS time-series in LOS direction\n",
    "\n",
    "- http://geodesy.unr.edu/NGLStationPages/gpsnetmap/GPSNetMap.html\n",
    "- http://geodesy.unr.edu/NGLStationPages/DataHoldings.txt\n",
    "\n",
    "### 4.3.1. Identify Potential GPS stations\n",
    "\n",
    "**Write the University of Nevada, Reno GPS station holdings metadata to GPS_stations.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with asfn.work_dir(mint_path):\n",
    "    url = 'http://geodesy.unr.edu/NGLStationPages/DataHoldings.txt'\n",
    "    response = urllib.request.urlopen(url, timeout=5)\n",
    "    content = response.read()\n",
    "    rows = content.decode('utf-8').splitlines()\n",
    "    holdings_txt = Path('.')/'DataHoldings.txt'\n",
    "    if holdings_txt.exists():\n",
    "        holdings_txt.unlink()\n",
    "\n",
    "with open(f'{mint_path}/GPS_stations.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter=',', escapechar=',', quoting=csv.QUOTE_NONE)\n",
    "    for row in rows:\n",
    "        csv_writer.writerow([re.sub('\\s+', ' ', row)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Build a list of GPS stations within your area of interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_long(long):\n",
    "    if long > 180:\n",
    "        long = long - 360\n",
    "    return long\n",
    "\n",
    "# get the InSAR stack's corner coordinates\n",
    "with h5py.File(f\"{mint_path}/inputs/geometryGeo.h5\", 'r') as f:\n",
    "    lon_west = float(f.attrs['LON_REF1'])\n",
    "    lon_east = float(f.attrs['LON_REF2'])\n",
    "    lat_south = float(f.attrs['LAT_REF1'])\n",
    "    lat_north = float(f.attrs['LAT_REF3'])\n",
    "\n",
    "# get the start and end dates of the timeseries\n",
    "if correct_tropo:\n",
    "    info = gdal.Info(f\"{mint_path}/timeseries_ERA5_demErr.h5\", format='json')\n",
    "else:\n",
    "    info = gdal.Info(f\"{mint_path}/timeseries_demErr.h5\", format='json')\n",
    "ts_start = info['metadata']['']['START_DATE']\n",
    "ts_start = datetime.strptime(ts_start, '%Y%m%d')\n",
    "ts_end = info['metadata']['']['END_DATE']\n",
    "ts_end = datetime.strptime(ts_end, '%Y%m%d')\n",
    "\n",
    "# find all stations that have data within the ts time range,\n",
    "# are located within the AOI and at an unmasked pixel location\n",
    "gps_stations = list()\n",
    "with open(f'{mint_path}/GPS_stations.csv', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in list(csv_reader)[1:]:\n",
    "        begin_date = datetime.strptime(row[7], '%Y-%m-%d')\n",
    "        mod_date = datetime.strptime(row[9], '%Y-%m-%d')\n",
    "        lat = float(row[1])\n",
    "        \n",
    "        lon = convert_long(float(row[2]))\n",
    "\n",
    "        n = [lat, lon]\n",
    "        a = [lat_north, lon_west]\n",
    "        b = [lat_south, lon_west]\n",
    "        c = [lat_south, lon_east]\n",
    "        ab = np.subtract(a, b)\n",
    "        an = np.subtract(a, n)\n",
    "        bc = np.subtract(b, c)\n",
    "        bn = np.subtract(b, n)\n",
    "\n",
    "        in_aoi = 0 <= np.dot(ab, an) <= np.dot(ab, ab) and 0 <= np.dot(bc, bn) <= np.dot(bc, bc)\n",
    "        in_date_range = ts_start >= begin_date and ts_end <= mod_date\n",
    "        \n",
    "        if in_aoi and in_date_range:\n",
    "            vel_file = f'{mint_path}/velocity.h5'\n",
    "            atr = mintpy.utils.readfile.read_attribute(vel_file)\n",
    "            coord = mintpy.utils.utils.coordinate(atr, lookup_file=f'{mint_path}/inputs/geometryRadar.h5')\n",
    "            y, x = coord.geo2radar(lat, lon)[:2]\n",
    "            msk = mintpy.utils.readfile.read(f'{mint_path}/maskTempCoh.h5')[0]\n",
    "            box = (x, y, x+1, y+1)\n",
    "            masked = not msk[y, x]\n",
    "            if not masked:\n",
    "                gps_stations.append(row[0].strip())\n",
    "                \n",
    "gps = len(gps_stations) > 0\n",
    "if not gps:\n",
    "    print(\"There were no GPS sites found in your AOI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary of metadata for each GPS site in yoour AOI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gps:\n",
    "    def get_gps_dict(stations):\n",
    "        gps_dict = {}\n",
    "        with open(f'{mint_path}/GPS_stations.csv', newline='') as csvfile:\n",
    "            csv_reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            for row in list(csv_reader)[1:]:\n",
    "                if row[0] in stations:\n",
    "                    gps_dict[row[0]] = {\n",
    "                        'lat': row[1],\n",
    "                        'long': row[2],\n",
    "                        'height':row[3],\n",
    "                        'x': row[4],\n",
    "                        'y': row[5],\n",
    "                        'z': row[6],\n",
    "                        'date_beg': row[7],\n",
    "                        'date_end': row[8],\n",
    "                        'date_mod': row[9],\n",
    "                        'num_sol': row[10],\n",
    "                        'st_og_name': 'na'\n",
    "                    }\n",
    "                    if len(row) > 11:\n",
    "                         gps_dict[row[0]]['st_og_name'] = row[11]\n",
    "        return gps_dict\n",
    "    \n",
    "    gps_dict = get_gps_dict(gps_stations)\n",
    "\n",
    "    _, my_dict = mintpy.utils.readfile.read(f'{mint_path}/inputs/geometryGeo.h5', datasetName='height')\n",
    "\n",
    "    x_first = float(my_dict['X_FIRST'])\n",
    "    x_step = float(my_dict['X_STEP'])\n",
    "    width = float(my_dict['WIDTH'])\n",
    "\n",
    "    y_first = float(my_dict['Y_FIRST'])\n",
    "    y_step = float(my_dict['Y_STEP'])\n",
    "    height = float(my_dict['LENGTH'])\n",
    "\n",
    "    # (xmin, ymin, xmax, ymax)\n",
    "    bounds = (x_first, y_first+(y_step*height), x_first+(x_step*width),  y_first)\n",
    "\n",
    "    # convert bounds to web-mercator (epsg:3857)\n",
    "    xmin, ymin, xmax, ymax = transform_bounds(4326, 3857, *bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the GPS station locations in your AOI**\n",
    "\n",
    "Hover over GPS sites to view site metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gps:\n",
    "    output_notebook()\n",
    "\n",
    "    longs = [convert_long(float(gps_dict[k]['long'])) for k in gps_stations]\n",
    "    lats = [float(gps_dict[k]['lat']) for k in gps_stations]\n",
    "    xy_web = transform(4326, 3857, longs, lats)\n",
    "\n",
    "    source = DataFrame(\n",
    "        data=dict(\n",
    "            x=xy_web[0],\n",
    "            y=xy_web[1],\n",
    "            stations=gps_dict.keys(),\n",
    "            lats=[gps_dict[k]['lat'] for k in gps_dict],\n",
    "            longs=[convert_long(float(gps_dict[k]['long'])) for k in gps_dict],\n",
    "            exes=[gps_dict[k]['x'] for k in gps_dict],\n",
    "            whys=[gps_dict[k]['y'] for k in gps_dict],\n",
    "            zees=[gps_dict[k]['z'] for k in gps_dict],\n",
    "            heights=[gps_dict[k]['height'] for k in gps_dict],\n",
    "            start_dates=[gps_dict[k]['date_beg'] for k in gps_dict],\n",
    "            end_dates=[gps_dict[k]['date_end'] for k in gps_dict],\n",
    "            mod_dates=[gps_dict[k]['date_mod'] for k in gps_dict],\n",
    "            num_sols=[gps_dict[k]['num_sol'] for k in gps_dict],\n",
    "            og_names=[gps_dict[k]['st_og_name'] for k in gps_dict],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    labels = LabelSet(\n",
    "                x='x',\n",
    "                y='y',\n",
    "                text='stations',\n",
    "                level='glyph',\n",
    "                x_offset=-15, \n",
    "                y_offset=15, \n",
    "                source=ColumnDataSource(source))\n",
    "\n",
    "    TOOLTIPS = [\n",
    "        (\"station\", \"@stations\"),\n",
    "        (\"lat(deg)\", \"@lats\"),\n",
    "        (\"long(deg)\", '@longs'),\n",
    "        ('height(m)', '@heights'), \n",
    "        (\"x(m)\", '@exes'),\n",
    "        (\"y(m)\", \"@whys\"),\n",
    "        (\"z(m)\", \"@zees\"),\n",
    "        (\"start date\", \"@start_dates\"),\n",
    "        (\"end date\", \"@end_dates\"),\n",
    "        (\"modification date\", \"@mod_dates\"),\n",
    "        (\"NumSol\", \"@num_sols\"),\n",
    "        (\"original station name\", \"@og_names\")\n",
    "\n",
    "    ]\n",
    "\n",
    "    # range bounds supplied in web mercator coordinates\n",
    "    p = figure(x_range=(xmin, xmax), y_range=(ymin, ymax),\n",
    "               x_axis_type=\"mercator\", y_axis_type=\"mercator\", tooltips=TOOLTIPS)\n",
    "\n",
    "    p.add_tile(\"STAMEN_TERRAIN_RETINA\")\n",
    "\n",
    "    p.circle_dot(x='x', y='y', size=20, fill_alpha=0.2, color='red', alpha=0.6, source=source)\n",
    "\n",
    "    p.add_layout(labels)\n",
    "    \n",
    "    p.title = \"GPS Site Locations\"\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a GPS station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gps:\n",
    "    gps_station = widgets.RadioButtons(\n",
    "        options=gps_stations,\n",
    "        description='',\n",
    "        disabled=False,\n",
    "        layout=Layout(min_width='800px'))\n",
    "    \n",
    "    print(\"Select a GPS station\")\n",
    "    display(gps_station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the GPS/velocity comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if gps:\n",
    "    scp_args = f\"{mint_path}/velocity.h5 velocity --show-gps --ref-gps {gps_station.value} --gps-comp enu2los --gps-label --figsize 9 9\"\n",
    "    with asfn.work_dir(mint_path):\n",
    "        view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Plotting a Motion Transect \n",
    "\n",
    "**Select the transect to plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "data, _ = mintpy.utils.readfile.read(ts_demErr, datasetName='timeseries')\n",
    "mask = np.ma.masked_where(data[11]==0, data[11])\n",
    "data = mask.filled(fill_value=np.nan)\n",
    "line = asfn.LineSelector(data, figsize=(9, 9), cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = list(work_path.glob('*/*_amp_clip.tif'))[0]\n",
    "amp = gdal.Open(str(amp))\n",
    "geotrans = amp.GetGeoTransform()\n",
    "\n",
    "def geolocation(x, y, geotrans):\n",
    "    return [geotrans[0]+x*geotrans[1], geotrans[3]+y*geotrans[5]]\n",
    "\n",
    "try:\n",
    "    pnt_1 = geolocation(line.pnt1[0][0], line.pnt1[0][1], geotrans)\n",
    "    pnt_2 = geolocation(line.pnt2[0][0], line.pnt2[0][1], geotrans)\n",
    "    print(f\"point 1: {pnt_1}\")\n",
    "    print(f\"point 2: {pnt_2}\")\n",
    "except TypeError:\n",
    "    print('TypeError')\n",
    "    display(Markdown(f'<text style=color:red>This error may occur if a line was not selected.</text>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "scp_args = f'{mint_path}/velocity.h5 --start-lalo {pnt_1[1]} {pnt_1[0]} --end-lalo {pnt_2[1]} {pnt_2[0]} --outfile x'\n",
    "with asfn.work_dir(plot_path):\n",
    "    plot_transection.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plot the Cumulative Displacement Map and Point Displacement Time Series\n",
    "\n",
    "- Use the `Time` bar below the Cumulative Displacement Map to view displacements for different time periods\n",
    "- Click on the Cumulative Displacement Map to select points for displaying Point Displacement Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "tsview.main([str(ts_demErr), \n",
    "                    f'-d={mint_path}/inputs/geometryGeo.h5', \n",
    "                    f'-o={mint_path}/displacement_ts', \n",
    "                    f'--outfile={mint_path}/displacement_ts.pdf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate coherence, velocity, and total displacement Geotiffs\n",
    "\n",
    "**Create a list dates for all timesteps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgramstack = inputs_path/\"ifgramStack.h5\"\n",
    "\n",
    "with h5py.File(ifgramstack, \"r\") as f:\n",
    "    dates = f[\"date\"][()]\n",
    "    dates = list(set([d.decode(\"utf-8\") for insar in dates for d in insar]))\n",
    "    dates.sort()\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the full displacement timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = f'{dates[0]}_{dates[-1]}'\n",
    "!save_gdal.py $ts_demErr -d $ds --of GTIFF -o $geotiff_path/\"save_gdal_ts_demErr.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the unwrapped displacement GeoTiffs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(ifgramstack, 'r') as f:\n",
    "    unw_pth = f.attrs['FILE_PATH']\n",
    "\n",
    "ds_unw = rasterio.open(unw_pth, 'r', driver='GTiff')\n",
    "\n",
    "for i, d in enumerate(tqdm(dates)):\n",
    "    date_range = f'{dates[0]}_{dates[i]}'\n",
    "    cmd = f'view.py {ts_demErr} {date_range} --notitle --notick --noaxis'\n",
    "    data, _, _ = mintpy.view.prep_slice(cmd)\n",
    "    data = data / 100 # cm -> meters\n",
    "        \n",
    "\n",
    "    with rasterio.open(f'{unwrapped_path}/{date_range}_{ts_demErr.stem}_unwrapped.tif', 'w', driver='GTiff',\n",
    "                  height = data.shape[0], width = data.shape[1],\n",
    "                  count=1, dtype=str(data.dtype),\n",
    "                  crs=ds_unw.read_crs(),\n",
    "                  transform=ds_unw.transform,\n",
    "                  nodata=np.nan) as ds:\n",
    "        \n",
    "        ds.write(data.astype(rasterio.float32), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function to add a color ramp to single band GeoTiff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_wrapped_insar(tif_path: Union[str, Path]):\n",
    "    \"\"\"\n",
    "    Blue: 0 and 2\n",
    "    Red: /2\n",
    "    Yellow: \n",
    "    Green 3/2\n",
    "    \"\"\"\n",
    "    ds = gdal.Open(str(tif_path), 1)\n",
    "    band = ds.GetRasterBand(1)\n",
    "\n",
    "    # create color table\n",
    "    colors = gdal.ColorTable()\n",
    "    \n",
    "    colors.CreateColorRamp(0, (0, 0, 255),  64, (255, 0, 0)) \n",
    "    colors.CreateColorRamp(64, (255, 0, 0),   128, (255, 255, 0))\n",
    "    colors.CreateColorRamp(128, (255, 255, 0), 192, (0, 255, 0))\n",
    "    colors.CreateColorRamp(192, (0, 255, 0),   255, (0, 0, 255))\n",
    "\n",
    "    # set color table and color interpretation\n",
    "    band.SetRasterColorTable(colors)\n",
    "    band.SetRasterColorInterpretation(gdal.GCI_PaletteIndex)\n",
    "\n",
    "    # close and save file\n",
    "    del band, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect paths to unwrapped displacement maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapped_paths = list(unwrapped_path.rglob('*_unwrapped.tif'))\n",
    "unwrapped_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the wrapped interferogram GeoTiffs**\n",
    "\n",
    "- Please note that the wrapped range used below is currently under review and may not yet correctly correspond to the Sentinel-1 wavelength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentinel_c_band_lambda = 5.5465763\n",
    "\n",
    "for unw_path in tqdm(unwrapped_paths):\n",
    "    date_range_regex = '(?<=/unwrapped/)\\d{8}_\\d{8}'\n",
    "    date_range = re.search(date_range_regex, str(unw_path)).group(0)\n",
    "    \n",
    "    with rxr.open_rasterio(unw_path, masked=True).squeeze() as ds:\n",
    "        # convert unwrapped raster to radians\n",
    "        with xr.set_options(keep_attrs=True):\n",
    "            unw_rad = (ds * -4 * np.pi) / sentinel_c_band_lambda\n",
    "          \n",
    "    # I don't know what it means to convert meters to radians\n",
    "    # since we did that to the unw data, let's try doing the same to the wrapped range\n",
    "    wrap_range = [\n",
    "        (-2.8 * -4 * np.pi) / (sentinel_c_band_lambda * 100),\n",
    "        (2.8 * -4 * np.pi) / (sentinel_c_band_lambda * 100)\n",
    "    ]\n",
    "       \n",
    "    # wrap the interferogram\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        wrap = mintpy.utils.utils0.wrap(unw_rad, wrap_range=wrap_range)\n",
    "        \n",
    "    # collect crs and transform\n",
    "    with rasterio.open(unw_path, 'r', driver='GTiff') as ds:\n",
    "        unw_crs = ds.read_crs()\n",
    "        unw_transform = ds.transform\n",
    "    \n",
    "    # Save wrapped interferogram as a GeoTiff\n",
    "    wrp_path = wrapped_path/f'{date_range}_{ts_demErr.stem}_wrapped_unscaled.tif'\n",
    "    with rasterio.open(wrp_path, 'w', driver='GTiff',\n",
    "                      height = wrap.shape[0], width = wrap.shape[1],\n",
    "                      count=1, dtype=str(wrap.dtype),\n",
    "                      crs=unw_crs,\n",
    "                      transform=unw_transform,\n",
    "                      nodata=np.nan) as ds:\n",
    "        ds.write(wrap.astype(rasterio.float32), 1)\n",
    "        \n",
    "    # scale wrapped interferogram (0 to 255)\n",
    "    scaled_path = wrapped_path/f'{wrp_path.stem.split(\"_unscaled\")[0]}_scaled.tif'\n",
    "    !gdal_translate -of GTiff -scale -ot BYTE $wrp_path $scaled_path\n",
    "    wrp_path.unlink()\n",
    "    \n",
    "    # add color ramp\n",
    "    colorize_wrapped_insar(scaled_path)\n",
    "    \n",
    "    # convert to 3-band rgb\n",
    "    three_band_path = wrapped_path/f'{scaled_path.stem.split(\"_scaled\")[0]}.tif'\n",
    "    !gdal_translate -of GTiff -expand rgb $scaled_path $three_band_path\n",
    "    scaled_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the temporal coherence geotiff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!save_gdal.py $mint_path/temporalCoherence.h5 --of GTIFF -o $geotiff_path/TemporalCoherence.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the average spatial coherence geotiff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!save_gdal.py $mint_path/avgSpatialCoh.h5 --of GTIFF -o $geotiff_path/avgSpatialCoh.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the velocity geotiff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_name = \"velocity\"\n",
    "if correct_tropo:\n",
    "    velocity_name = f'{velocity_name}ERA5'\n",
    "vel_h5 = mint_path/f'{velocity_name}.h5'\n",
    "vel_tiff = geotiff_path/f'{velocity_name}.tif'\n",
    "!save_gdal.py $vel_h5 --of GTIFF -o $vel_tiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mintpy reference: *Yunjun, Z., H. Fattahi, F. Amelung (2019), Small baseline InSAR time series analysis: unwrapping error correction and noise reduction, preprint doi:[10.31223/osf.io/9sz6m](https://eartharxiv.org/9sz6m/).*\n",
    "\n",
    "- University of Miami online time-series viewer: https://insarmaps.miami.edu/\n",
    "\n",
    "- Mintpy Github repository: https://github.com/insarlab/MintPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MintPy_Time_Series_From_Prepared_Data_Stack.ipynb - Version 0.4.0 - June 2023*\n",
    "\n",
    "*Version Changes*\n",
    "\n",
    "- *Add option to skip data load and template creation on repeat runs*\n",
    "- *Adjust colorize_wrapped_insar function to better match \"jet\"*\n",
    "- *Use cumulative displacement for transect selection plot*\n",
    "- *Add GPS site and metadata viewer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar_analysis [conda env:.local-insar_analysis]",
   "language": "python",
   "name": "conda-env-.local-insar_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
