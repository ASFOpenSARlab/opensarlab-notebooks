{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.jpg\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"5\"> <b>Deep Learning in Earth Observation: RNN Cosine Demo</b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Lichao Mou, German Aerospace Center; Xiaoxiang Zhu, German Aerospace Center & Technical University Munich </b> <br>\n",
    "</font>\n",
    "\n",
    "<img src=\"NotebookAddons/dlr-logo-png-transparent.png\" width=\"170\" align=\"right\" border=\"2\"/> <font size=\"3\"> This notebook introduces you to the basic concepts of Deep Learning in Earth Observation. Specifically, it uses the simple example of learning the temporal pattern of a cosine curve to demonstrate the concepts of Recurrent Neural Networks (RNNs). The notebook let's you experiment with several hyper-parameters needed for training Deep Learning Networks such as RNNs, CNNs, or similar.\n",
    "    \n",
    "This notebook will introduce the following data analysis concepts:\n",
    "<br>\n",
    "- How to set up a recurrent deep network within the Python-based <i>keras/tensorflow</i> environment\n",
    "- How to create an LSTM (long-term/short-term memory) recurrent network \n",
    "- How to optimize hyper-parameters when training a deep neural network\n",
    "</font>\n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"5\" color=\"red\"> <b>Important Note about JupyterHub</b> </font>\n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"> <b>Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook.</b> </font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict a cosine wave using RNNs\n",
    "\n",
    "* A simple tutorial on LSTM and GRU to perdict a trigonometric wave.\n",
    "\n",
    "* Data noise can be added to test the robustness of the model.\n",
    "\n",
    "* Hyperparamters of the RNNs can be tweaked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"> <b>0. Importing Relevant Python Packages </b> </font>\n",
    "\n",
    "<font size=\"3\">Our first step is to <b>import the necessary python libraries into your Jupyter Notebook.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, GRU, TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from asf_notebook import new_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>1. Create a working directory for the analysis and change into it:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/jovyan/notebooks/SAR_Training/English/data_RNN_cosine\"\n",
    "new_directory(base_path)\n",
    "os.chdir(base_path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>2. Get cosine data</b></font>\n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\">Data to train and evaluate the RNN:</font>\n",
    "\n",
    "- Start, end and step define the range of the data series.\n",
    "- Sequence length defines the series to look back to train the model\n",
    "- Noisy data can be added to make the training data imperfect.\n",
    "\n",
    "<br>\n",
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to define training data:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes: starting point, end point, number of steps between points\n",
    "# number of steps to backpropagate through time, and noise to create imperfect data\n",
    "# Returns: X,Y data\n",
    "\n",
    "\n",
    "def cosine_data(start, end, step, sequence_length, noise_level=0):\n",
    "\n",
    "    t = np.arange(start, end, step)\n",
    "    cosine = np.cos(2 * np.pi * t) + noise_level * \\\n",
    "        np.random.normal(0, 1, np.shape(t))\n",
    "    cosine = cosine.reshape((cosine.shape[0], 1))\n",
    "\n",
    "    dX, dY = [], []\n",
    "    for i in range(len(cosine) - 2*sequence_length):\n",
    "        dX.append(cosine[i:i + sequence_length])\n",
    "        dY.append(cosine[i + sequence_length:i + 2*sequence_length])\n",
    "    dataX = np.array(dX)\n",
    "    dataY = np.array(dY)\n",
    "    return dataX, dataY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>3. Create an LSTM model</b></font>\n",
    "\n",
    "- Linear activation\n",
    "- Loss in mean squared error\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"><b> Write a function that creates an LSTM model:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes: number of neurons to train a GRU network, number of features to predict, and learning_rate\n",
    "# Returns: model for training\n",
    "\n",
    "\n",
    "def LSTM_(hidden_neurons, feature_count, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_dim=feature_count,\n",
    "                   output_dim=hidden_neurons, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(feature_count)))\n",
    "    model.add(Activation('linear'))\n",
    "    optimizer = RMSprop(lr=learning_rate)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>3. Create a GRU model</b></font>\n",
    "\n",
    "- Linear activation\n",
    "- Get loss using a mean squared error\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"><b>Write a function that creates a GRU model:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes number of neurons to train a GRU network, number of features to predict, and learning_rate\n",
    "# Returns: model for training\n",
    "\n",
    "\n",
    "def GRU_(hidden_neurons, feature_count, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(input_dim=feature_count,\n",
    "                  output_dim=hidden_neurons, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(feature_count)))\n",
    "    model.add(Activation('linear'))\n",
    "    optimizer = RMSprop(lr=learning_rate)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer, metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>4. Write a function to train an RNN model:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes: load RNN model, X cosine train data, Y cosine train data,\n",
    "# number of samples to be propagated through the network, and\n",
    "# number of time dataset is processed\n",
    "# Returns: training and validation loss\n",
    "\n",
    "\n",
    "def train_cosine(model, dataX, dataY, batch_size, epoch_count, count):\n",
    "\n",
    "    history = model.fit(dataX, dataY, batch_size=batch_size,\n",
    "                        epochs=epoch_count, validation_split=0.05)\n",
    "    loss_history = history.history['loss']\n",
    "    loss_history = np.array(loss_history)\n",
    "    #np.savetxt(\"loss_history.txt\", numpy_loss_history, delimiter=\",\")\n",
    "    val_loss_history = history.history['val_loss']\n",
    "    val_loss_history = np.array(val_loss_history)\n",
    "    #np.savetxt(\"val_loss_history.txt\", numpy_loss_history, delimiter=\",\")\n",
    "    loss = history.history['loss']\n",
    "    loss_val = history.history['val_loss']\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    fig = plt.figure(figsize=(8, 7))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    plt.plot(loss)\n",
    "    plt.plot(loss_val)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.savefig(f\"{os.getcwd()}/loss_{count}.png\", dpi=72)\n",
    "    plt.show()\n",
    "\n",
    "    return loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>5. Write a function to run an RNN model:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes: number of Epochs, noise level in training data, sequence length available to train a RNN,\n",
    "# learning_rate, batch_size, nb_units, boolean if results should be plotted\n",
    "# Return: Loss and Plot\n",
    "\n",
    "\n",
    "def test_cosine(EPOCHS, count, noise_level=0.3, sequence_length=100, learning_rate=1e-3, batch_size=16, nb_units=32, plot_results=False):\n",
    "\n",
    "    dataX, dataY = cosine_data(\n",
    "        0.0, 10, 0.02, sequence_length, noise_level)  # 4.0\n",
    "    # create and fit the LSTM network\n",
    "    print('creating model...')\n",
    "\n",
    "    # Choose RNN to train\n",
    "    model = LSTM_(nb_units, 1, learning_rate)\n",
    "    #model = GRU_(nb_units, 1, learning_rate)\n",
    "\n",
    "    # Train RNN model\n",
    "    tr_loss, val_loss = train_cosine(model, dataX, dataY, batch_size, EPOCHS, count)\n",
    "\n",
    "    # now test\n",
    "    dataX1, dataY1 = cosine_data(15.0, 21.0, 0.02, sequence_length)\n",
    "    predict = model.predict(dataX1)\n",
    "    if plot_results:\n",
    "        plot_RNN_results(dataX, dataX1, dataY1, predict, sequence_length, count)\n",
    "\n",
    "    return tr_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>6. Write a function to plot RNN results:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RNN_results(dataX, dataX1, dataY1, predict, sequence_length, count):\n",
    "    nan_array = np.empty((sequence_length - 1))\n",
    "    nan_array.fill(np.nan)\n",
    "    nan_array2 = np.empty(sequence_length)\n",
    "    nan_array2.fill(np.nan)\n",
    "    ind = np.arange(2*sequence_length)\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    fig = plt.figure(figsize=(8, 7))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    forecasts = np.concatenate(\n",
    "        (nan_array, dataX1[0, -1:, 0], predict[0, :, 0]))\n",
    "    ground_truth = np.concatenate(\n",
    "        (nan_array, dataX1[0, -1:, 0], dataY1[0, :, 0]))\n",
    "    network_input = np.concatenate((dataX[0, :, 0], nan_array2))\n",
    "\n",
    "    ax.plot(ind, network_input, 'b-x', label='Network input')\n",
    "    ax.plot(ind, forecasts, 'r-x', label='Many to many model forecast')\n",
    "    ax.plot(ind, ground_truth, 'g-x', label='Ground truth')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('cos(t)')\n",
    "    plt.title('Cosine Many to Many Forecast')\n",
    "    text = ax.text(-0.2,1.05, \" \", transform=ax.transAxes) #this is dummy text, needed by bbox_inches='tight', which requires >1 artist \n",
    "    lgd = ax.legend(handles, labels, bbox_to_anchor=(0.5, -0.1), loc='upper center')\n",
    "    plt.savefig(f\"{os.getcwd()}/cosine_wave_{count}.png\", dpi=72, bbox_extra_artists=(lgd, text), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\" size=\"4\"><b>7. Write functions to group and save the plots created by train_cosine() and plot_RNN_results():</b></font>\n",
    "<br><br>\n",
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to group the paths to the plots that will be concatenated:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_plot_paths(count):\n",
    "    paths = []\n",
    "    for i in range (0, count):\n",
    "        paths.append(glob.glob(f\"*_{i}.*\"))\n",
    "    for i in range (0, len(paths)):\n",
    "        paths[i].sort()\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to concatenate and save the plots:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_plots(count):\n",
    "    plot_pairs = group_plot_paths(count)\n",
    "    for i in range(0, len(plot_pairs)):\n",
    "        images = list(map(Image.open, plot_pairs[i]))\n",
    "        widths, heights = zip(*(x.size for x in images))\n",
    "        total_width = sum(widths)\n",
    "        max_height = max(heights)\n",
    "        x_offset = 0\n",
    "        new_image = Image.new('RGBA', (total_width, max_height))\n",
    "        for im in images:\n",
    "            new_image.paste(im, (x_offset, 0))\n",
    "            x_offset += im.size[0]\n",
    "        new_image.save(f\"cosine_wave_loss_{i}.png\", \"png\")\n",
    "    delete_files(plot_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>Write a function to delete the original seperate image files after they have been concatenated:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(files_list):\n",
    "    assert type(files_list) == list\n",
    "    assert type(files_list[0]) == list\n",
    "    for files in files_list:\n",
    "        for file in files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "            except:\n",
    "                FileNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"4\"><b>8. Train and run the RNN model:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_epochs = 10\n",
    "noise_level = 0.0\n",
    "sequence_length = 100\n",
    "'''\n",
    "learning_rate = 1e-3  # 1e-1, 1e-2, 1e-3, 1e-4, 1e-5\n",
    "batch_size = 16  # 1, 2, 4, 8, 16\n",
    "nb_units = 32  # 8, 16, 32, 64, 128\n",
    "\n",
    "_, _ = test_cosine(EPOCHS = nb_epochs, noise_level =noise_level, \n",
    "                   plot_results=True)\n",
    "'''\n",
    "# try with different noise\n",
    "'''\n",
    "noise_level_range = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for nl in noise_level_range:\n",
    "    tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, noise_level=nl)\n",
    "    print('tr_loss:', tr_loss)\n",
    "    print('val_loss:', val_loss)\n",
    "'''\n",
    "\n",
    "# hyperparameter: 1) lr\n",
    "learning_rate_range = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "count = 0\n",
    "for lr in learning_rate_range:\n",
    "    tr_loss, val_loss = test_cosine(nb_epochs, count, learning_rate=lr, plot_results=True)\n",
    "    count += 1\n",
    "    print('tr_loss:', tr_loss)\n",
    "    print('val_loss:', val_loss)\n",
    "concat_plots(len(learning_rate_range))\n",
    "\n",
    "'''\n",
    "# hyperparameter: 2) batch size\n",
    "batch_size_range = [2, 4, 8, 16, 32]\n",
    "for bs in batch_size_range:\n",
    "    tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, batch_size=bs, plot_results=True)\n",
    "    print('tr_loss:', tr_loss)\n",
    "    print('val_loss:', val_loss)\n",
    "'''\n",
    "'''\n",
    "# hyperparameter: 3) sequence length\n",
    "sequence_length_range = [20, 50, 100, 200, 500]\n",
    "for sl in sequence_length_range:\n",
    "    tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, sequence_length=sl, plot_results=True)\n",
    "    print('tr_loss:', tr_loss)\n",
    "    print('val_loss:', val_loss)\n",
    "'''\n",
    "'''\n",
    "# hyperparameter: 4) nb_units\n",
    "nb_units_range = [8, 16, 32, 64, 128]\n",
    "for nu in nb_units_range:\n",
    "    tr_loss, val_loss = test_cosine(EPOCHS = nb_epochs, nb_units=nu, plot_results=True)\n",
    "    print('tr_loss:', tr_loss)\n",
    "    print('val_loss:', val_loss)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
