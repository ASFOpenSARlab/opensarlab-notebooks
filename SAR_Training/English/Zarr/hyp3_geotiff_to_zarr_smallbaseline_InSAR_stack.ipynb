{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f747e7-da32-4bfa-aea8-1e4ec49f19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Union\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "import numpy as np\n",
    "import pycrs\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "import opensarlab_lib as osl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb489027-4e58-46d7-ae16-0fef88942ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO Add to opensarlab-lib\n",
    "\n",
    "# def stack_aligned(data_paths):\n",
    "#     corner_coords = [osl.get_corner_coords(i) for i in data_paths]\n",
    "#     ulx = set([c[0][0] for c in corner_coords])\n",
    "#     uly = set([c[0][1] for c in corner_coords])\n",
    "#     lrx = set([c[1][0] for c in corner_coords])\n",
    "#     lry = set([c[1][1] for c in corner_coords])\n",
    "#     return len(ulx) == len(uly) == len(lrx) == len(lry) == 1\n",
    "\n",
    "def get_corners_gdal(file):\n",
    "    ds=gdal.Open(str(file))\n",
    "    transform = ds.GetGeoTransform()\n",
    "    x = ds.RasterXSize\n",
    "    y = ds.RasterYSize\n",
    "    \n",
    "    ulx = transform[0]\n",
    "    uly = transform[3]\n",
    "    lrx = transform[0] + x * transform[1]\n",
    "    lry = transform[3] + y * transform[5]\n",
    "    \n",
    "    return {'ul': [ulx, uly], 'lr': [lrx, lry]}\n",
    "\n",
    "def get_epsg(file):\n",
    "    info = gdal.Info(str(file), format='json')\n",
    "    info = info['coordinateSystem']['wkt']\n",
    "    return info.split('ID[\"EPSG\"')[-1][1:6]\n",
    "\n",
    "def polarization_from_filename(file):\n",
    "    pol_regex = 'vv|VV|vh|VH|hh|HH|hv|HV'\n",
    "    pol = re.search(pol_regex, str(file))\n",
    "    if pol:\n",
    "        pol = pol.group(0).upper()\n",
    "    return pol\n",
    "\n",
    "def product_type_from_filename(file):\n",
    "    if \"RTC\" in str(file):\n",
    "        prod_type = \"RTC\"\n",
    "    elif \"INT\" in str(file):\n",
    "        prod_type = \"INSAR\"\n",
    "    else:\n",
    "        return\n",
    "    return prod_type\n",
    "\n",
    "def parse_proj_crs(proj_crs):\n",
    "    crs = pycrs.parse.from_ogc_wkt(proj_crs)\n",
    "    cfg_p = {}\n",
    "    cfg_p['grid_mapping_name'] = crs.name\n",
    "    cfg_p['crs_wkt'] = crs.proj.name.ogc_wkt.lower()\n",
    "\n",
    "    # Is there a better way to do this? \n",
    "    for p in crs.params:\n",
    "        if isinstance(p,pycrs.elements.parameters.LatitudeOrigin):\n",
    "            cfg_p['latitude_of_projection_origin'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.CentralMeridian):\n",
    "            cfg_p['longitude_of_central_meridian'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.FalseEasting):\n",
    "            cfg_p['false_easting'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.FalseNorthing):\n",
    "            cfg_p['false_northing'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.ScalingFactor):\n",
    "            cfg_p['scale_factor_at_centeral_meridian'] = p.value\n",
    "\n",
    "    cfg_p['projected_coordinate_system_name'] = crs.name\n",
    "    cfg_p['geographic_coordinate_system_name'] = crs.geogcs.name\n",
    "    cfg_p['horizontal_datum_name'] = crs.geogcs.datum.name.ogc_wkt\n",
    "    cfg_p['reference_ellipsoid_name'] = crs.geogcs.datum.ellips.name.ogc_wkt\n",
    "    cfg_p['semi_major_axis'] = crs.geogcs.datum.ellips.semimaj_ax.value\n",
    "    cfg_p['inverse_flattening'] = crs.geogcs.datum.ellips.inv_flat.value\n",
    "    cfg_p['longitude_of_prime_meridian'] = crs.geogcs.prime_mer.value\n",
    "    cfg_p['units'] = crs.unit.unitname.ogc_wkt\n",
    "    cfg_p['projection_x_coordinate'] = \"x\"\n",
    "    cfg_p['projection_y_coordinate'] = \"y\"\n",
    "\n",
    "    return cfg_p\n",
    "\n",
    "def dates_from_product_name(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Takes: a string or posix path to a HyP3 product\n",
    "    Returns: a string date and timestamp parsed from the name or None if none found\n",
    "    \"\"\"\n",
    "    regex = \"[0-9]{8}T[0-9]{6}_[0-9]{8}T[0-9]{6}\"\n",
    "    results = re.search(regex, str(product_name))\n",
    "    if results:\n",
    "        return results.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def datetime_from_hyp3_dt_str(date_str):\n",
    "    return datetime(int(date_str[0:4]),int(date_str[4:6]),int(date_str[6:8]),\n",
    "                    int(date_str[9:11]),int(date_str[11:13]),int(date_str[13:15]),0)\n",
    "\n",
    "def get_RTC_prod_hash(tiff):\n",
    "    stem = Path(tiff).stem\n",
    "    regex = \"(?<=gpuned_)[0-9A-Z]{4}(?=_[V|H][V|H])\"\n",
    "    p_hash = re.search(regex, str(stem))\n",
    "    if p_hash:\n",
    "        return (p_hash.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70760b-7b22-4d98-b7b8-97c0f12c708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insar_attrs(tiff):\n",
    "    ds=gdal.Open(str(tiff))\n",
    "\n",
    "    #get the product type and polarization from the filename\n",
    "    prod_type = product_type_from_filename(tiff)\n",
    "    \n",
    "    #get corner coords and extents\n",
    "    corners = get_corners_gdal(tiff)\n",
    "    x_extent = [corners['ul'][0], corners['lr'][0]]\n",
    "    y_extent = [corners['ul'][1], corners['lr'][1]]\n",
    "\n",
    "    # pixel resolution\n",
    "    geo_trans = ds.GetGeoTransform()\n",
    "    res_x = geo_trans[1]\n",
    "    res_y = geo_trans[5]\n",
    "\n",
    "    # create x and y arrays based on extents and pixel resolution\n",
    "    x_coords = np.arange(x_extent[0], x_extent[1], res_x)\n",
    "    y_coords = np.arange(y_extent[0], y_extent[1], res_y)\n",
    "\n",
    "    # get the no_data value\n",
    "    tiff_info = gdal.Info(str(tiff), format='json')\n",
    "    no_data = tiff_info['bands'][0]['noDataValue']\n",
    "\n",
    "    attrs = {\n",
    "            'institution': 'Alaska Sattelite Facility (ASF)',\n",
    "            'references': 'https://asf.alaska.edu/', \n",
    "            'source': 'SAR observation',\n",
    "            'Conventions': 'CF-1.8',\n",
    "            'platform': tiff.name[:2],\n",
    "            'product_type': prod_type,\n",
    "            'fill_value' : no_data,\n",
    "            'sensor_band_identifier' : 'C',\n",
    "            'x_spacing' : res_x,\n",
    "            'y_spacing' : res_y,\n",
    "            'title': 'SAR InSAR',\n",
    "            'long_name': f'SAR InSAR',\n",
    "            'description': f'SAR InSAR data',\n",
    "            'times': dates_from_product_name(tiff),\n",
    "        }\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07c955-61d8-49d0-8d16-e50cd2f56e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insar_product_type_from_filename(path):\n",
    "    if re.search('\\w+_corr_\\w*.tif', str(path)):\n",
    "        p_type = 'corr'\n",
    "    elif re.search('\\w+_dem_\\w*.tif', str(path)):\n",
    "        p_type = 'dem'\n",
    "    elif re.search('\\w+_lv_phi_\\w*.tif', str(path)):\n",
    "        p_type = 'lv_phi'\n",
    "    elif re.search('\\w+_lv_theta_\\w*.tif', str(path)):\n",
    "        p_type = 'lv_theta'\n",
    "    elif re.search('\\w+_unw_phase_\\w*.tif', str(path)):\n",
    "        p_type = 'unw_phase'\n",
    "    elif re.search('\\w+_water_mask_\\w*.tif', str(path)):\n",
    "        p_type = 'water_mask'\n",
    "    else:\n",
    "        p_type = None\n",
    "    return p_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b14af-7462-4e8c-b7b3-ddc1ef72bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp3_mintpy_InSAR_to_xarray(insar):\n",
    "\n",
    "    # put each product type in an ndarray\n",
    "    for f in insar:\n",
    "        ds = gdal.Open(str(f))\n",
    "        banddata = ds.GetRasterBand(1)\n",
    "        data = banddata.ReadAsArray()\n",
    "        prod_type = get_insar_product_type_from_filename(f)\n",
    "        if prod_type:\n",
    "            exec(f\"{prod_type} = np.ma.masked_invalid(data, copy=True)\")\n",
    "\n",
    "    ds=gdal.Open(str(insar[1]))\n",
    "    \n",
    "\n",
    "    # get coordinate system projection\n",
    "    # prj = ds.GetProjection()\n",
    "    # print(prj)\n",
    "    # crs = pycrs.parse.from_ogc_wkt(prj)\n",
    "    # crs_proj = crs.proj.name.ogc_wkt.lower()   \n",
    "\n",
    "    # pixel resolution\n",
    "    geo_trans = ds.GetGeoTransform()\n",
    "    res_x = geo_trans[1]\n",
    "    res_y = geo_trans[5]\n",
    "    \n",
    "    #get corner coords and extents\n",
    "    corners = get_corners_gdal(insar[0])\n",
    "    x_extent = [corners['ul'][0], corners['lr'][0]]\n",
    "    y_extent = [corners['ul'][1], corners['lr'][1]]\n",
    "\n",
    "    # create x and y arrays based on extents and pixel resolution\n",
    "    x_coords = np.arange(x_extent[0], x_extent[1], res_x)\n",
    "    y_coords = np.arange(y_extent[0], y_extent[1], res_y)\n",
    "\n",
    "    # create xarray dataset\n",
    "    data_set = xr.Dataset(\n",
    "        data_vars={\n",
    "            'y': y_coords,\n",
    "            'x': x_coords,\n",
    "            'corr': (\n",
    "                ('y', 'x'),\n",
    "                locals()['corr'].filled(0.0),\n",
    "            ),\n",
    "            'dem': (\n",
    "                ('y', 'x'),\n",
    "                locals()['dem'].filled(0.0),\n",
    "            ),\n",
    "            'lv_phi': (\n",
    "                ('y', 'x'),\n",
    "                locals()['lv_phi'].filled(0.0),\n",
    "            ),\n",
    "            'lv_theta': (\n",
    "                ('y', 'x'),\n",
    "                locals()['lv_theta'].filled(0.0),\n",
    "            ),\n",
    "            'unw_phase': (\n",
    "                ('y', 'x'),\n",
    "                locals()['unw_phase'].filled(0.0),\n",
    "            ),\n",
    "            'water_mask': (\n",
    "                ('y', 'x'),\n",
    "                locals()['water_mask'].filled(0.0),\n",
    "            ),            \n",
    "        },\n",
    "        attrs=get_insar_attrs(insar[0])\n",
    "    )\n",
    "\n",
    "    # Set x and y coord attributes\n",
    "    attrs_x = {\n",
    "        'axis': 'X',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_x_coordinate',\n",
    "        'long_name': 'Easting'\n",
    "    }\n",
    "    attrs_y = {\n",
    "        'axis': 'Y',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_y_coordinate',\n",
    "        'long_name': 'Northing'\n",
    "    }\n",
    "    for key in attrs_x:\n",
    "        data_set.x.attrs[key] = attrs_x[key]\n",
    "    for key in attrs_y:\n",
    "        data_set.y.attrs[key] = attrs_y[key]     \n",
    "\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe715854-2d6e-447f-b9b9-d3b5f081f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser(Path.cwd())\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e7459-5aca-4eba-981c-6f44d67511ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tiff_dirs = [p for p in list(Path(fc.selected_path).glob('*')) if p.is_dir()]\n",
    "stack_paths = []\n",
    "for d in tiff_dirs:\n",
    "    tiffs = list(d.glob('*_clip.tif'))\n",
    "    stack_paths.append(tiffs)\n",
    "\n",
    "insar_arrays = []\n",
    "for insar in stack_paths:\n",
    "    insar_arrays.append(hyp3_mintpy_InSAR_to_xarray(insar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce3d15-5639-412d-a306-6af0b58b45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort vv_vh_arrays by time and create a list of times for the stack time dimension\n",
    "def time_sort(a):\n",
    "    return a.times\n",
    "\n",
    "insar_arrays.sort(key=time_sort)\n",
    "times = [a.times for a in insar_arrays]\n",
    "\n",
    "# Create the stack\n",
    "stack = insar_arrays[0]\n",
    "stack = stack.drop_vars('corr')\n",
    "stack = stack.drop_vars('dem')\n",
    "stack = stack.drop_vars('lv_phi')\n",
    "stack = stack.drop_vars('lv_theta')\n",
    "stack = stack.drop_vars('unw_phase')\n",
    "stack = stack.drop_vars('water_mask')\n",
    "\n",
    "stack = stack.assign_coords(times=times)\n",
    "stack.times.attrs['axis'] = \"T\" \n",
    "stack.times.attrs['units'] = f\"timestamp in format %Y%m%dT%H%M%S\"\n",
    "stack.times.attrs['calendar'] = \"proleptic_gregorian\"\n",
    "stack.times.attrs['long_name'] = \"Time\"\n",
    "\n",
    "xarr3d = xr.concat([d.corr for d in insar_arrays], dim=stack.times)\n",
    "for d in insar_arrays:\n",
    "    d = d.drop_vars('corr')\n",
    "stack['corr'] = xarr3d\n",
    "\n",
    "xarr3d = xr.concat([d.dem for d in insar_arrays], dim=stack.times)\n",
    "for d in insar_arrays:\n",
    "    d = d.drop_vars('dem')\n",
    "stack['dem'] = xarr3d\n",
    "\n",
    "xarr3d = xr.concat([d.lv_phi for d in insar_arrays], dim=stack.times)\n",
    "for d in insar_arrays:\n",
    "    d = d.drop_vars('lv_phi')\n",
    "stack['lv_phi'] = xarr3d\n",
    "\n",
    "xarr3d = xr.concat([d.lv_theta for d in insar_arrays], dim=stack.times)\n",
    "for d in insar_arrays:\n",
    "    d = d.drop_vars('lv_theta')\n",
    "stack['lv_theta'] = xarr3d\n",
    "\n",
    "xarr3d = xr.concat([d.unw_phase for d in insar_arrays], dim=stack.times)\n",
    "for d in insar_arrays:\n",
    "    d = d.drop_vars('unw_phase')\n",
    "stack['unw_phase'] = xarr3d\n",
    "\n",
    "xarr3d = xr.concat([d.water_mask for d in insar_arrays], dim=stack.times)\n",
    "for d in insar_arrays:\n",
    "    d = d.drop_vars('water_mask')\n",
    "stack['water_mask'] = xarr3d\n",
    "\n",
    "\n",
    "del xarr3d\n",
    "del insar_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606d566-af4d-4619-8513-0fbca428a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack.to_netcdf(Path(fc.selected_path).parent/\"full_stack.nc4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21fb21b-d036-40fd-9690-3e2972480ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fa523-cc58-492f-b0f6-0a343b1631de",
   "metadata": {},
   "source": [
    "**Calculate the time optimized chunk shape given:**\n",
    "\n",
    "- 1MB < Optimal chunk size < ?MB\n",
    "    - https://zarr.readthedocs.io/en/stable/tutorial.html\n",
    "        - \"In general, chunks of at least 1 megabyte (1M) uncompressed size seem to provide better performance, at least when using the Blosc compression library.\"\n",
    "- The depth of the stack\n",
    "- The number of xarray.DataArray variables\n",
    "- square spatial chunks\n",
    "    - x chunk dimension == y chunk dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0951b-41d4-4b08-a7db-51b9fbba27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = \"s3://asf-jupyter-data-west/zarr_test/indonesia\"\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "store = s3fs.S3Map(root=s3_path, s3=s3, check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c63649-13b9-498a-ba1a-023bd970a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_per_mb = 8000000\n",
    "mb_per_chunk = 100\n",
    "bits_per_chunk = bits_per_mb * mb_per_chunk\n",
    "bits_per_pixel = 32\n",
    "pixels_per_chunk = bits_per_chunk / bits_per_pixel\n",
    "print(f'Desired pixels per chunk: {pixels_per_chunk}')\n",
    "depth = len(times)\n",
    "print(f'depth: {depth}')\n",
    "data_array_var_count = 2 # vh_backscatter and vv_backscatter\n",
    "spatial_pixels = pixels_per_chunk // (depth * data_array_var_count)\n",
    "print(f'spatial_pixels: {spatial_pixels}')\n",
    "x_y_pixels = math.floor(math.sqrt(spatial_pixels))\n",
    "print(f'x_y_pixels: {x_y_pixels}')\n",
    "print(f'Actual pixels per chunk: {depth * data_array_var_count * x_y_pixels * x_y_pixels}')\n",
    "time_optimized_chunk = (depth, x_y_pixels, x_y_pixels)\n",
    "print(time_optimized_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee77ea-2a25-4efa-925f-53a3e94b24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'stack_time_optimized_100MB_chunks'\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "encoding = {vname: {'compressor': compressor, 'chunks': (depth,x_y_pixels,x_y_pixels)} for vname in stack.data_vars}\n",
    "\n",
    "zarr_stack = stack.to_zarr(store=store, encoding=encoding, consolidated=True, group=group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90e08c-8d4d-4abc-a272-da47c9e27bdb",
   "metadata": {},
   "source": [
    "**Calculate the spatially optimized chunk shape**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c92cc-ea78-453b-8c60-deb7f33ca1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_per_mb = 8000000\n",
    "mb_per_chunk = 100\n",
    "bits_per_chunk = bits_per_mb * mb_per_chunk\n",
    "bits_per_pixel = 32\n",
    "pixels_per_chunk = bits_per_chunk / bits_per_pixel\n",
    "print(f'Desired pixels per chunk: {pixels_per_chunk}')\n",
    "depth = 1\n",
    "print(f'depth: {depth}')\n",
    "data_array_var_count = 2 # vh_backscatter and vv_backscatter\n",
    "spatial_pixels = pixels_per_chunk // (depth * data_array_var_count)\n",
    "print(f'spatial_pixels: {spatial_pixels}')\n",
    "x_y_pixels = math.floor(math.sqrt(spatial_pixels))\n",
    "print(f'x_y_pixels: {x_y_pixels}')\n",
    "print(f'Actual pixels per chunk: {depth * data_array_var_count * x_y_pixels * x_y_pixels}')\n",
    "space_optimized_chunk = (depth, x_y_pixels, x_y_pixels)\n",
    "print(space_optimized_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e30752-21bb-4e67-b65b-b5e654c05a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'stack_space_optimized_100MB_chunks'\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "encoding = {vname: {'compressor': compressor, 'chunks': (depth,x_y_pixels,x_y_pixels)} for vname in stack.data_vars}\n",
    "\n",
    "zarr_stack = stack.to_zarr(store=store, encoding=encoding, consolidated=True, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d5b7d-cb24-406f-931b-886b05b29b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc_analysis",
   "language": "python",
   "name": "conda-env-.local-rtc_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
