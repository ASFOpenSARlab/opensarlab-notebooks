{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OpenSARlab notebook banner](NotebookAddons/blackboard-banner.png)\n",
    "\n",
    "# Prepare a HyP3 smallbaseline InSAR stack and convert to a CARD4L compliant NetCDF and/or Zarr Store \n",
    "### Alex Lewandowski; Alaska Satellite Facility\n",
    "\n",
    "<img style=\"padding: 7px\" src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\"/></font>\n",
    "\n",
    "**This notebook:**\n",
    "1. Downloads all or part of a HyP3 smallbaseline InSAR stack\n",
    "1. Projects all scenes to the same EPSG\n",
    "1. Creates an xarray.Datset containing:\n",
    "  \n",
    "    \n",
    "**This notebook does NOT merge data from the same date because:**\n",
    "- The stack contains timesteps for acquisition start-times at second-scale precision\n",
    "- Each scene will have a different acquisition start-time and their data will occupy different timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note about JupyterHub\n",
    "\n",
    "**Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/rtc_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"rtc_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select the \"rtc_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"rtc_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing Relevant Python Packages\n",
    "\n",
    "In this notebook we will use the following scientific libraries:\n",
    "\n",
    "1. [GDAL](https://www.gdal.org/) is a software library for reading and writing raster and vector geospatial data formats. It includes a collection of programs tailored for geospatial data processing. Most modern GIS systems (such as ArcGIS or QGIS) use GDAL in the background.\n",
    "1. [NumPy](http://www.numpy.org/) is one of the principal packages for scientific applications of Python. It is intended for processing large multidimensional arrays and matrices, and an extensive collection of high-level mathematical functions and implemented methods makes it possible to perform various operations with these objects.\n",
    "\n",
    "**Our first step is to import them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import copy\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json # for loads\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from tqdm.auto import tqdm \n",
    "from typing import Union\n",
    "import warnings\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pycrs\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import yaml\n",
    "import zarr\n",
    "\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "import asf_search\n",
    "import opensarlab_lib as osl\n",
    "from hyp3_sdk import Batch, HyP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Your Own Data Stack Into the Notebook\n",
    "\n",
    "This notebook assumes that you've created an RTC data stack over your personal area of interest using the [Alaska Satellite Facility's](https://www.asf.alaska.edu/) value-added product system HyP3, available via [ASF Data Search (Vertex)](https://search.asf.alaska.edu/#/). HyP3 is an ASF service used to prototype value added products and provide them to users to collect feedback.\n",
    "\n",
    "We will retrieve HyP3 data via the hyp3_sdk or work with previously downloaded data. As both HyP3 and the Notebook environment sit in the [Amazon Web Services (AWS)](https://aws.amazon.com/) cloud, data transfer is quick and cost effective.\n",
    "\n",
    "---\n",
    "\n",
    "If downloading data, create a data directory in which to download dual-pol HyP3 RTC products.\n",
    "\n",
    "If working with previously downloaded dual-pol HyP3 RTCs, each product should contain VH and VV  or HH and HV data, the HyP3 log file, and the HyP3 product README in subdirectories of the data directory:\n",
    "\n",
    "```\n",
    "data_directory   \n",
    "│\n",
    "└───product_1_directory\n",
    "│   │   *_VH.tif\n",
    "│   │   *_VV.tif\n",
    "│   │   *.README.md.txt\n",
    "│   │   *.log\n",
    "│   ...\n",
    "│   \n",
    "└───product_2_directory\n",
    "│   │   *_VH.tif\n",
    "│   │   *_VV.tif\n",
    "│   │   *.README.md.txt\n",
    "│   │   *.log\n",
    "│   ...\n",
    "│ \n",
    "...\n",
    "```\n",
    "\n",
    "**Select or create a data directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Download HyP3 data or work with previously downloaded data?\")\n",
    "data_source = osl.select_parameter(['Download data from HyP3', 'Use existing data'])\n",
    "display(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = 'Download' in data_source.value\n",
    "if download:\n",
    "    choice = None\n",
    "    while True:\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "        data_dir = Path(input(f\"\\nPlease enter the name of a directory in which to store your downloaded data.\"))\n",
    "        if data_dir == Path('.'):\n",
    "            continue\n",
    "        if data_dir.is_dir():\n",
    "            contents = data_dir.glob('*')\n",
    "            if len(list(contents)) > 0:\n",
    "                choice = osl.handle_old_data(data_dir)\n",
    "                if choice == 1:\n",
    "                    if data_dir.exists():\n",
    "                        shutil.rmtree(data_dir)\n",
    "                    data_dir.mkdir()\n",
    "                    break\n",
    "                elif choice == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    clear_output()\n",
    "                    continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            data_dir.mkdir()\n",
    "            break\n",
    "else:\n",
    "    print(\"Select your data directory\")\n",
    "    fc = FileChooser(Path.cwd())\n",
    "    display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Define absolute path to  analysis directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    data_directory = Path.cwd()/data_dir\n",
    "else:\n",
    "    data_directory = Path(fc.selected_path)\n",
    "\n",
    "print(f\"data_directory: {data_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a HyP3 object and authenticate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if download:\n",
    "    hyp3 = HyP3(prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide whether to search for a HyP3 project or jobs unattached to a project:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download: \n",
    "    options = ['project', 'projectless jobs']\n",
    "    search_type = osl.select_parameter(options, '')\n",
    "    print(\"Select whether to search for HyP3 Project or HyP3 Jobs unattached to a project\")\n",
    "    display(search_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List projects containing active products of the type chosen in the previous cell and select one:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    my_hyp3_info = hyp3.my_info()\n",
    "    active_projects = dict()\n",
    "\n",
    "    if search_type.value == 'project':\n",
    "        for project in my_hyp3_info['job_names']:\n",
    "            batch = Batch()\n",
    "            batch = hyp3.find_jobs(name=project, job_type='INSAR_GAMMA').filter_jobs(running=False, include_expired=False)\n",
    "            if len(batch) > 0:\n",
    "                active_projects.update({batch.jobs[0].name: batch})\n",
    "\n",
    "        if len(active_projects) > 0:\n",
    "            display(Markdown(\"<text style='color:darkred;'>Note: After selecting a project, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "            display(Markdown(\"<text style='color:darkred;'>Otherwise, you will rerun this code cell.</text>\"))\n",
    "            print('\\nSelect a Project:')\n",
    "            project_select = osl.select_parameter(active_projects)\n",
    "            display(project_select)\n",
    "    if search_type.value == 'projectless jobs' or len(active_projects) == 0:\n",
    "        project_select = False\n",
    "        if search_type.value == 'project':\n",
    "            print(f\"There were no {'INSAR_GAMMA'} jobs found in any current projects.\\n\")\n",
    "        jobs = hyp3.find_jobs(job_type='RTC_GAMMA').filter_jobs(running=False, include_expired=False)\n",
    "        orphaned_jobs = Batch()\n",
    "        for j in jobs:\n",
    "            if not j.name:\n",
    "                orphaned_jobs += j\n",
    "        jobs = orphaned_jobs\n",
    "\n",
    "        if len(jobs) > 0:\n",
    "            print(f\"Found {len(jobs)} {'INSAR_GAMMA'} jobs that are not part of a project.\")\n",
    "            print(f\"Select the jobs you wish to download\")\n",
    "            jobs = {i.files[0]['filename']: i for i in jobs}\n",
    "            jobs_select = osl.select_mult_parameters(jobs, '', width='500px')\n",
    "            display(jobs_select)\n",
    "        else:\n",
    "            print(f\"There were no {'INSAR_GAMMA'} jobs found that are not part of a project either.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a date range of products to download:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    if project_select:\n",
    "        batch = project_select.value\n",
    "    else:\n",
    "        batch = Batch()\n",
    "        for j in jobs_select.value:\n",
    "            batch += j\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a date range, you should select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you may simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Date Range:')\n",
    "    dates = osl.get_job_dates(batch)\n",
    "    date_picker = osl.gui_date_picker(dates)\n",
    "    display(date_picker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected date range and remove products falling outside of it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if download:\n",
    "    date_range = osl.get_slider_vals(date_picker)\n",
    "    date_range[0] = date_range[0].date()\n",
    "    date_range[1] = date_range[1].date()\n",
    "    print(f\"Date Range: {str(date_range[0])} to {str(date_range[1])}\")\n",
    "    batch = osl.filter_jobs_by_date(batch, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather the available paths and orbit directions for the remaining products:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    display(Markdown(\"<text style='color:darkred;'><text style='font-size:150%;'>This may take some time for projects containing many jobs...</text></text>\"))\n",
    "    osl.set_paths_orbits(batch)\n",
    "    paths = set()\n",
    "    orbit_directions = set()\n",
    "    for p in batch:\n",
    "        paths.add(p.path)\n",
    "        orbit_directions.add(p.orbit_direction)\n",
    "    paths.add('All Paths')\n",
    "    display(Markdown(f\"<text style=color:blue><text style='font-size:175%;'>Done.</text></text>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Select a path or paths (use shift or ctrl to select multiple paths):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a path, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Path:')\n",
    "    path_choice = osl.select_mult_parameters(paths)\n",
    "    display(path_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected flight path/s:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    flight_path = path_choice.value\n",
    "    if flight_path:\n",
    "        if flight_path:\n",
    "            print(f\"Flight Path: {flight_path}\")\n",
    "        else:\n",
    "            print('Flight Path: All Paths')\n",
    "    else:\n",
    "        print(\"WARNING: You must select a flight path in the previous cell, then rerun this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select an orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    if len(orbit_directions) > 1:\n",
    "        display(Markdown(\"<text style='color:red;'>Note: After selecting a flight direction, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "        display(Markdown(\"<text style='color:red;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Flight Direction:')\n",
    "    direction_choice = osl.select_parameter(orbit_directions, 'Direction:')\n",
    "    display(direction_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    direction = direction_choice.value\n",
    "    print(f\"Orbit Direction: {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter jobs by path and orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    batch = osl.filter_jobs_by_path(batch, flight_path)\n",
    "    batch = osl.filter_jobs_by_orbit(batch, direction)\n",
    "    print(f\"There are {len(batch)} products to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the products, unzip them into a directory named after the product type, and delete the zip files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if download:\n",
    "    print(f\"\\nProject: {batch.jobs[0].name}\")\n",
    "    project_zips = batch.download_files(data_directory)\n",
    "    for z in project_zips:\n",
    "        osl.asf_unzip(str(data_directory), str(z))\n",
    "        z.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect the paths to the GeoTiffs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_paths = list(data_directory.glob('*/*.tif'))\n",
    "# product_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fix multiple UTM Zone-related issues\n",
    "\n",
    "Fix multiple UTM Zone-related issues should they exist in your data set. If multiple UTM zones are found, the following code cells will identify the predominant UTM zone and reproject the rest into that zone. This step must be completed prior to merging frames or performing any analysis. AutoRIFT products do not come with projection metadata and so will not be reprojected.\n",
    "\n",
    "**Use gdal.Info to determine the UTM definition types and zones in each product:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones = []\n",
    "utm_types = []\n",
    "print('Checking UTM Zones in the data stack ...\\n')\n",
    "for k in range(0, len(product_paths)):\n",
    "    info = (gdal.Info(str(product_paths[k]), options = ['-json']))\n",
    "    info = json.dumps(info)\n",
    "    info = (json.loads(info))['coordinateSystem']['wkt']\n",
    "    zone = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "    utm_zones.append(zone)\n",
    "    typ = info.split('ID')[-1].split('\"')[1]\n",
    "    utm_types.append(typ)\n",
    "print(f\"UTM Zones:\\n {utm_zones}\\n\")\n",
    "print(f\"UTM Types:\\n {utm_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify the most commonly used UTM Zone in the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_unique, counts = np.unique(utm_zones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "predominant_utm = utm_unique[a][0]\n",
    "print(f\"Predominant UTM Zone: {predominant_utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reproject all tiffs to the predominate UTM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_indicies = [i for i, j in enumerate(utm_zones) if j != predominant_utm] #makes list of indicies in utm_zones that need to be reprojected\n",
    "print('--------------------------------------------')\n",
    "print('Reprojecting %4.1f files' %(len(reproject_indicies)))\n",
    "print('--------------------------------------------')\n",
    "for k in reproject_indicies:\n",
    "    temppath = f\"{str(product_paths[k].parent)}/r{product_paths[k].name}\"\n",
    "    print(temppath)  \n",
    "\n",
    "    cmd = f\"gdalwarp -overwrite {product_paths[k]} {temppath} -s_srs {utm_types[k]}:{utm_zones[k]} -t_srs EPSG:{predominant_utm}\"\n",
    "#     print(cmd)\n",
    "    !{cmd}\n",
    "\n",
    "    product_paths[k].unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_paths = list(data_directory.glob('*/*.tif'))\n",
    "product_paths.sort()\n",
    "for p in product_paths:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tidy these functions up, add type hints and doc strings, add to opensarlab-lib\n",
    "\n",
    "def dates_from_product_name(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Takes: a string or posix path to a HyP3 product\n",
    "    Returns: a string date and timestamp parsed from the name or None if none found\n",
    "    \"\"\"\n",
    "    regex = \"[0-9]{8}T[0-9]{6}_[0-9]{8}T[0-9]{6}\"\n",
    "    results = re.search(regex, str(product_name))\n",
    "    if results:\n",
    "        return results.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_hyp3_log_val(log_path, regex):\n",
    "    with open(str(log_path), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            val = re.search(regex, line)\n",
    "            if val:\n",
    "                return val.group(0)\n",
    "            \n",
    "def get_beam_IDs(log_path, regex):\n",
    "    with open(str(log_path), 'r') as f:\n",
    "        ids = None\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            val = re.search(regex, line)\n",
    "            if val and not ids:\n",
    "                ids = val.group(0)\n",
    "            elif val and ids:\n",
    "                ids = f'{ids}, {val.group(0)}'\n",
    "    return ids\n",
    "            \n",
    "def mission_from_filename(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    regex = 'S1(A|B|C)'\n",
    "    results = re.search(regex, Path(product_name).name)\n",
    "    if results:\n",
    "        return results.group(0)\n",
    "    else:\n",
    "        return None\n",
    "            \n",
    "def observation_mode_from_filename(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    return Path(product_name).name.split('_')[1]\n",
    "\n",
    "def orbit_data_source_from_filename(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    orbit =  Path(product_name).name.split('_')[3][2]\n",
    "    if orbit == 'P':\n",
    "        return 'Precise'\n",
    "    elif orbit == 'R':\n",
    "        return 'Restituted'\n",
    "    elif orbit == 'O':\n",
    "        return 'Original Predicted'\n",
    "    \n",
    "def get_corners_gdal(file):\n",
    "    ds=gdal.Open(str(file))\n",
    "    transform = ds.GetGeoTransform()\n",
    "    x = ds.RasterXSize\n",
    "    y = ds.RasterYSize\n",
    "    \n",
    "    ulx = transform[0]\n",
    "    uly = transform[3]\n",
    "    lrx = transform[0] + x * transform[1]\n",
    "    lry = transform[3] + y * transform[5]\n",
    "    \n",
    "    return {'ul': [ulx, uly], 'lr': [lrx, lry]}\n",
    "    \n",
    "def parse_proj_crs(proj_crs):\n",
    "    crs = pycrs.parse.from_ogc_wkt(proj_crs)\n",
    "    cfg_p = {}\n",
    "    cfg_p['grid_mapping_name'] = crs.name\n",
    "    cfg_p['crs_wkt'] = crs.proj.name.ogc_wkt.lower()\n",
    "\n",
    "    # Is there a better way to do this? \n",
    "    for p in crs.params:\n",
    "        if isinstance(p,pycrs.elements.parameters.LatitudeOrigin):\n",
    "            cfg_p['latitude_of_projection_origin'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.CentralMeridian):\n",
    "            cfg_p['longitude_of_central_meridian'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.FalseEasting):\n",
    "            cfg_p['false_easting'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.FalseNorthing):\n",
    "            cfg_p['false_northing'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.ScalingFactor):\n",
    "            cfg_p['scale_factor_at_centeral_meridian'] = p.value\n",
    "\n",
    "    cfg_p['projected_coordinate_system_name'] = crs.name\n",
    "    cfg_p['geographic_coordinate_system_name'] = crs.geogcs.name\n",
    "    cfg_p['horizontal_datum_name'] = crs.geogcs.datum.name.ogc_wkt\n",
    "    cfg_p['reference_ellipsoid_name'] = crs.geogcs.datum.ellips.name.ogc_wkt\n",
    "    cfg_p['semi_major_axis'] = crs.geogcs.datum.ellips.semimaj_ax.value\n",
    "    cfg_p['inverse_flattening'] = crs.geogcs.datum.ellips.inv_flat.value\n",
    "    cfg_p['longitude_of_prime_meridian'] = crs.geogcs.prime_mer.value\n",
    "    cfg_p['units'] = crs.unit.unitname.ogc_wkt\n",
    "    cfg_p['projection_x_coordinate'] = \"x\"\n",
    "    cfg_p['projection_y_coordinate'] = \"y\"\n",
    "\n",
    "    return cfg_p\n",
    "\n",
    "def get_InSAR_prod_id(tiff):\n",
    "    stem = Path(tiff).stem\n",
    "    regex = \"(?<=_G_[u|w][e|c][1|2|3|4|F]_)\\w{4}(?=_)\"\n",
    "    p_hash = re.search(regex, str(stem))\n",
    "    if p_hash:\n",
    "        return (p_hash.group(0))\n",
    "    \n",
    "def get_insar_product_type_from_filename(path):\n",
    "    if re.search('\\w+_amp\\w*.tif', str(path)):\n",
    "        p_type = 'amp'    \n",
    "    elif re.search('\\w+_corr\\w*.tif', str(path)):\n",
    "        p_type = 'corr'\n",
    "    elif re.search('\\w+_dem\\w*.tif', str(path)):\n",
    "        p_type = 'dem'\n",
    "    elif re.search('\\w+_lv_phi\\w*.tif', str(path)):\n",
    "        p_type = 'lv_phi'\n",
    "    elif re.search('\\w+_lv_theta\\w*.tif', str(path)):\n",
    "        p_type = 'lv_theta'\n",
    "    elif re.search('\\w+_unw_phase\\w*.tif', str(path)):\n",
    "        p_type = 'unw_phase'\n",
    "    elif re.search('\\w+_water_mask\\w*.tif', str(path)):\n",
    "        p_type = 'water_mask'\n",
    "    else:\n",
    "        p_type = None\n",
    "    return p_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an xarray Dataset containing the RTC stack\n",
    "\n",
    "### 3.1. Write Functions to gather the needed CARD4L metadata from the backscatter GeoTiffs and log files\n",
    "\n",
    "**Write a function to gather the metadata needed for each dual-pol pair:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_product_InSAR_vars(tiff):\n",
    "    f = gdal.Open(str(tiff))\n",
    "    info = gdal.Info(str(tiff), format='json')\n",
    "    \n",
    "    source_file_name = Path(info['files'][0]).name\n",
    "    \n",
    "    prod_id = get_InSAR_prod_id(tiff)\n",
    "    \n",
    "    try:\n",
    "        metadata_path = list(Path(tiff).parent.glob(f'*_{prod_id}.txt'))[0]\n",
    "    except:\n",
    "        print(\"Metadata file not found\")\n",
    "        raise\n",
    "        \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        metadata = dict()\n",
    "        for l in lines:\n",
    "            kv = l.strip().split(':')\n",
    "            metadata.update(json.loads(f'{{\"{kv[0]}\": \"{kv[1].strip()}\"}}'))\n",
    "\n",
    "    try:\n",
    "        readme_path = list(Path(tiff).parent.glob('*.README.md.txt'))[0]\n",
    "    except:\n",
    "        print(\"README not found\")\n",
    "        raise\n",
    "        \n",
    "    date_regex = '[0-9]{2}/[0-9]{2}/[0-9]{4} [0-9]{2}:[0-9]{2}:[0-9]{2} [A|P]M'\n",
    "    \n",
    "    speckle = metadata['Speckle filter'].lower() == 'yes'\n",
    "    range_bandpass_filter = metadata['Range bandpass filter'].lower() == 'yes'\n",
    "    azimuth_bandpass_filter = metadata['Azimuth bandpass filter'].lower() == 'yes'\n",
    "    \n",
    "    ref_gran = metadata['Reference Granule']\n",
    "    ref_metadata = asf_search.granule_search(ref_gran).geojson()\n",
    "    \n",
    "    var = {\n",
    "        'Pedigree': {\n",
    "            'ReferenceGranule': ref_gran,\n",
    "            'SecondaryGranule': metadata['Secondary Granule'],\n",
    "            'ProductName': '_'.join(Path(tiff).stem.split('_')[:-1]),\n",
    "        },\n",
    "        'SensorParameters': {\n",
    "            'Sensor': ref_metadata['features'][1]['properties']['sensor'],\n",
    "            'RadarFrequency': '',\n",
    "            'ADCSamplingRate': '',\n",
    "            'ChirpBandwidth': '',\n",
    "            'PRF': '',\n",
    "            'AzimuthProcBandwidth': '',\n",
    "            'RangePixelSpacing': '',\n",
    "        },\n",
    "        'SceneParameters': {\n",
    "            'Date': '',\n",
    "            'StartTime': '',\n",
    "            'CenterTime': '',\n",
    "            'EndTime': '',\n",
    "            'AzimuthLineTime': '',\n",
    "            'RangeSamples': '',\n",
    "            'AzimuthLines': '',\n",
    "            'CenterLatitude': '',\n",
    "            'CenterLongitude': '',\n",
    "            'Heading': float(metadata['Heading']),\n",
    "            'AzimuthPixelSpacing': '',\n",
    "            'NearRangeSLC': '',\n",
    "            'CenterRangeSLC': '',\n",
    "            'FarRangeSLC': '',\n",
    "            'IncidenceAngle': '',\n",
    "            'SARToEarthCenter': '',\n",
    "            'EarthRadiusAtNadir': float(metadata['Earth radius at nadir']),\n",
    "            'EarthSemiMajor': '',\n",
    "            'EarthSemiMinor': '',\n",
    "            'PerpendicularBaseline': float(metadata['Baseline']),\n",
    "        },\n",
    "        \n",
    "        'StateVectors': {\n",
    "            'StateVectorQuantity': '',\n",
    "            'TimeFirstStateVector': '',\n",
    "            'StateVectorInterval': '',\n",
    "            'StateVectorPosition1': '',\n",
    "            'StateVectorVelocity1': '',\n",
    "            \n",
    "        },\n",
    "        'ProcessingParameters': {\n",
    "            'DopplerPolynomial': '',\n",
    "            'ZeroDoppler': '',\n",
    "            'AzimuthLooks': int(metadata['Azimuth looks']),\n",
    "            'RangeLooks': int(metadata['Range looks']),\n",
    "            'PhaseFilter': metadata['INSAR phase filter'],\n",
    "            'PhaseFilterParameter': float(metadata['Phase filter parameter']),\n",
    "            'PixelSpacing': 20 * int(metadata['Azimuth looks']),\n",
    "            'RangeBandpassFilter': range_bandpass_filter,\n",
    "            'AzimuthBandpassFilter': azimuth_bandpass_filter,\n",
    "            'DEMSource': metadata['DEM source'],\n",
    "            'DEMResolution': int(metadata['DEM resolution (m)']),\n",
    "            'UTCTime': float(metadata['UTC time']),\n",
    "            'UnwrappingType': metadata['Unwrapping type'],\n",
    "            'UnwrappingThreshold': metadata['Unwrapping threshold'],\n",
    "            'SpeckleFilter': speckle,\n",
    "            \n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_per_product_InSAR_vars(product_paths[0])\n",
    "print(yaml.dump(d, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_backscatter_stack_attrs(tiff):\n",
    "    f = gdal.Open(str(tiff))\n",
    "    info = gdal.Info(str(tiff), format='json')\n",
    "    \n",
    "    source_file_name = Path(info['files'][0]).name\n",
    "    \n",
    "    mission = mission_from_filename(source_file_name)\n",
    "    polarization = polarization_from_filename(source_file_name)\n",
    "    \n",
    "    try:\n",
    "        log_path = list(Path(tiff).parent.glob('*.log'))[0]\n",
    "    except:\n",
    "        print(\"Log file not found\")\n",
    "        raise\n",
    "        \n",
    "    date_regex = '[0-9]{2}/[0-9]{2}/[0-9]{4} [0-9]{2}:[0-9]{2}:[0-9]{2} [A|P]M'\n",
    "    \n",
    "    observation_mode = observation_mode_from_filename(source_file_name)\n",
    "    \n",
    "    pulse_rep_freq_regex = f'(?<={date_regex} - INFO - Proc: effective PRF derived from azimuth).*: [0-9]{{,5}}\\.[0-9]{{,10}}(?=\\n)'\n",
    "    pulse_rep_freq = get_hyp3_log_val(log_path, pulse_rep_freq_regex)\n",
    "    try:\n",
    "        pulse_rep_freq = float(pulse_rep_freq.split(' ')[-1])\n",
    "    except: \n",
    "        print(\"No valid Pulse Repetition Frequency found\") \n",
    "        raise\n",
    "    \n",
    "    projection = f.GetProjection()\n",
    "    epsg = int(projection.split('AUTHORITY[\"EPSG\",\"')[-1].split('\"')[0])\n",
    "    \n",
    "    if epsg == 4326:\n",
    "        y = 'Latitude'\n",
    "        x = 'Longitude'\n",
    "    else:\n",
    "        y = 'Northing'\n",
    "        x = 'Easting'\n",
    "        \n",
    "    azimuth_regex = '(?<=(azimuth angle: ))[0-9]{,4}\\.[0-9]{,6} \\((right|left) looking\\)'\n",
    "    azimuth = get_hyp3_log_val(log_path, azimuth_regex)\n",
    "    antenna_pointing = azimuth.split(' ')[1][1:]\n",
    "    \n",
    "    beam_id_regex = f'(?<={date_regex} - INFO - Proc: sensor: {mission} {observation_mode} )\\w{{,4}}(?= {polarization}\\n)'\n",
    "    beam_ids = get_beam_IDs(log_path, beam_id_regex)\n",
    "        \n",
    "    attrs =  {\n",
    "        'CoordinateReferenceSystem': {\n",
    "            'EPSG': epsg,\n",
    "            'WKT': projection,\n",
    "        },\n",
    "        'DataCollectionTime': {\n",
    "            'NumberOfAcquisitions': None,\n",
    "            'FirstAcquisitionDate': None,\n",
    "            'LastAcquisitionDate': None,\n",
    "        },\n",
    "        'PixelCoordinateConvention': f.GetMetadata()['AREA_OR_POINT'],\n",
    "        'Product': 'Normalized Radar Backscatter (Radiometrically Terrain-Corrected)',\n",
    "        'SourceAttributes': {\n",
    "            'Instrument': 'C-SAR',\n",
    "            'SourceDataAcquisitionParameters': {\n",
    "                'RadarBand': 'C',\n",
    "                'RadarCenterFrequency': pulse_rep_freq,\n",
    "                'ObservationMode': observation_mode,\n",
    "                'Polarizations': ' '.join(get_polarizations_in_dir(Path(tiff).parent)),\n",
    "                'AntennaPointing': antenna_pointing,\n",
    "                'BeamID': beam_ids,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print an example call to `get_backscatter_stack_attrs`\n",
    "\n",
    "# d = get_backscatter_stack_attrs(product_paths[0])\n",
    "# print(yaml.dump(d, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Write a function to create an xarray.Dataset for a dual-pol pair of RTCs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp3_mintpy_InSAR_to_xarray(insar_dir):\n",
    "    \n",
    "    insar_paths = list(Path(insar_dir).glob('*.tif'))\n",
    "    insar_paths.sort()\n",
    "    \n",
    "    insar_vars = get_per_product_InSAR_vars(insar_paths[1])\n",
    "    \n",
    "    # put each product type in an ndarray\n",
    "    for f in insar_paths:\n",
    "        if '.dem' not in str(f):\n",
    "            ds = gdal.Open(str(f))\n",
    "            banddata = ds.GetRasterBand(1)\n",
    "            data = banddata.ReadAsArray()\n",
    "            prod_type = get_insar_product_type_from_filename(f)\n",
    "            if prod_type:\n",
    "                exec(f\"{prod_type} = np.ma.masked_invalid(data, copy=True)\")\n",
    "\n",
    "    ds=gdal.Open(str(insar_paths[1]))\n",
    "    \n",
    "    # pixel resolution\n",
    "    geo_trans = ds.GetGeoTransform()\n",
    "    res_x = geo_trans[1]\n",
    "    res_y = geo_trans[5]\n",
    "    \n",
    "    #get corner coords and extents\n",
    "    corners = get_corners_gdal(insar_paths[0])\n",
    "    x_extent = [corners['ul'][0], corners['lr'][0]]\n",
    "    y_extent = [corners['ul'][1], corners['lr'][1]]\n",
    "\n",
    "    # create x and y arrays based on extents and pixel resolution\n",
    "    x_coords = np.arange(x_extent[0], x_extent[1], res_x)\n",
    "    y_coords = np.arange(y_extent[0], y_extent[1], res_y)\n",
    "\n",
    "    # create xarray dataset\n",
    "    data_set = xr.Dataset(\n",
    "        data_vars={\n",
    "            'y': y_coords,\n",
    "            'x': x_coords,\n",
    "            'amp': (\n",
    "                ('y', 'x'),\n",
    "                locals()['amp'].filled(0.0),\n",
    "            ),\n",
    "            'corr': (\n",
    "                ('y', 'x'),\n",
    "                locals()['corr'].filled(0.0),\n",
    "            ),\n",
    "            'lv_phi': (\n",
    "                ('y', 'x'),\n",
    "                locals()['lv_phi'].filled(0.0),\n",
    "            ),\n",
    "            'lv_theta': (\n",
    "                ('y', 'x'),\n",
    "                locals()['lv_theta'].filled(0.0),\n",
    "            ),\n",
    "            'unw_phase': (\n",
    "                ('y', 'x'),\n",
    "                locals()['unw_phase'].filled(0.0),\n",
    "            ),\n",
    "            'water_mask': (\n",
    "                ('y', 'x'),\n",
    "                locals()['water_mask'].filled(0.0),\n",
    "            ),            \n",
    "        },\n",
    "        attrs=None\n",
    "    )\n",
    "\n",
    "    # Set x and y coord attributes\n",
    "    attrs_x = {\n",
    "        'axis': 'X',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_x_coordinate',\n",
    "        'long_name': 'Easting'\n",
    "    }\n",
    "    attrs_y = {\n",
    "        'axis': 'Y',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_y_coordinate',\n",
    "        'long_name': 'Northing'\n",
    "    }\n",
    "    for key in attrs_x:\n",
    "        data_set.x.attrs[key] = attrs_x[key]\n",
    "    for key in attrs_y:\n",
    "        data_set.y.attrs[key] = attrs_y[key]  \n",
    "        \n",
    "    for k in insar_vars.keys():\n",
    "        data_set[k] = json.dumps(insar_vars[k])\n",
    "\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backscatter_pair_to_xarray(polar_pair):   \n",
    "    bsv_gen = get_per_pair_RTC_vars(polar_pair[0])\n",
    "    \n",
    "    polarizations = get_polarizations_in_dir(Path(polar_pair[0]).parent)\n",
    "    \n",
    "    for tiff in polar_pair:\n",
    "        bsv = get_per_pol_RTC_vars(tiff)\n",
    "        ds=gdal.Open(str(tiff))\n",
    "        polarization = bsv['BackscatterMeasurementData']['Polarization']\n",
    "        banddata = ds.GetRasterBand(1)\n",
    "        data = banddata.ReadAsArray()\n",
    "        \n",
    "        if polarization == 'VH' or polarization == 'HH':\n",
    "            p1_backscatter = np.ma.masked_invalid(data, copy=True)\n",
    "            p1_BackscatterMeasurementData = json.dumps(bsv['BackscatterMeasurementData'])\n",
    "            p1_DocumentIdentifier = bsv['DocumentIdentifier']\n",
    "            \n",
    "        elif polarization == 'VV' or polarization == 'HV':\n",
    "            p2_backscatter = np.ma.masked_invalid(data, copy=True)\n",
    "            p2_BackscatterMeasurementData = json.dumps(bsv['BackscatterMeasurementData'])\n",
    "            p2_DocumentIdentifier = bsv['DocumentIdentifier']\n",
    "\n",
    " \n",
    "    ds=gdal.Open(str(polar_pair[0]))\n",
    "\n",
    "    # get coordinate system projection\n",
    "    prj = ds.GetProjection()\n",
    "    crs = pycrs.parse.from_ogc_wkt(prj)\n",
    "    crs_proj = crs.proj.name.ogc_wkt.lower()   \n",
    "\n",
    "    # pixel resolution\n",
    "    geo_trans = ds.GetGeoTransform()\n",
    "    res_x = geo_trans[1]\n",
    "    res_y = geo_trans[5]\n",
    "    \n",
    "    #get corner coords and extents\n",
    "    corners = get_corners_gdal(polar_pair[0])\n",
    "    x_extent = [corners['ul'][0], corners['lr'][0]]\n",
    "    y_extent = [corners['ul'][1], corners['lr'][1]]\n",
    "\n",
    "    # create x and y arrays based on extents and pixel resolution\n",
    "    x_coords = np.arange(x_extent[0], x_extent[1], res_x)\n",
    "    y_coords = np.arange(y_extent[0], y_extent[1], res_y)\n",
    "\n",
    "    # create xarray dataset\n",
    "    data_set = xr.Dataset(\n",
    "        data_vars={\n",
    "            'y': y_coords,\n",
    "            'x': x_coords,\n",
    "            f'{polarizations[0]}_backscatter': (\n",
    "                ('y', 'x'),\n",
    "                p1_backscatter.filled(0.0),\n",
    "            ),\n",
    "            f'{polarizations[1]}_backscatter': (\n",
    "                ('y', 'x'),\n",
    "                p2_backscatter.filled(0.0),\n",
    "            ),\n",
    "            f'{polarizations[0]}_BackscatterMeasurementData': p1_BackscatterMeasurementData,\n",
    "            f'{polarizations[1]}_BackscatterMeasurementData': p2_BackscatterMeasurementData,\n",
    "            f'{polarizations[0]}_DocumentIdentifier': p1_DocumentIdentifier,\n",
    "            f'{polarizations[1]}_DocumentIdentifier': p2_DocumentIdentifier,\n",
    "        },\n",
    "        attrs=None\n",
    "    )\n",
    "\n",
    "    # Set x and y coord attributes\n",
    "    attrs_x = {\n",
    "        'axis': 'X',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_x_coordinate',\n",
    "        'long_name': 'Easting'\n",
    "    }\n",
    "    attrs_y = {\n",
    "        'axis': 'Y',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_y_coordinate',\n",
    "        'long_name': 'Northing'\n",
    "    }\n",
    "    for key in attrs_x:\n",
    "        data_set.x.attrs[key] = attrs_x[key]\n",
    "    for key in attrs_y:\n",
    "        data_set.y.attrs[key] = attrs_y[key]\n",
    "        \n",
    "    for k in bsv_gen.keys():\n",
    "        data_set[k] = json.dumps(bsv_gen[k])\n",
    "    \n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create the xarray stack\n",
    "\n",
    "**Create a list of paths to each InSAR directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_date(a):\n",
    "    return osl.date_from_product_name(a)\n",
    "\n",
    "insar_dirs = [p for p in list(data_directory.glob('*')) if p.is_dir() and '.' not in str(p)]\n",
    "insar_dirs.sort(key=sort_by_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a list of xarray.Datsets for each InSAR product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_arrays = []\n",
    "for d in tqdm(insar_dirs):\n",
    "    insar_arrays.append(hyp3_mintpy_InSAR_to_xarray(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insar_arrays[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare an `xarray.Dataset` to hold the stack**\n",
    "\n",
    "- Contains the same x/y dimensions as stack data\n",
    "- Contains a time dimension for the timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [dates_from_product_name(d) for d in insar_dirs]\n",
    "\n",
    "stack = insar_arrays[0]\n",
    "variables = list(stack.variables)\n",
    "\n",
    "for v in variables:\n",
    "    if v not in ['x', 'y']:\n",
    "        stack = stack.drop_vars(v)\n",
    "        \n",
    "stack = stack.assign_coords(time=times)\n",
    "stack.time.attrs['axis'] = \"T\" \n",
    "stack.time.attrs['units'] = f\"timestamps in format %Y%m%dT%H%M%S_%Y%m%dT%H%M%S\"\n",
    "stack.time.attrs['calendar'] = \"proleptic_gregorian\"\n",
    "stack.time.attrs['long_name'] = \"Time\" \n",
    "stack.time.attrs['description'] = \"<reference scene acquisition time>_<secondary scene acquisition time>\" \n",
    "\n",
    "print(\"Take a look at the stack.\")\n",
    "print(\"It should contain 'x', 'y', and 'time' Dimensions but no Attributes or Data Variables yet.\")\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate the stack Variables and Attributes with**\n",
    "\n",
    "- `xarray.DataArrays` for each amp, corr, lv_phi, lv_theta, unw_phase, and water mask array\n",
    "- `xarray.DataArray` for a single DEM\n",
    "- `xarray.Variables` holding InSAR product level metadata\n",
    "- `xarray.Dataset.attrs` holding stack level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in variables:\n",
    "    if v not in ['x', 'y']:\n",
    "        xarray_3d = xr.concat([d[v] for d in insar_arrays], dim=stack.time)\n",
    "        stack[v] = xarray_3d\n",
    "        for d in insar_arrays:\n",
    "            d = d.drop_vars(v)\n",
    "\n",
    "for p in product_paths:\n",
    "    if 'dem' in str(p):\n",
    "        dem = p\n",
    "        break\n",
    "ds = gdal.Open(str(dem))\n",
    "banddata = ds.GetRasterBand(1)\n",
    "data = banddata.ReadAsArray()\n",
    "stack['dem'] = (('y', 'x'), np.ma.masked_invalid(data, copy=True))\n",
    "\n",
    "            \n",
    "attrs = {\n",
    "    'DataCollectionTime': {\n",
    "        'NumberOfAcquisitions': len(times),\n",
    "        'FirstAcquisitionDate': times[0],\n",
    "        'LastAcquisitionDate': times[1],\n",
    "    },\n",
    "}\n",
    "attrs = {k: (v if type(v) != dict else json.dumps(v)) for k, v in attrs.items()}\n",
    "stack = stack.assign_attrs(attrs)\n",
    "\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Write the stack to a NetCDF and/or Zarr store\n",
    "\n",
    "**Save the time-series stack as a NetCDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%df\n",
    "print(\"\\nNoting your current available storage (shown above), do you wish to save the time-series as a NetCDF?\")\n",
    "netcdf = osl.select_parameter(['No, do NOT save a NetCDF', 'Yes, save a NetCDF'])\n",
    "display(netcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = json.loads(stack.DataCollectionTime)['FirstAcquisitionDate']\n",
    "end = json.loads(stack.DataCollectionTime)['LastAcquisitionDate']\n",
    "\n",
    "if 'Yes' in netcdf.value:\n",
    "    netcdf_path = data_directory/f\"InSAR_stack_{start}__{end}.nc4\"\n",
    "    stack.to_netcdf(netcdf_path, engine='netcdf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the time-series stack as a local or remote Zarr-store. Chunk it for temporally optimized access, spatially optimized access, or both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do you wish to save the time-series as a local or remote Zarr-Store?\")\n",
    "locale = osl.select_parameter(['Local Zarr store (on my volume)', 'Remote Zarr store (in an S3 bucket)'])\n",
    "display(locale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do you wish to save a temporally optimized Zarr-store, a spatially optiimized Zarr-store, or both?\")\n",
    "opt = osl.select_parameter(['Temporally Optimized', 'Spatially Optimized', 'Both'])\n",
    "display(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chunks(stack: xr.Dataset, chunk_size=100, pol_count=2):\n",
    "    \"\"\"\n",
    "    stack: the xr.Dataset for which to determine chunks\n",
    "    chunk_size: int in MB\n",
    "    pol_count: number of polarizations\n",
    "    \"\"\"\n",
    "    chunks = list()\n",
    "    bits_per_mb = 8000000\n",
    "    bits_per_chunk = bits_per_mb * chunk_size\n",
    "    bits_per_pixel = 32\n",
    "    pixels_per_chunk = bits_per_chunk / bits_per_pixel\n",
    "    depth = len(stack.time)\n",
    "    \n",
    "    temp_op_xy_pixels = pixels_per_chunk // (depth * pol_count)\n",
    "    spatial_op_xy_pixels = pixels_per_chunk // pol_count\n",
    "    \n",
    "    temp_x_y_side = math.floor(math.sqrt(temp_op_xy_pixels))\n",
    "    spatial_x_y_side = math.floor(math.sqrt(spatial_op_xy_pixels))\n",
    "    \n",
    "    return {\n",
    "        'temporal': (depth, temp_x_y_side, temp_x_y_side),\n",
    "        'spatial': (1, spatial_x_y_side, spatial_x_y_side)\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_chunks(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write the Zarr store with a group for each selected chunking scheme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO handle bad s3 paths\n",
    "\n",
    "if 'Local' in locale.value:\n",
    "    store = data_directory/f\"InSAR_stack_{start}__{end}.zarr\"\n",
    "else:\n",
    "    s3_path = input(\"Enter the S3 path to your store\")\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "chunks = calc_chunks(stack)\n",
    "if 'Temporally' in opt.value or 'Both' in opt.value:\n",
    "    encoding = {vname: {'compressor': compressor, 'chunks': chunks['temporal']} for vname in stack.data_vars}\n",
    "    stack.to_zarr(store=store, encoding=encoding, consolidated=True, group='temporally_optimized', mode='w')\n",
    "\n",
    "if 'Spatially' in opt.value or 'Both' in opt.value:\n",
    "    encoding = {vname: {'compressor': compressor, 'chunks': chunks['spatial']} for vname in stack.data_vars}\n",
    "    stack.to_zarr(store=store, encoding=encoding, consolidated=True, group='spatially_optimized', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prepare_Hyp3_RTC_TimeSeries_NetCDF_Zarr.ipynb - Version 0.1.0 - March 2021*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc_analysis",
   "language": "python",
   "name": "conda-env-.local-rtc_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
