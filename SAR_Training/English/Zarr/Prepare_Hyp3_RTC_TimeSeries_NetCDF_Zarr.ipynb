{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OpenSARlab notebook banner](NotebookAddons/blackboard-banner.png)\n",
    "\n",
    "# Prepare a Dual-Pol HyP3 RTC stack and convert to a CARD4L compliant NetCDF and/or Zarr Store \n",
    "### Alex Lewandowski; Alaska Satellite Facility\n",
    "\n",
    "<img style=\"padding: 7px\" src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\"/></font>\n",
    "\n",
    "**This notebook:**\n",
    "1. Downloads all or part of a dual-pol ASF-HyP3 RTC project (VH/VV or HH/HV)\n",
    "1. Projects all scenes to the same EPSG\n",
    "1. Creates an xarray.Datset containing:\n",
    "    1. Dual-pol backscatter xarray.DataArrays for each timestep\n",
    "    1. Polarization specific CARD4L metadata for each timestep\n",
    "    1. General RTC CARD4L metadata for each timestep\n",
    "    1. RTC CARD4L metadata for entire time-series stack\n",
    "    \n",
    "**This notebook does NOT merge data from the same date because:**\n",
    "- The stack contains timesteps for acquisition start-times at second-scale precision\n",
    "- Each scene will have a different acquisition start-time and their data will occupy different timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note about JupyterHub\n",
    "\n",
    "**Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/rtc_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"rtc_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select the \"rtc_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"rtc_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing Relevant Python Packages\n",
    "\n",
    "In this notebook we will use the following scientific libraries:\n",
    "\n",
    "1. [GDAL](https://www.gdal.org/) is a software library for reading and writing raster and vector geospatial data formats. It includes a collection of programs tailored for geospatial data processing. Most modern GIS systems (such as ArcGIS or QGIS) use GDAL in the background.\n",
    "1. [NumPy](http://www.numpy.org/) is one of the principal packages for scientific applications of Python. It is intended for processing large multidimensional arrays and matrices, and an extensive collection of high-level mathematical functions and implemented methods makes it possible to perform various operations with these objects.\n",
    "\n",
    "**Our first step is to import them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import copy\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json # for loads\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from tqdm.auto import tqdm \n",
    "from typing import Union\n",
    "import warnings\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pycrs\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import yaml\n",
    "import zarr\n",
    "\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "import opensarlab_lib as osl\n",
    "\n",
    "from hyp3_sdk import Batch, HyP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Your Own Data Stack Into the Notebook\n",
    "\n",
    "This notebook assumes that you've created an RTC data stack over your personal area of interest using the [Alaska Satellite Facility's](https://www.asf.alaska.edu/) value-added product system HyP3, available via [ASF Data Search (Vertex)](https://search.asf.alaska.edu/#/). HyP3 is an ASF service used to prototype value added products and provide them to users to collect feedback.\n",
    "\n",
    "We will retrieve HyP3 data via the hyp3_sdk or work with previously downloaded data. As both HyP3 and the Notebook environment sit in the [Amazon Web Services (AWS)](https://aws.amazon.com/) cloud, data transfer is quick and cost effective.\n",
    "\n",
    "---\n",
    "\n",
    "If downloading data, create a data directory in which to download dual-pol HyP3 RTC products.\n",
    "\n",
    "If working with previously downloaded dual-pol HyP3 RTCs, each product should contain VH and VV  or HH and HV data, the HyP3 log file, and the HyP3 product README in subdirectories of the data directory:\n",
    "\n",
    "```\n",
    "data_directory   \n",
    "│\n",
    "└───product_1_directory\n",
    "│   │   *_VH.tif\n",
    "│   │   *_VV.tif\n",
    "│   │   *.README.md.txt\n",
    "│   │   *.log\n",
    "│   ...\n",
    "│   \n",
    "└───product_2_directory\n",
    "│   │   *_VH.tif\n",
    "│   │   *_VV.tif\n",
    "│   │   *.README.md.txt\n",
    "│   │   *.log\n",
    "│   ...\n",
    "│ \n",
    "...\n",
    "```\n",
    "\n",
    "**Select or create a data directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Download HyP3 data or work with previously downloaded data?\")\n",
    "data_source = osl.select_parameter(['Download data from HyP3', 'Use existing data'])\n",
    "display(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = 'Download' in data_source.value\n",
    "if download:\n",
    "    choice = None\n",
    "    while True:\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "        data_dir = Path(input(f\"\\nPlease enter the name of a directory in which to store your downloaded data.\"))\n",
    "        if data_dir == Path('.'):\n",
    "            continue\n",
    "        if data_dir.is_dir():\n",
    "            contents = data_dir.glob('*')\n",
    "            if len(list(contents)) > 0:\n",
    "                choice = osl.handle_old_data(data_dir)\n",
    "                if choice == 1:\n",
    "                    if data_dir.exists():\n",
    "                        shutil.rmtree(data_dir)\n",
    "                    data_dir.mkdir()\n",
    "                    break\n",
    "                elif choice == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    clear_output()\n",
    "                    continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            data_dir.mkdir()\n",
    "            break\n",
    "else:\n",
    "    print(\"Select your data directory\")\n",
    "    fc = FileChooser(Path.cwd())\n",
    "    display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Define absolute path to  analysis directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    data_directory = Path.cwd()/data_dir\n",
    "else:\n",
    "    data_directory = Path(fc.selected_path)\n",
    "\n",
    "print(f\"data_directory: {data_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a HyP3 object and authenticate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if download:\n",
    "    hyp3 = HyP3(prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide whether to search for a HyP3 project or jobs unattached to a project:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download: \n",
    "    options = ['project', 'projectless jobs']\n",
    "    search_type = osl.select_parameter(options, '')\n",
    "    print(\"Select whether to search for HyP3 Project or HyP3 Jobs unattached to a project\")\n",
    "    display(search_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List projects containing active products of the type chosen in the previous cell and select one:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    my_hyp3_info = hyp3.my_info()\n",
    "    active_projects = dict()\n",
    "\n",
    "    if search_type.value == 'project':\n",
    "        for project in my_hyp3_info['job_names']:\n",
    "            batch = Batch()\n",
    "            batch = hyp3.find_jobs(name=project, job_type='RTC_GAMMA').filter_jobs(running=False, include_expired=False)\n",
    "            if len(batch) > 0:\n",
    "                active_projects.update({batch.jobs[0].name: batch})\n",
    "\n",
    "        if len(active_projects) > 0:\n",
    "            display(Markdown(\"<text style='color:darkred;'>Note: After selecting a project, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "            display(Markdown(\"<text style='color:darkred;'>Otherwise, you will rerun this code cell.</text>\"))\n",
    "            print('\\nSelect a Project:')\n",
    "            project_select = osl.select_parameter(active_projects)\n",
    "            display(project_select)\n",
    "    if search_type.value == 'projectless jobs' or len(active_projects) == 0:\n",
    "        project_select = False\n",
    "        if search_type.value == 'project':\n",
    "            print(f\"There were no {'RTC_GAMMA'} jobs found in any current projects.\\n\")\n",
    "        jobs = hyp3.find_jobs(job_type='RTC_GAMMA').filter_jobs(running=False, include_expired=False)\n",
    "        orphaned_jobs = Batch()\n",
    "        for j in jobs:\n",
    "            if not j.name:\n",
    "                orphaned_jobs += j\n",
    "        jobs = orphaned_jobs\n",
    "\n",
    "        if len(jobs) > 0:\n",
    "            print(f\"Found {len(jobs)} {'RTC_GAMMA'} jobs that are not part of a project.\")\n",
    "            print(f\"Select the jobs you wish to download\")\n",
    "            jobs = {i.files[0]['filename']: i for i in jobs}\n",
    "            jobs_select = osl.select_mult_parameters(jobs, '', width='500px')\n",
    "            display(jobs_select)\n",
    "        else:\n",
    "            print(f\"There were no {'RTC_GAMMA'} jobs found that are not part of a project either.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a date range of products to download:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    if project_select:\n",
    "        batch = project_select.value\n",
    "    else:\n",
    "        batch = Batch()\n",
    "        for j in jobs_select.value:\n",
    "            batch += j\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a date range, you should select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you may simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Date Range:')\n",
    "    dates = osl.get_job_dates(batch)\n",
    "    date_picker = osl.gui_date_picker(dates)\n",
    "    display(date_picker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected date range and remove products falling outside of it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if download:\n",
    "    date_range = osl.get_slider_vals(date_picker)\n",
    "    date_range[0] = date_range[0].date()\n",
    "    date_range[1] = date_range[1].date()\n",
    "    print(f\"Date Range: {str(date_range[0])} to {str(date_range[1])}\")\n",
    "    batch = osl.filter_jobs_by_date(batch, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather the available paths and orbit directions for the remaining products:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    display(Markdown(\"<text style='color:darkred;'><text style='font-size:150%;'>This may take some time for projects containing many jobs...</text></text>\"))\n",
    "    osl.set_paths_orbits(batch)\n",
    "    paths = set()\n",
    "    orbit_directions = set()\n",
    "    for p in batch:\n",
    "        paths.add(p.path)\n",
    "        orbit_directions.add(p.orbit_direction)\n",
    "    paths.add('All Paths')\n",
    "    display(Markdown(f\"<text style=color:blue><text style='font-size:175%;'>Done.</text></text>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Select a path or paths (use shift or ctrl to select multiple paths):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a path, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Path:')\n",
    "    path_choice = osl.select_mult_parameters(paths)\n",
    "    display(path_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected flight path/s:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    flight_path = path_choice.value\n",
    "    if flight_path:\n",
    "        if flight_path:\n",
    "            print(f\"Flight Path: {flight_path}\")\n",
    "        else:\n",
    "            print('Flight Path: All Paths')\n",
    "    else:\n",
    "        print(\"WARNING: You must select a flight path in the previous cell, then rerun this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select an orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    if len(orbit_directions) > 1:\n",
    "        display(Markdown(\"<text style='color:red;'>Note: After selecting a flight direction, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "        display(Markdown(\"<text style='color:red;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Flight Direction:')\n",
    "    direction_choice = osl.select_parameter(orbit_directions, 'Direction:')\n",
    "    display(direction_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    direction = direction_choice.value\n",
    "    print(f\"Orbit Direction: {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter jobs by path and orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    batch = osl.filter_jobs_by_path(batch, flight_path)\n",
    "    batch = osl.filter_jobs_by_orbit(batch, direction)\n",
    "    print(f\"There are {len(batch)} products to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the products, unzip them into a directory named after the product type, and delete the zip files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if download:\n",
    "    print(f\"\\nProject: {batch.jobs[0].name}\")\n",
    "    project_zips = batch.download_files(data_directory)\n",
    "    for z in project_zips:\n",
    "        osl.asf_unzip(str(data_directory), str(z))\n",
    "        z.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect the paths to the GeoTiffs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarizations_in_dir(product_dir):\n",
    "    polarizations = {\n",
    "        'VH': len(list(Path(product_dir).glob('*VH*.tif'))) > 0,\n",
    "        'VV': len(list(Path(product_dir).glob('*VV*.tif'))) > 0,\n",
    "        'HH': len(list(Path(product_dir).glob('*HH*.tif'))) > 0,\n",
    "        'HV': len(list(Path(product_dir).glob('*HV*.tif'))) > 0,\n",
    "    }\n",
    "    return [k for k in polarizations.keys() if polarizations[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarizations = get_polarizations_in_dir(list(data_directory.glob('*'))[0])\n",
    "\n",
    "if 'VH' in polarizations and 'VV' in polarizations:\n",
    "    polar_wildcard = '*/*_V*.tif'\n",
    "elif 'HH' in polarizations and 'HV' in polarizations:\n",
    "    polar_wildcard = '*/*_H*.tif'\n",
    "product_paths = list(data_directory.glob(polar_wildcard))\n",
    "product_paths.sort()\n",
    "for p in product_paths:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fix multiple UTM Zone-related issues\n",
    "\n",
    "Fix multiple UTM Zone-related issues should they exist in your data set. If multiple UTM zones are found, the following code cells will identify the predominant UTM zone and reproject the rest into that zone. This step must be completed prior to merging frames or performing any analysis. AutoRIFT products do not come with projection metadata and so will not be reprojected.\n",
    "\n",
    "**Use gdal.Info to determine the UTM definition types and zones in each product:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones = []\n",
    "utm_types = []\n",
    "print('Checking UTM Zones in the data stack ...\\n')\n",
    "for k in range(0, len(product_paths)):\n",
    "    info = (gdal.Info(str(product_paths[k]), options = ['-json']))\n",
    "    info = json.dumps(info)\n",
    "    info = (json.loads(info))['coordinateSystem']['wkt']\n",
    "    zone = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "    utm_zones.append(zone)\n",
    "    typ = info.split('ID')[-1].split('\"')[1]\n",
    "    utm_types.append(typ)\n",
    "print(f\"UTM Zones:\\n {utm_zones}\\n\")\n",
    "print(f\"UTM Types:\\n {utm_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify the most commonly used UTM Zone in the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_unique, counts = np.unique(utm_zones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "predominant_utm = utm_unique[a][0]\n",
    "print(f\"Predominant UTM Zone: {predominant_utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reproject all tiffs to the predominate UTM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_indicies = [i for i, j in enumerate(utm_zones) if j != predominant_utm] #makes list of indicies in utm_zones that need to be reprojected\n",
    "print('--------------------------------------------')\n",
    "print('Reprojecting %4.1f files' %(len(reproject_indicies)))\n",
    "print('--------------------------------------------')\n",
    "for k in reproject_indicies:\n",
    "    temppath = f\"{str(product_paths[k].parent)}/r{product_paths[k].name}\"\n",
    "    print(temppath)  \n",
    "\n",
    "    cmd = f\"gdalwarp -overwrite {product_paths[k]} {temppath} -s_srs {utm_types[k]}:{utm_zones[k]} -t_srs EPSG:{predominant_utm}\"\n",
    "#     print(cmd)\n",
    "    !{cmd}\n",
    "\n",
    "    product_paths[k].unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_paths = list(data_directory.glob(polar_wildcard))\n",
    "product_paths.sort()\n",
    "for p in product_paths:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tidy these functions up, add type hints and doc strings, add to opensarlab-lib\n",
    "\n",
    "\n",
    "def polarization_from_filename(file):\n",
    "    pol_regex = 'vv|VV|vh|VH|hh|HH|hv|HV'\n",
    "    pol = re.search(pol_regex, str(file))\n",
    "    if pol:\n",
    "        pol = pol.group(0).upper()\n",
    "    return pol\n",
    "\n",
    "def dates_from_product_name(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Takes: a string or posix path to a HyP3 product\n",
    "    Returns: a string date and timestamp parsed from the name or None if none found\n",
    "    \"\"\"\n",
    "    regex = \"[0-9]{8}T[0-9]{6}_[0-9]{8}T[0-9]{6}\"\n",
    "    results = re.search(regex, str(product_name))\n",
    "    if results:\n",
    "        return results.group(0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_hyp3_log_val(log_path, regex):\n",
    "    with open(str(log_path), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            val = re.search(regex, line)\n",
    "            if val:\n",
    "                return val.group(0)\n",
    "            \n",
    "def get_beam_IDs(log_path, regex):\n",
    "    with open(str(log_path), 'r') as f:\n",
    "        ids = None\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            val = re.search(regex, line)\n",
    "            if val and not ids:\n",
    "                ids = val.group(0)\n",
    "            elif val and ids:\n",
    "                ids = f'{ids}, {val.group(0)}'\n",
    "    return ids\n",
    "            \n",
    "def mission_from_filename(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    regex = 'S1(A|B|C)'\n",
    "    results = re.search(regex, Path(product_name).name)\n",
    "    if results:\n",
    "        return results.group(0)\n",
    "    else:\n",
    "        return None\n",
    "            \n",
    "def observation_mode_from_filename(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    return Path(product_name).name.split('_')[1]\n",
    "\n",
    "def orbit_data_source_from_filename(product_name: Union[str, Path]) -> Union[str, None]:\n",
    "    orbit =  Path(product_name).name.split('_')[3][2]\n",
    "    if orbit == 'P':\n",
    "        return 'Precise'\n",
    "    elif orbit == 'R':\n",
    "        return 'Restituted'\n",
    "    elif orbit == 'O':\n",
    "        return 'Original Predicted'\n",
    "    \n",
    "def get_corners_gdal(file):\n",
    "    ds=gdal.Open(str(file))\n",
    "    transform = ds.GetGeoTransform()\n",
    "    x = ds.RasterXSize\n",
    "    y = ds.RasterYSize\n",
    "    \n",
    "    ulx = transform[0]\n",
    "    uly = transform[3]\n",
    "    lrx = transform[0] + x * transform[1]\n",
    "    lry = transform[3] + y * transform[5]\n",
    "    \n",
    "    return {'ul': [ulx, uly], 'lr': [lrx, lry]}\n",
    "    \n",
    "def parse_proj_crs(proj_crs):\n",
    "    crs = pycrs.parse.from_ogc_wkt(proj_crs)\n",
    "    cfg_p = {}\n",
    "    cfg_p['grid_mapping_name'] = crs.name\n",
    "    cfg_p['crs_wkt'] = crs.proj.name.ogc_wkt.lower()\n",
    "\n",
    "    # Is there a better way to do this? \n",
    "    for p in crs.params:\n",
    "        if isinstance(p,pycrs.elements.parameters.LatitudeOrigin):\n",
    "            cfg_p['latitude_of_projection_origin'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.CentralMeridian):\n",
    "            cfg_p['longitude_of_central_meridian'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.FalseEasting):\n",
    "            cfg_p['false_easting'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.FalseNorthing):\n",
    "            cfg_p['false_northing'] = p.value\n",
    "        if isinstance(p,pycrs.elements.parameters.ScalingFactor):\n",
    "            cfg_p['scale_factor_at_centeral_meridian'] = p.value\n",
    "\n",
    "    cfg_p['projected_coordinate_system_name'] = crs.name\n",
    "    cfg_p['geographic_coordinate_system_name'] = crs.geogcs.name\n",
    "    cfg_p['horizontal_datum_name'] = crs.geogcs.datum.name.ogc_wkt\n",
    "    cfg_p['reference_ellipsoid_name'] = crs.geogcs.datum.ellips.name.ogc_wkt\n",
    "    cfg_p['semi_major_axis'] = crs.geogcs.datum.ellips.semimaj_ax.value\n",
    "    cfg_p['inverse_flattening'] = crs.geogcs.datum.ellips.inv_flat.value\n",
    "    cfg_p['longitude_of_prime_meridian'] = crs.geogcs.prime_mer.value\n",
    "    cfg_p['units'] = crs.unit.unitname.ogc_wkt\n",
    "    cfg_p['projection_x_coordinate'] = \"x\"\n",
    "    cfg_p['projection_y_coordinate'] = \"y\"\n",
    "\n",
    "    return cfg_p\n",
    "\n",
    "def get_RTC_prod_hash(tiff):\n",
    "    stem = Path(tiff).stem\n",
    "    regex = \"(?<=gpu[f|n]ed_)[0-9A-Z]{4}(?=_[V|H][V|H])\"\n",
    "    p_hash = re.search(regex, str(stem))\n",
    "    if p_hash:\n",
    "        return (p_hash.group(0))\n",
    "    \n",
    "def hyp3_log_time_to_utc(time_str):\n",
    "    time = datetime.strptime(time_str[:-3], '%m/%d/%Y %H:%M:%S')\n",
    "    if 'PM' in time_str and time.hour < 12:\n",
    "        return time + timedelta(hours=12)\n",
    "    elif 'AM' in time_str and time.hour == 12:\n",
    "        return time + timedelta(hours=12)\n",
    "    else:\n",
    "        return time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3_log_time_to_utc(\"03/05/2022 01:14:23 AM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an xarray Dataset containing the RTC stack\n",
    "\n",
    "### 3.1. Write Functions to gather the needed CARD4L metadata from the backscatter GeoTiffs and log files\n",
    "\n",
    "**Write a function to gather the metadata needed for each dual-pol pair:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_pair_RTC_vars(tiff):\n",
    "    f = gdal.Open(str(tiff))\n",
    "    info = gdal.Info(str(tiff), format='json')\n",
    "    \n",
    "    source_file_name = Path(info['files'][0]).name\n",
    "    \n",
    "    try:\n",
    "        log_path = list(Path(tiff).parent.glob('*.log'))[0]\n",
    "    except:\n",
    "        print(\"Log file not found\")\n",
    "        raise\n",
    "        \n",
    "    try:\n",
    "        readme_path = list(Path(tiff).parent.glob('*.README.md.txt'))[0]\n",
    "    except:\n",
    "        print(\"README not found\")\n",
    "        raise\n",
    "        \n",
    "    date_regex = '[0-9]{2}/[0-9]{2}/[0-9]{4} [0-9]{2}:[0-9]{2}:[0-9]{2} [A|P]M'\n",
    "    \n",
    "    dem_regex = f'(?<={date_regex} - INFO -\\s{{5}}DEM name\\s{{18}}: )\\w+(?=\\n)'\n",
    "    dem = get_hyp3_log_val(log_path, dem_regex)\n",
    "    if dem.lower() == 'copernicus':\n",
    "        dem = f\"{dem}: https://spacedata.copernicus.eu/explore-more/news-archive/-/asset_publisher/Ye8egYeRPLEs/blog/id/434960\"\n",
    "    elif 'srtm' in dem.lower():\n",
    "        dem = f\"{dem}: https://www.usgs.gov/centers/eros/science/usgs-eros-archive-digital-elevation-shuttle-radar-topography-mission-srtm-1\"\n",
    "    else:\n",
    "        dem = 'unknown'\n",
    "        \n",
    "    earth_gravitational_model_regex = '(WGS84 ellipsoid|egm2020|egm2008|egm96|egm84)'\n",
    "    earth_gravitational_model = get_hyp3_log_val(log_path, earth_gravitational_model_regex)\n",
    "    earth_gravitational_model = f\"{earth_gravitational_model}: https://earth-info.nga.mil/index.php?dir=wgs84&action=wgs84\"\n",
    "    \n",
    "    noise_regex = f'(?<={date_regex} - INFO -\\s{{5}}Speckle filter\\s{{12}}: )(True|False)(?=\\n)'\n",
    "    noise = get_hyp3_log_val(log_path, noise_regex)\n",
    "    noise = noise.lower() in ['t', 'true']\n",
    "    if noise:\n",
    "        noise_removal_algorithm = 'Lee Enhanced: https://gamma-rs.ch/uploads/media/GAMMA_Software_information.pdf'\n",
    "        win_size_col_regex = f'(?<={date_regex} - INFO - Proc: filter width in range \\(bx\\):)\\s+[0-9]+(?=\\n)'\n",
    "        win_size_col = int(get_hyp3_log_val(log_path, win_size_col_regex))\n",
    "        win_size_line_regex = f'(?<={date_regex} - INFO - Proc: filter width in azimuth \\(by\\):)\\s+[0-9]+(?=\\n)'\n",
    "        win_size_line = int(get_hyp3_log_val(log_path, win_size_line_regex))\n",
    "    else:\n",
    "        noise_removal_algorithm = 'n/a'\n",
    "        win_size_col = 'n/a'\n",
    "        win_size_line = 'n/a'\n",
    "\n",
    "    \n",
    "    safe_regex = f'(?<={date_regex} - INFO -\\s{{5}}SAFE directory\\s{{12}}: )\\w{{50,80}}.SAFE(?=\\n)'\n",
    "    safe = get_hyp3_log_val(log_path, safe_regex)\n",
    "    acquisition_dates = dates_from_product_name(safe).split('_')\n",
    "    start_time = datetime.strftime(datetime.strptime(acquisition_dates[0], '%Y%m%dT%H%M%S'), '%Y-%m-%dT%H:%M:%S')\n",
    "    end_time = datetime.strftime(datetime.strptime(acquisition_dates[1], '%Y%m%dT%H%M%S'), '%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "        \n",
    "    mission = mission_from_filename(source_file_name)\n",
    "    observation_mode = observation_mode_from_filename(source_file_name)\n",
    "    polarization = polarization_from_filename(source_file_name)\n",
    "    \n",
    "    pass_direction_regex = f'(?<={date_regex} - INFO - Proc: orbit )(ascending|descending)(?= node time)'\n",
    "    pass_direction = get_hyp3_log_val(log_path, pass_direction_regex)\n",
    "       \n",
    "    processing_date = get_hyp3_log_val(log_path, date_regex)\n",
    "    processing_date = hyp3_log_time_to_utc(processing_date)\n",
    "    processing_date = datetime.strftime(processing_date, '%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    \n",
    "    product_level_regex = f'(?<={date_regex} - INFO - Running command: par_S1_)[A-Z]{{3}}(?= S1)'\n",
    "    product_level = get_hyp3_log_val(log_path, product_level_regex)\n",
    "    \n",
    "    azimuth_looks_regex = '(?<=number of azimuth looks:)\\s+[0-9]+(?=\\n)'\n",
    "    azimuth_looks = int(get_hyp3_log_val(log_path, azimuth_looks_regex))\n",
    "    \n",
    "    range_looks_regex ='(?<=number of range looks:)\\s+[0-9]+(?=\\n)'\n",
    "    range_looks = int(get_hyp3_log_val(log_path, range_looks_regex))\n",
    "    \n",
    "    azimuth_pixel_spacing_regex = '(?<=SLC azimuth pixel spacing \\(m\\):)\\s+\\d*.\\d*(?=\\n)'\n",
    "    azimuth_pixel_spacing = float(get_hyp3_log_val(log_path, azimuth_pixel_spacing_regex))\n",
    "    \n",
    "    range_pixel_spacing_regex = '(?<=SLC slant range pixel spacing \\(m\\):)\\s+\\d*.\\d*(?=\\n)'\n",
    "    range_pixel_spacing = float(get_hyp3_log_val(log_path, range_pixel_spacing_regex))\n",
    "    \n",
    "    range_azimuth_resolution_regex = '(?<=range-azimuth pixel area \\(rps\\*azps\\):)\\s+\\d+.\\d+(?= \\[m\\^2\\]\\n)'\n",
    "    range_azimuth_resolution = math.sqrt(float(get_hyp3_log_val(log_path, range_azimuth_resolution_regex)))\n",
    "    \n",
    "    projection = f.GetProjection()\n",
    "    epsg = int(projection.split('AUTHORITY[\"EPSG\",\"')[-1].split('\"')[0])\n",
    "    \n",
    "    corners = get_corners_gdal(tiff)\n",
    "    \n",
    "    hyp3_doi_regex = '(?<=\\* HyP3 processing environment, DOI: )\\[\\S+\\]\\(\\S+(?=\\)\\n)'\n",
    "    hyp3_doi = get_hyp3_log_val(readme_path, hyp3_doi_regex)\n",
    "    hyp3_doi = hyp3_doi.split('(')[-1]\n",
    "    \n",
    "    gamma_doi_regex = '(?<=\\* HyP3 GAMMA plugin, DOI: )\\[\\S+\\]\\(\\S+(?=\\)\\n)'\n",
    "    gamma_doi = get_hyp3_log_val(readme_path, hyp3_doi_regex)\n",
    "    gamma_doi = hyp3_doi.split('(')[-1]\n",
    "    \n",
    "    source_proc_version_regex = f'(?<={date_regex} - INFO - Proc: GeoTIFF software version: ).+(?=\\n)'\n",
    "    source_proc_version = get_hyp3_log_val(log_path, source_proc_version_regex)\n",
    "    \n",
    "    if epsg == 4326:\n",
    "        y = 'Latitude'\n",
    "        x = 'Longitude'\n",
    "    else:\n",
    "        y = 'Northing'\n",
    "        x = 'Easting'\n",
    "        \n",
    "    grid = pycrs.parse.from_ogc_wkt(projection).name\n",
    "        \n",
    "    \n",
    "    var = {\n",
    "        'DataAccess': {\n",
    "            'ProcessingFacility': 'Alaska Satellite Facility',\n",
    "            'ProcessingTime': processing_date,\n",
    "            'SoftwareVersion': hyp3_doi,\n",
    "            'RepositoryURL': 'Non-peristent data: reorder from ASF-HyP3'\n",
    "        },\n",
    "        'DigitalElevationModel': {\n",
    "            'DEMReference': dem,\n",
    "            'EGMReference': earth_gravitational_model,\n",
    "        },\n",
    "        'Filtering': {\n",
    "            'FilterApplied': noise,\n",
    "            'FilterType': noise_removal_algorithm,\n",
    "            'WindowSizeCol': win_size_col,\n",
    "            'WindowSizeLine': win_size_line,\n",
    "        },\n",
    "        'GriddingConvention': grid,\n",
    "        'NoiseRemoval': {\n",
    "            'NoiseRemovalApplied': noise,\n",
    "            'NoiseRemovalAlgorithm': noise_removal_algorithm,\n",
    "        },\n",
    "        'OrbitInformation': {\n",
    "            'PassDirection': pass_direction,\n",
    "            'OrbitDataSource': orbit_data_source_from_filename(source_file_name),\n",
    "        },\n",
    "        'ProductBoundingBox': {\n",
    "            'UpperLeftCorner': corners['ul'],\n",
    "            'LowerRightCorner': corners['lr'],\n",
    "        },\n",
    "        'ProductGeographicalExtent': info['wgs84Extent'],\n",
    "        'ProductImageSize': {\n",
    "            'NumberLines': f.RasterYSize,\n",
    "            'NumPixelsPerLine': f.RasterXSize,\n",
    "        },\n",
    "        'ProdcutSampleSpacing': {\n",
    "            'ProductColumnSpacing': azimuth_pixel_spacing,\n",
    "            'ProductRowSpacing': range_pixel_spacing,\n",
    "        },\n",
    "        'RTCAlgorithm': gamma_doi,\n",
    "        'SourceAttributes': {\n",
    "            'Satellite': f'Sentinel-1: {mission}',\n",
    "            'SourceDataAcquisitionTime': {\n",
    "                'StartTime': start_time,\n",
    "                'EndTime': end_time,\n",
    "            },\n",
    "            'SourceDataImageAttributes': {\n",
    "                'SourceDataGeometry': projection,\n",
    "                'AzimuthPixelSpacing': azimuth_pixel_spacing,\n",
    "                'RangePixelSpacing': range_pixel_spacing,\n",
    "                'AzimuthResolution': range_azimuth_resolution,\n",
    "                'RangeResolution': range_azimuth_resolution,\n",
    "            },\n",
    "            'SourceProcParam': {\n",
    "                'ProcessingFacility': 'Alaska Satellite Facility',\n",
    "                'ProcessingDate': processing_date,\n",
    "                'SoftwareVersion': source_proc_version,\n",
    "                'ProductID': safe.split('.')[0],\n",
    "                'ProductLevel': product_level,\n",
    "                'AzimuthNumberOfLooks': azimuth_looks,\n",
    "                'RangeNumberOfLooks': range_looks,\n",
    "            },\n",
    "        },\n",
    "     \n",
    "    }\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to print an example call to `get_per_pair_RTC_vars`\n",
    "\n",
    "d = get_per_pair_RTC_vars(product_paths[0])\n",
    "print(yaml.dump(d, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function to gather the metadata needed for each, individual backscatter xr.DataArray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_pol_RTC_vars(tiff):\n",
    "    f = gdal.Open(str(tiff))\n",
    "    info = gdal.Info(str(tiff), format='json')\n",
    "    \n",
    "    data_point = f.ReadAsArray()[0][0]\n",
    "    \n",
    "    source_file_name = Path(info['files'][0]).name\n",
    "    \n",
    "    try:\n",
    "        log_path = list(Path(tiff).parent.glob('*.log'))[0]\n",
    "    except:\n",
    "        print(\"Log file not found\")\n",
    "        raise\n",
    "        \n",
    "    date_regex = '[0-9]{2}/[0-9]{2}/[0-9]{4} [0-9]{2}:[0-9]{2}:[0-9]{2} [A|P]M'\n",
    "    \n",
    "    scale_regex = f'(?<={date_regex} - INFO -\\s{{5}}Scale\\s{{21}}: )[a-z]{{5,9}}(?=\\n)'\n",
    "    scale = get_hyp3_log_val(log_path, scale_regex)\n",
    "    \n",
    "    noise_regex = f'(?<={date_regex} - INFO -\\s{{5}}Speckle filter\\s{{12}}: )(True|False)(?=\\n)'\n",
    "    noise = get_hyp3_log_val(log_path, noise_regex)\n",
    "    noise = noise.lower() in ['t', 'true']\n",
    "    if noise:\n",
    "        noise_removal_algorithm = 'Lee Enhanced: https://gamma-rs.ch/uploads/media/GAMMA_Software_information.pdf'\n",
    "    else:\n",
    "        noise_removal_algorithm = 'n/a'\n",
    "    \n",
    "    bs_measurement_regex = f'(?<={date_regex} - INFO -\\s{{5}}Radiometry\\s{{16}}: )(gamma0|sigma0)(?=\\n)'\n",
    "    backscatter_measurement = get_hyp3_log_val(log_path, bs_measurement_regex)\n",
    "    \n",
    "    if sys.byteorder == 'little':\n",
    "        byte_order = 'Little Endian'\n",
    "    elif sys.byteorder == 'big':\n",
    "        byte_order = 'Big Endian'\n",
    "    else:\n",
    "        byte_order = 'unknown'\n",
    "\n",
    "    polarization = polarization_from_filename(source_file_name)\n",
    "\n",
    "    var = {\n",
    "        'BackscatterMeasurementData': {\n",
    "            'BackscatterMeasurement': backscatter_measurement,\n",
    "            'BackscatterConvention': scale,\n",
    "            'BackscatterConversionEq': '\"dB = 10*log10(DN)\" where DN are data values in <FileName>',\n",
    "            'Polarization': polarization,\n",
    "            'FileName': source_file_name,\n",
    "            'DataFormat': 'xarray.DataArray',\n",
    "            'DataType': repr(type(data_point)),\n",
    "            'BitsPerSample': data_point.itemsize * 8,\n",
    "            'ByteOrder': byte_order,\n",
    "        },\n",
    "        'DocumentIdentifier': source_file_name,\n",
    "    }\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to print an example call to `get_per_pol_RTC_vars`\n",
    "\n",
    "# d = get_per_pol_RTC_vars(product_paths[0])\n",
    "# print(yaml.dump(d, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function to gather metadata needed for the entire stack**\n",
    "\n",
    "Note that this function is tiny and could have instead been handled with hard-coded values when creating the xr.Dataset. \n",
    "\n",
    "However, writing this function will make stack metadata easier to extend later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backscatter_stack_attrs(tiff):\n",
    "    f = gdal.Open(str(tiff))\n",
    "    info = gdal.Info(str(tiff), format='json')\n",
    "    \n",
    "    source_file_name = Path(info['files'][0]).name\n",
    "    \n",
    "    mission = mission_from_filename(source_file_name)\n",
    "    polarization = polarization_from_filename(source_file_name)\n",
    "    \n",
    "    try:\n",
    "        log_path = list(Path(tiff).parent.glob('*.log'))[0]\n",
    "    except:\n",
    "        print(\"Log file not found\")\n",
    "        raise\n",
    "        \n",
    "    date_regex = '[0-9]{2}/[0-9]{2}/[0-9]{4} [0-9]{2}:[0-9]{2}:[0-9]{2} [A|P]M'\n",
    "    \n",
    "    observation_mode = observation_mode_from_filename(source_file_name)\n",
    "    \n",
    "    pulse_rep_freq_regex = f'(?<={date_regex} - INFO - Proc: effective PRF derived from azimuth).*: [0-9]{{,5}}\\.[0-9]{{,10}}(?=\\n)'\n",
    "    pulse_rep_freq = get_hyp3_log_val(log_path, pulse_rep_freq_regex)\n",
    "    try:\n",
    "        pulse_rep_freq = float(pulse_rep_freq.split(' ')[-1])\n",
    "    except: \n",
    "        print(\"No valid Pulse Repetition Frequency found\") \n",
    "        raise\n",
    "    \n",
    "    projection = f.GetProjection()\n",
    "    epsg = int(projection.split('AUTHORITY[\"EPSG\",\"')[-1].split('\"')[0])\n",
    "    \n",
    "    if epsg == 4326:\n",
    "        y = 'Latitude'\n",
    "        x = 'Longitude'\n",
    "    else:\n",
    "        y = 'Northing'\n",
    "        x = 'Easting'\n",
    "        \n",
    "    azimuth_regex = '(?<=(azimuth angle: ))[0-9]{,4}\\.[0-9]{,6} \\((right|left) looking\\)'\n",
    "    azimuth = get_hyp3_log_val(log_path, azimuth_regex)\n",
    "    antenna_pointing = azimuth.split(' ')[1][1:]\n",
    "    \n",
    "    beam_id_regex = f'(?<={date_regex} - INFO - Proc: sensor: {mission} {observation_mode} )\\w{{,4}}(?= {polarization}\\n)'\n",
    "    beam_ids = get_beam_IDs(log_path, beam_id_regex)\n",
    "        \n",
    "    attrs =  {\n",
    "        'CoordinateReferenceSystem': {\n",
    "            'EPSG': epsg,\n",
    "            'WKT': projection,\n",
    "        },\n",
    "        'DataCollectionTime': {\n",
    "            'NumberOfAcquisitions': None,\n",
    "            'FirstAcquisitionDate': None,\n",
    "            'LastAcquisitionDate': None,\n",
    "        },\n",
    "        'PixelCoordinateConvention': f.GetMetadata()['AREA_OR_POINT'],\n",
    "        'Product': 'Normalized Radar Backscatter (Radiometrically Terrain-Corrected)',\n",
    "        'SourceAttributes': {\n",
    "            'Instrument': 'C-SAR',\n",
    "            'SourceDataAcquisitionParameters': {\n",
    "                'RadarBand': 'C',\n",
    "                'RadarCenterFrequency': pulse_rep_freq,\n",
    "                'ObservationMode': observation_mode,\n",
    "                'Polarizations': ' '.join(get_polarizations_in_dir(Path(tiff).parent)),\n",
    "                'AntennaPointing': antenna_pointing,\n",
    "                'BeamID': beam_ids,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print an example call to `get_backscatter_stack_attrs`\n",
    "\n",
    "# d = get_backscatter_stack_attrs(product_paths[0])\n",
    "# print(yaml.dump(d, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Write a function to create an xarray.Dataset for a dual-pol pair of RTCs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backscatter_pair_to_xarray(polar_pair):   \n",
    "    bsv_gen = get_per_pair_RTC_vars(polar_pair[0])\n",
    "    \n",
    "    polarizations = get_polarizations_in_dir(Path(polar_pair[0]).parent)\n",
    "    \n",
    "    for tiff in polar_pair:\n",
    "        bsv = get_per_pol_RTC_vars(tiff)\n",
    "        ds=gdal.Open(str(tiff))\n",
    "        polarization = bsv['BackscatterMeasurementData']['Polarization']\n",
    "        banddata = ds.GetRasterBand(1)\n",
    "        data = banddata.ReadAsArray()\n",
    "        \n",
    "        if polarization == 'VH' or polarization == 'HH':\n",
    "            p1_backscatter = np.ma.masked_invalid(data, copy=True)\n",
    "            p1_BackscatterMeasurementData = json.dumps(bsv['BackscatterMeasurementData'])\n",
    "            p1_DocumentIdentifier = bsv['DocumentIdentifier']\n",
    "            \n",
    "        elif polarization == 'VV' or polarization == 'HV':\n",
    "            p2_backscatter = np.ma.masked_invalid(data, copy=True)\n",
    "            p2_BackscatterMeasurementData = json.dumps(bsv['BackscatterMeasurementData'])\n",
    "            p2_DocumentIdentifier = bsv['DocumentIdentifier']\n",
    "\n",
    " \n",
    "    ds=gdal.Open(str(polar_pair[0]))\n",
    "\n",
    "    # get coordinate system projection\n",
    "    prj = ds.GetProjection()\n",
    "    crs = pycrs.parse.from_ogc_wkt(prj)\n",
    "    crs_proj = crs.proj.name.ogc_wkt.lower()   \n",
    "\n",
    "    # pixel resolution\n",
    "    geo_trans = ds.GetGeoTransform()\n",
    "    res_x = geo_trans[1]\n",
    "    res_y = geo_trans[5]\n",
    "    \n",
    "    #get corner coords and extents\n",
    "    corners = get_corners_gdal(polar_pair[0])\n",
    "    x_extent = [corners['ul'][0], corners['lr'][0]]\n",
    "    y_extent = [corners['ul'][1], corners['lr'][1]]\n",
    "\n",
    "    # create x and y arrays based on extents and pixel resolution\n",
    "    x_coords = np.arange(x_extent[0], x_extent[1], res_x)\n",
    "    y_coords = np.arange(y_extent[0], y_extent[1], res_y)\n",
    "\n",
    "    # create xarray dataset\n",
    "    data_set = xr.Dataset(\n",
    "        data_vars={\n",
    "            'y': y_coords,\n",
    "            'x': x_coords,\n",
    "            f'{polarizations[0]}_backscatter': (\n",
    "                ('y', 'x'),\n",
    "                p1_backscatter.filled(0.0),\n",
    "            ),\n",
    "            f'{polarizations[1]}_backscatter': (\n",
    "                ('y', 'x'),\n",
    "                p2_backscatter.filled(0.0),\n",
    "            ),\n",
    "            f'{polarizations[0]}_BackscatterMeasurementData': p1_BackscatterMeasurementData,\n",
    "            f'{polarizations[1]}_BackscatterMeasurementData': p2_BackscatterMeasurementData,\n",
    "            f'{polarizations[0]}_DocumentIdentifier': p1_DocumentIdentifier,\n",
    "            f'{polarizations[1]}_DocumentIdentifier': p2_DocumentIdentifier,\n",
    "        },\n",
    "        attrs=None\n",
    "    )\n",
    "\n",
    "    # Set x and y coord attributes\n",
    "    attrs_x = {\n",
    "        'axis': 'X',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_x_coordinate',\n",
    "        'long_name': 'Easting'\n",
    "    }\n",
    "    attrs_y = {\n",
    "        'axis': 'Y',\n",
    "        'units': 'm',\n",
    "        'standard_name': 'projection_y_coordinate',\n",
    "        'long_name': 'Northing'\n",
    "    }\n",
    "    for key in attrs_x:\n",
    "        data_set.x.attrs[key] = attrs_x[key]\n",
    "    for key in attrs_y:\n",
    "        data_set.y.attrs[key] = attrs_y[key]\n",
    "        \n",
    "    for k in bsv_gen.keys():\n",
    "        data_set[k] = json.dumps(bsv_gen[k])\n",
    "    \n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create the xarray stack\n",
    "\n",
    "**Create a list of paths to each dual-pol pair**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "products_dir = product_paths[0].parents[1]\n",
    "tiffs = list(products_dir.glob(polar_wildcard))\n",
    "hashes = list(set([get_RTC_prod_hash(p) for p in tiffs]))\n",
    "polar_pairs = [[list(products_dir.glob(f'*/*{h}_{polarizations[0]}*.tif'))[0], \n",
    "                list(products_dir.glob(f'*/*{h}_{polarizations[1]}*.tif'))[0]] for h in hashes if h]\n",
    "\n",
    "# We must sort the paths to properly order timesteps\n",
    "polar_pairs.sort()\n",
    "polar_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a list of xarray.Datsets for each dual-pol pair**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_arrays = []\n",
    "for pair in tqdm(polar_pairs):\n",
    "    polar_arrays.append(backscatter_pair_to_xarray(pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_arrays[0].VH_backscatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare an `xarray.Dataset` to hold the stack**\n",
    "\n",
    "- Contains the same x/y dimensions as stack data\n",
    "- Contains a time dimension for the timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "times = [json.loads(str(a.SourceAttributes.data))['SourceDataAcquisitionTime']['StartTime'] for a in polar_arrays]\n",
    "\n",
    "# Create the stack\n",
    "stack = polar_arrays[0]\n",
    "# stack = stack.assign_attrs(get_backscatter_stack_attrs(product_paths[0]))\n",
    "\n",
    "per_polar_keys = [f'{polarizations[0]}_backscatter', f'{polarizations[1]}_backscatter',\n",
    "                  f'{polarizations[0]}_BackscatterMeasurementData', f'{polarizations[1]}_BackscatterMeasurementData',\n",
    "                  f'{polarizations[0]}_DocumentIdentifier', f'{polarizations[1]}_DocumentIdentifier',                            \n",
    "                 ]\n",
    "for k in per_polar_keys:\n",
    "    stack = stack.drop_vars(k)\n",
    "\n",
    "\n",
    "per_pair_keys = get_per_pair_RTC_vars(product_paths[0]).keys()\n",
    "for k in per_pair_keys:\n",
    "    stack = stack.drop_vars(k)\n",
    "\n",
    "stack = stack.assign_coords(time=times)\n",
    "stack.time.attrs['axis'] = \"T\" \n",
    "stack.time.attrs['units'] = f\"timestamp in format %Y-%m-%dT%H:%M:%S\"\n",
    "stack.time.attrs['calendar'] = \"proleptic_gregorian\"\n",
    "stack.time.attrs['long_name'] = \"Time\"        \n",
    "\n",
    "print(\"Take a look at the stack.\")\n",
    "print(\"It should contain 'x', 'y', and 'time' Dimensions but no Attributes or Data Variables yet.\")\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate the stack Variables and Attributes with**\n",
    "\n",
    "- `xarray.DataArrays` for backscatter data in each polarization\n",
    "- `xarray.Variables` holding polarization-pair metadata\n",
    "- `xarray.Variables` holding per-polarization metadata\n",
    "- `xarray.Dataset.attrs` holding stack level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p1_bs_3d = xr.concat([d[f'{polarizations[0]}_backscatter'] for d in polar_arrays], dim=stack.time)\n",
    "stack[f'{polarizations[0]}_backscatter'] = p1_bs_3d\n",
    "for d in polar_arrays:\n",
    "    d = d.drop_vars(f'{polarizations[0]}_backscatter')\n",
    "del p1_bs_3d\n",
    "\n",
    "p2_bs_3d = xr.concat([d[f'{polarizations[1]}_backscatter'] for d in polar_arrays], dim=stack.time)\n",
    "stack[f'{polarizations[1]}_backscatter'] = p2_bs_3d\n",
    "for d in polar_arrays:\n",
    "    d = d.drop_vars(f'{polarizations[1]}_backscatter')\n",
    "del p2_bs_3d\n",
    "\n",
    "for k in per_polar_keys:\n",
    "    if 'backscatter' not in k:\n",
    "        var_3d = xr.concat([xr.DataArray(data=d[k]) for d in polar_arrays], dim=stack.time)\n",
    "        stack[k] = var_3d   \n",
    "\n",
    "for k in per_pair_keys:\n",
    "    if polar_arrays[0][k].dtype == 'O':\n",
    "        var_3d = xr.concat([xr.DataArray(data=[d[k]]) for d in polar_arrays if d[k].dtype], dim=stack.time)\n",
    "    else:\n",
    "        var_3d = xr.concat([xr.DataArray(data=d[k]) for d in polar_arrays if d[k].dtype], dim=stack.time)\n",
    "    stack[k] = var_3d   \n",
    "    \n",
    "\n",
    "attrs = (get_backscatter_stack_attrs(product_paths[0]))\n",
    "attrs['DataCollectionTime']['NumberOfAcquisitions'] = len(times)\n",
    "attrs['DataCollectionTime']['FirstAcquisitionDate'] = times[0]\n",
    "attrs['DataCollectionTime']['LastAcquisitionDate'] = times[-1]\n",
    "attrs = {k: (v if type(v) != dict else json.dumps(v)) for k, v in attrs.items()}\n",
    "stack = stack.assign_attrs(attrs)\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Write the stack to a NetCDF and/or Zarr store\n",
    "\n",
    "**Save the time-series stack as a NetCDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%df\n",
    "print(\"\\nNoting your current available storage (shown above), do you wish to save the time-series as a NetCDF?\")\n",
    "netcdf = osl.select_parameter(['No, do NOT save a NetCDF', 'Yes, save a NetCDF'])\n",
    "display(netcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt_str(dt_str):\n",
    "    return datetime.strftime(datetime.strptime(dt_str, '%Y-%m-%dT%H:%M:%S'), '%Y%m%dT%H%M%S')\n",
    "\n",
    "if 'Yes' in netcdf.value:\n",
    "    netcdf_path = data_directory/f\"RTC_stack_{get_dt_str(times[0])}_{get_dt_str(times[-1])}.nc4\"\n",
    "    stack.to_netcdf(netcdf_path, engine='netcdf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the time-series stack as a local or remote Zarr-store. Chunk it for temporally optimized access, spatially optimized access, or both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do you wish to save the time-series as a local or remote Zarr-Store?\")\n",
    "locale = osl.select_parameter(['Local Zarr store (on my volume)', 'Remote Zarr store (in an S3 bucket)'])\n",
    "display(locale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do you wish to save a temporally optimized Zarr-store, a spatially optiimized Zarr-store, or both?\")\n",
    "opt = osl.select_parameter(['Temporally Optimized', 'Spatially Optimized', 'Both'])\n",
    "display(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chunks(stack: xr.Dataset, chunk_size=100, pol_count=2):\n",
    "    \"\"\"\n",
    "    stack: the xr.Dataset for which to determine chunks\n",
    "    chunk_size: int in MB\n",
    "    pol_count: number of polarizations\n",
    "    \"\"\"\n",
    "    chunks = list()\n",
    "    bits_per_mb = 8000000\n",
    "    bits_per_chunk = bits_per_mb * chunk_size\n",
    "    bits_per_pixel = int(json.loads(str(stack['VH_BackscatterMeasurementData'][0].data))['BitsPerSample'])\n",
    "    pixels_per_chunk = bits_per_chunk / bits_per_pixel\n",
    "    depth = len(stack.time)\n",
    "    \n",
    "    temp_op_xy_pixels = pixels_per_chunk // (depth * pol_count)\n",
    "    spatial_op_xy_pixels = pixels_per_chunk // pol_count\n",
    "    \n",
    "    temp_x_y_side = math.floor(math.sqrt(temp_op_xy_pixels))\n",
    "    spatial_x_y_side = math.floor(math.sqrt(spatial_op_xy_pixels))\n",
    "    \n",
    "    return {\n",
    "        'temporal': (depth, temp_x_y_side, temp_x_y_side),\n",
    "        'spatial': (1, spatial_x_y_side, spatial_x_y_side)\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_chunks(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write the Zarr store with a group for each selected chunking scheme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO handle bad s3 paths\n",
    "\n",
    "if 'Local' in locale.value:\n",
    "    store = data_directory/f\"RTC_stack_{get_dt_str(times[0])}_{get_dt_str(times[-1])}.zarr\"\n",
    "else:\n",
    "    s3_path = input(\"Enter the S3 path to your store\")\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    store = s3fs.S3Map(root=s3_path, s3=s3, check=False)\n",
    "\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "chunks = calc_chunks(stack)\n",
    "if 'Temporally' in opt.value or 'Both' in opt.value:\n",
    "    encoding = {vname: {'compressor': compressor, 'chunks': chunks['temporal']} for vname in stack.data_vars}\n",
    "    stack.to_zarr(store=store, encoding=encoding, consolidated=True, group='temporally_optimized', mode='w')\n",
    "\n",
    "if 'Spatially' in opt.value or 'Both' in opt.value:\n",
    "    encoding = {vname: {'compressor': compressor, 'chunks': chunks['spatial']} for vname in stack.data_vars}\n",
    "    stack.to_zarr(store=store, encoding=encoding, consolidated=True, group='spatially_optimized', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prepare_Hyp3_RTC_TimeSeries_NetCDF_Zarr.ipynb - Version 0.1.0 - March 2021*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc_analysis",
   "language": "python",
   "name": "conda-env-.local-rtc_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
