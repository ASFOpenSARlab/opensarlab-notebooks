{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "![OpenSARlab notebook banner](NotebookAddons/blackboard-banner.png)\n",
    "\n",
    "# Exploring RGB Visualzations of SAR and Deriving SAR Time Series Metrics\n",
    "\n",
    "### Franz J Meyer; University of Alaska Fairbanks & Josef Kellndorfer, [Earth Big Data, LLC](http://earthbigdata.com/) \n",
    "\n",
    "<img style=\"padding:7px;\" src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /></font>\n",
    "\n",
    "This notebook introduces you to some popular RGB visualizations of multi-temporal and multi-polarization SAR data. The exercise is done in the framework of *Jupyter Notebooks*. The Jupyter Notebook environment is easy to launch in any web browser for interactive data exploration with provided or new training data. Notebooks are comprised of text written in a combination of executable python code and markdown formatting including latex style mathematical equations. Another advantage of Jupyter Notebooks is that they can easily be expanded, changed, and shared with new data sets or newly available time series steps. Therefore, they provide an excellent basis for collaborative and repeatable data analysis. \n",
    "\n",
    "**This notebook covers the following data analysis concepts:**\n",
    "\n",
    "- How to create RGB visualizations of multi-temporal SAR data\n",
    "- How to interprete these RGB images\n",
    "- Popular RGB visualizations of dual-polarization SAR data\n",
    "- How do derive time-series metrics from deep SAR data stacks\n",
    "- How to export RGB products as GeoTiffs for visualization in a GIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**<font face=\"Calibri\" size=\"5\" color='rgba(200,0,0,0.2)'>Important Note about JupyterHub</font>**\n",
    "\n",
    "**Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%%javascript\n",
    "var kernel = Jupyter.notebook.kernel;\n",
    "var command = [\"notebookUrl = \",\n",
    "               \"'\", window.location, \"'\" ].join('')\n",
    "// alert(command)\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/rtc_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"rtc_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select the \"rtc_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"rtc_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **0. Importing Relevant Python Packages**\n",
    "In this notebook we will use the following scientific libraries:\n",
    "\n",
    "1. **[Pandas](https://pandas.pydata.org/)** is a Python library that provides high-level data structures and a vast variety of tools for analysis. The great feature of this package is the ability to translate rather complex operations with data into one or two commands. Pandas contains many built-in methods for filtering and combining data, as well as the time-series functionality. \n",
    "\n",
    "1. **[GDAL](https://www.gdal.org/)** is a software library for reading and writing raster and vector geospatial data formats. It includes a collection of programs tailored for geospatial data processing. Most modern GIS systems (such as ArcGIS or QGIS) use GDAL in the background.\n",
    "\n",
    "1. **[NumPy](http://www.numpy.org/)** is one of the principal packages for scientific applications of Python. It is intended for processing large multidimensional arrays and matrices, and an extensive collection of high-level mathematical functions and implemented methods makes it possible to perform various operations with these objects.\n",
    "\n",
    "1. **[Matplotlib](https://matplotlib.org/index.html)** is a low-level library for creating two-dimensional diagrams and graphs. With its help, you can build diverse charts, from histograms and scatterplots to non-Cartesian coordinates graphs. Moreover, many popular plotting libraries are designed to work in conjunction with matplotlib.\n",
    "\n",
    "1. The **[asf-hyp3 API](https://www.pydoc.io/pypi/asf-hyp3-1.1.1/index.html)** provides useful functions and scripts for accessing and processing SAR data via the Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline, or HyP3 (pronounced \"hype\").\n",
    "\n",
    "1. **[SciPY](https://www.scipy.org/about.html)** is a library that provides functions for numerical integration, interpolation, optimization, linear algebra and statistics.\n",
    "\n",
    "Our first step is to **import them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd # for DatetimeIndex\n",
    "from osgeo import gdal\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plb # for figure, grid, rcParams, savefig\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "from skimage import exposure # to enhance image display\n",
    "\n",
    "import asf_notebook as asfn\n",
    "asfn.jupytertheme_matplotlib_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **1. Load Data Stack <img src=\"NotebookAddons/Deforest-MadreDeDios.jpg\" width=\"350\" style=\"padding:7px;\" align=\"right\" />**\n",
    "\n",
    "This notebook will be using a 78-image deep dual-polarization C-band SAR data stack over Madre de Dios in Peru to demonstrate how to create color composites from multi temporal and dual-polarization SAR data and how to derive higher level parameters such as the multi-temporal mean, the coefficient of variation, and others. The C-band data were acquired by ESA's Sentinel-1 SAR sensor constellation and are available to you through the services of the [Alaska Satellite Facility](https://www.asf.alaska.edu/).\n",
    "\n",
    "The site in question is interesting as it has experienced extensive logging over the last 10 years (see image to the right; [Monitoring of the Andean Amazon Project](https://blog.globalforestwatch.org/)). Since the 1980s, people have been clearing forests in this area for farming, cattle ranching, logging, and (recently) gold mining. Creating RGB color composites is an easy way to visualize ongoing changes in the landscape.\n",
    "\n",
    "Before we get started, let's first **create a working directory for this analysis and change into it:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/jovyan/notebooks/SAR_Training/English/Ecosystems/S1-MadreDeDios\")\n",
    "\n",
    "if not path.exists():\n",
    "    path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will **retrieve the relevant data** from an [Amazon Web Service (AWS)](https://aws.amazon.com/) cloud storage bucket **using the following command**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_path = 's3://asf-jupyter-data/MadreDeDios.zip'\n",
    "time_series = Path(time_series_path).name\n",
    "!aws --region=us-east-1 --no-sign-request s3 cp $time_series_path $time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's **unzip the file (overwriting previous extractions) and clean up after ourselves:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(time_series).exists():\n",
    "    asfn.asf_unzip(str(path), time_series)\n",
    "    Path(time_series).unlink()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Define Data Directory and Path to VRT**\n",
    "**Create a variable containing the VRT filename and the image acquisition dates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalbuildvrt -separate {path}/raster_stack.vrt {path}/tiffs/*_VV.tiff\n",
    "image_file_VV = f\"{path}/raster_stack.vrt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an index of timedelta64 data with Pandas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {path}/tiffs/*_VV.tiff | sort | sed 's/[^0-9]*//g' | cut -c 2-9  > {path}/raster_stack_VV.dates\n",
    "\n",
    "datefile_VV = f'{path}/raster_stack_VV.dates'\n",
    "dates_VV = open(datefile_VV).readlines()\n",
    "tindex_VV = pd.DatetimeIndex(dates_VV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the bands and dates for all images in the virtual raster table (VRT):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Bands and dates for {image_file_VV}\")\n",
    "for i, d in enumerate(tindex_VV):\n",
    "    print(\"{:4d} {}\".format(i+1, d.date()), end=' ')\n",
    "    if (i+1)%5 == 1:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Open Your Data Stack and Visualize Some Layers**\n",
    "\n",
    "We will **open your VRT** and visualize some layers using Matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = gdal.Open(image_file_VV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the bands, pixels, and lines:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of  bands: {img.RasterCount}\")\n",
    "print(f\"Number of pixels: {img.RasterXSize}\")\n",
    "print(f\"Number of  lines: {img.RasterYSize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in raster data for the first two bands:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_1 = img.GetRasterBand(1).ReadAsArray() # change the number passed to GetRasterBand() to \n",
    "where_are_NaNs = np.isnan(raster_1)           # read rasters from different bands\n",
    "raster_1[where_are_NaNs] = 0\n",
    "\n",
    "raster_2 = img.GetRasterBand(78).ReadAsArray() #must pass a valid band number to GetRasterBand()\n",
    "where_are_NaNs = np.isnan(raster_2)\n",
    "raster_2[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot images and histograms for bands 1 and 2:**<br>\n",
    "Note: Depending the histograms plotted by this cell, you may wish to adjust `vmax` when calling `imshow()` on `ax1` and `ax3`. Increase the `vmax` value if the histogram cuts off much of the end of the peak, making your image too bright to see features well. Decrease `vmax` if the histogram extends much beyond the end of the peak, which will make your image appear dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the pyplot plots\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig = plb.figure(figsize=(18, 10)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(221)  # 221 determines: 2 rows, 2 plots, first plot\n",
    "ax2 = fig.add_subplot(222)  # 222 determines: 2 rows, 2 plots, second plot\n",
    "ax3 = fig.add_subplot(223)  # 223 determines: 2 rows, 2 plots, third plot\n",
    "ax4 = fig.add_subplot(224)  # 224 determines: 2 rows, 2 plots, fourth plot\n",
    "fig.subplots_adjust(hspace=.35)\n",
    "\n",
    "\n",
    "# Plot the band 1 image\n",
    "band_number = 1\n",
    "ax1.imshow(raster_1, cmap='gray', vmin=0, vmax=0.3) #,vmin=2000,vmax=10000)\n",
    "ax1.set_title(f'Image Band {band_number} {tindex_VV[band_number-1].date()}')\n",
    "\n",
    "# Flatten the band 1 image into a 1 dimensional vector and plot the histogram:\n",
    "h = ax2.hist(raster_1.flatten(), bins=200, range=(0, 0.3))\n",
    "ax2.xaxis.set_label_text('Calibrated Radar Cross Section (RCS) [Power Scale]')\n",
    "ax2.set_title(f'Histogram Band {band_number} {tindex_VV[band_number-1].date()}')\n",
    "\n",
    "# Plot the band 2 image\n",
    "band_number = 78\n",
    "ax3.imshow(raster_2, cmap='gray', vmin=0, vmax=0.3) #,vmin=2000,vmax=10000)\n",
    "ax3.set_title(f'Image Band {band_number} {tindex_VV[band_number-1].date()}')\n",
    "\n",
    "# Flatten the band 2 image into a 1 dimensional vector and plot the histogram:\n",
    "h = ax4.hist(raster_2.flatten(), bins=200, range=(0, 0.3))\n",
    "ax4.xaxis.set_label_text('Calibrated Radar Cross Section (RCS) [Power Scale]')\n",
    "ax4.set_title(f'Histogram Band {band_number} {tindex_VV[band_number-1].date()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **4. Create a Time Series Animation to get an Idea of the Dynamics at the Site**\n",
    "\n",
    "### 4.1 Load Time Series Stack\n",
    "\n",
    "Now we are ready to create a time series animation from the calibrated SAR data.\n",
    "\n",
    "**First, create a raster from band 0 and a raster stack from all the images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = img.GetRasterBand(1)\n",
    "raster0 = band.ReadAsArray()\n",
    "band_number = 0 # Needed for updates\n",
    "rasterstack_VV = img.ReadAsArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calibration and Data Conversion between dB and Power Scales \n",
    "\n",
    "**<font color='rgba(200,0,0,0.2)'>Note, that if your data were generated by HyP3, this step is not necessary! HyP3 performs the full data calibration and provides you with calibrated data in power scale.</font>**\n",
    "    \n",
    "If, your data is from a different source, however, calibration may be necessary to ensure that image gray values correspond to proper radar cross section information. \n",
    "\n",
    "Calibration coefficients for SAR data are often defined in the decibel (dB) scale due to the high dynamic range of the imaging system. For the L-band ALOS PALSAR data at hand, the conversion from uncalibrated DN values to calibrated radar cross section values in dB scale is performed by applying a standard **calibration factor of -83 dB**. \n",
    "\n",
    "$\\gamma^0_{dB} = 20 \\cdot log10(DN) -83$\n",
    "\n",
    "The data at hand are radiometrically terrain corrected images, which are often expressed as terrain flattened $\\gamma^0$ backscattering coefficients. For forest and land cover monitoring applications $\\gamma^o$ is the preferred metric.\n",
    "\n",
    "**To apply the calibration constant for your data and export in *dB* scale, uncomment the following code cell**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #caldB=20*np.log10(rasterstack)-83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **dB**-scaled images are often \"visually pleasing\", they are often not a good basis for mathematical operations on data. For instance, when we compute the mean of observations, it makes a difference whether we do that in power or dB scale. Since dB scale is a logarithmic scale, we cannot simply average data in that scale. \n",
    "    \n",
    "Please note that the **correct scale** in which operations need to be performed **is the power scale.** This is critical, e.g. when speckle filters are applied, spatial operations like block averaging are performed, or time series are analyzed.\n",
    "\n",
    "To **convert from dB to power**, apply: $\\gamma^o_{pwr} = 10^{\\frac{\\gamma^o_{dB}}{10}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calPwr=np.power(10.,caldB/10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 Create Time Series Animation \n",
    "\n",
    "**Create and move into a directory in which to store our plots and animations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_path = path/'plots_and_animations'\n",
    "\n",
    "if not product_path.exists():\n",
    "    product_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "ax.axis('off')\n",
    "vmin = np.percentile(rasterstack_VV.flatten(), 5)\n",
    "vmax = np.percentile(rasterstack_VV.flatten(), 95)\n",
    "\n",
    "r0dB = 20 * np.log10(raster0) - 83\n",
    "\n",
    "im = ax.imshow(raster0, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "ax.set_title(\"{}\".format(tindex_VV[0].date()))\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"{}\".format(tindex_VV[i].date()))\n",
    "    im.set_data(rasterstack_VV[i])\n",
    "\n",
    "# Interval is given in milliseconds\n",
    "ani = animation.FuncAnimation(fig, animate, frames=rasterstack_VV.shape[0], interval=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure matplotlib's RC settings for the animation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('animation', embed_limit=40971520.0)  # We need to increase the limit maybe to show the entire animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a javascript animation of the time-series running inline in the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete the dummy png** that was saved to the current working directory while generating the javascript animation in the last code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tmp_rm = product_path/'None0000000.png'\n",
    "    tmp_rm.unlink()\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the animation (animation.gif):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(product_path/'animation.gif', writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **5. Create RGB Visualization from Multi-Temporal SAR Images**\n",
    "\n",
    "Now we are ready to create our first RGB visualization. We will pick images from different dates/times/years to form an RGB. The colors in the RGB will then provide information on changes that occurred during the time span covered by the data.\n",
    "\n",
    "**First, we mask out pixels without relevant information to be unbiased in statical number we calculate later:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = rasterstack_VV == 0\n",
    "rasterPwr = np.ma.array(rasterstack_VV, mask=mask, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make an RGB stack to display the first, center, and last time step of our available stack as a multi-temporal color composite. The `np.dstack` results in an array of the form `[lines,pixels,bands]`, which is the format we need for RGB display with matplotlib's `imshow()` function. Note that numpy array indexing starts with 0, so band 1 is `raster[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_bands = (1, int(img.RasterCount/2), img.RasterCount)  # first, center, last band\n",
    "\n",
    "rgb_idx = np.array(rgb_bands)-1  # get array index from bands by subtracting 1\n",
    "\n",
    "rgb = np.dstack((rasterPwr[rgb_idx[0]], \n",
    "                 rasterPwr[rgb_idx[1]], \n",
    "                 rasterPwr[rgb_idx[2]]))\n",
    "\n",
    "rgb_dates = (tindex_VV[rgb_idx[0]].date(),\n",
    "             tindex_VV[rgb_idx[1]].date(), \n",
    "             tindex_VV[rgb_idx[2]].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in displaying the image enhanced with histogram equalization. We can use the function `exposure.equalize_hist()` from the `skimage.exposure` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stretched = rgb.copy()\n",
    "# For each band we apply the strech\n",
    "for i in range(rgb_stretched.shape[2]):\n",
    "    rgb_stretched[:,:,i] = exposure.\\\n",
    "    equalize_hist(rgb_stretched[:,:,i].data,\n",
    "    mask =~ np.equal(rgb_stretched[:,:,i].data,0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's **display the the unstrechted and histogram equalized images side by side.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle('Multi-temporal Sentinel-1 backscatter image R:{} G:{} B:{}'\n",
    "             .format(rgb_dates[0],rgb_dates[1],rgb_dates[2]))\n",
    "plt.axis('off')\n",
    "ax[0].imshow(rgb)\n",
    "ax[0].set_title('Unstreched')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(rgb_stretched)\n",
    "ax[1].set_title('Histogram Equalized')\n",
    "_ = ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<span style='font-size:20pt; font-weight:bold; color:red'><u>EXERCISE</u>: </span> Pick Different Time Steps and Interpret Resulting RGB Visualization\n",
    "\n",
    "To get a feeling for the RGB visualizations, please explore the 78 image deep data stack a bit more. Pick different mutli-temporal image layers and form more RGB visualizations. Interpret the colors you see in terms of the change that likely occurred on the ground.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, you may want to analyze these images in a GIS framework, where you can combine the SAR RGBs with other relevant data layers. To do so, we will export **your favorite multi-temporal RGB image** as a GeoTiff. First, we **write a function to convert our plots into GeoTiffs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = gdal.Open(image_file_VV)\n",
    "vrt_info = gdal.Info(vrt, format='json')\n",
    "coords = [vrt_info['cornerCoordinates']['upperLeft'], \n",
    "          vrt_info['cornerCoordinates']['lowerRight']]\n",
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zone = vrt_info['coordinateSystem']['wkt'].split('ID')[-1].split(',')[1][:-2]\n",
    "print(f\"utm_zone: {utm_zone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not include a file extension in out_filename\n",
    "# extent must be in the form of a list: [[upper_left_x, upper_left_y], [lower_right_x, lower_right_y]]\n",
    "def geotiff_from_plot(source_image, out_filename, extent, utm_zone, cmap=None, vmin=None, vmax=None, interpolation=None, dpi=300):\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(source_image, cmap=cmap, vmin=vmin, vmax=vmax, interpolation=interpolation)\n",
    "    temp = f\"{out_filename}_temp.png\"\n",
    "    plt.savefig(temp, dpi=dpi, transparent='true', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    cmd = f\"gdal_translate -of Gtiff -a_ullr {extent[0][0]} {extent[0][1]} {extent[1][0]} {extent[1][1]} -a_srs EPSG:{utm_zone} {temp} {out_filename}.tiff\"\n",
    "    !{cmd}\n",
    "    try:\n",
    "        Path(temp).unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can save your last RGB image as a GeoTIFF (`MadreDeDios-RGB.tiff`):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "geotiff_from_plot(rgb_stretched, product_path/'MadreDeDios-multitemp-RGB', coords, utm_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **6. Create RGB Visualization from Dual-Pol SAR Images**\n",
    "\n",
    "Now let's demonstrate another popular RGB visualization, one that is particularly useful for dual-pol SAR data such as Sentinel-1. The color composites are constructed with co-polarized (VV-channel) data assigned to the red channel, cross-polarized (VH) data to the green channel, and the co-/cross-polarized ratio (VV/VH ratio) to the blue channel. A nice effect for forest applications with this color assignment strategy is that forests tend to be shown in shades of green, and typically the brightness of green corresponds to the amount of biomass in the forest. Also, water tends to be represented in blue colors, which also represents other surface scattering components. Naturally, different histogram stretches may be applied to enhance various surface components.\n",
    "<br><br>\n",
    "\n",
    "**To create this visualization, we first need to load the cross-polarized VH data into the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalbuildvrt -separate {path}/raster_stack_VH.vrt {path}/tiffs/*_VH.tiff\n",
    "image_file_VH = path/\"raster_stack_VH.vrt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {path}/tiffs/*_VH.tiff | sort | sed 's/[^0-9]*//g' | cut -c 2-9 > {path}/raster_stack_VH.dates\n",
    "datefile_VH = path/'raster_stack_VH.dates'\n",
    "dates_VH = open(str(datefile_VH)).readlines()\n",
    "tindex_VH = pd.DatetimeIndex(dates_VH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_file_VH.exists():\n",
    "    print(f\"Bands and dates for {image_file_VH}\")\n",
    "    for i, d in enumerate(tindex_VH):\n",
    "        print(\"{:4d} {}\".format(i+1, d.date()),end=' ')\n",
    "        if (i+1)%5 == 1: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we load the VH data stack and prepare it for visualization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_VH = gdal.Open(str(image_file_VH))\n",
    "band_VH = img_VH.GetRasterBand(1)\n",
    "raster0_VH = band_VH.ReadAsArray()\n",
    "band_number = 0 # Needed for updates\n",
    "rasterstack_VH = img_VH.ReadAsArray()\n",
    "mask = rasterstack_VH == 0\n",
    "rasterPwr_VH = np.ma.array(rasterstack_VH, mask=mask, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='rgba(200,0,0,0.2)'>Pick a band number</font> you would like to visualize:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a band number for visualization\n",
    "bandno = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can create an RGB image using the mentioned color assignments (R: VV; G: VH; B: VV/VH), color stretch the RGB, and visualize it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_band = (bandno, bandno, bandno)  # first, center, last band\n",
    "rgb_idx = np.array(rgb_bands)-1  # get array index from bands by subtracting 1\n",
    "rgb = np.dstack((rasterPwr[rgb_idx[0]], rasterPwr_VH[rgb_idx[0]], rasterPwr[rgb_idx[0]]/rasterPwr_VH[rgb_idx[0]]))\n",
    "rgb_date = (tindex_VH[rgb_idx[0]].date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stretched_POL = rgb.copy()\n",
    "# For each band we apply the strech\n",
    "for i in range(rgb_stretched_POL.shape[2]):\n",
    "    rgb_stretched_POL[:,:,i] = exposure.\\\n",
    "    equalize_hist(rgb_stretched_POL[:,:,i].data,\n",
    "    mask =~ np.equal(rgb_stretched_POL[:,:,i].data,0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plb.figure(figsize=(16, 16)) # Initialize figure with a size\n",
    "ax1 = fig.add_subplot(221)  # 221 determines: 2 rows, 2 plots, first plot\n",
    "ax2 = fig.add_subplot(222)  # 222 determines: 2 rows, 2 plots, second plot\n",
    "ax3 = fig.add_subplot(223)  # 223 determines: 2 rows, 2 plots, third plot\n",
    "ax4 = fig.add_subplot(224)  # 224 determines: 2 rows, 2 plots, fourth plot\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Plot the VV band\n",
    "r_1 = img.GetRasterBand(bandno).ReadAsArray()\n",
    "vmin = np.percentile(r_1.flatten(), 5)\n",
    "vmax = np.percentile(r_1.flatten(), 95)\n",
    "ax1.imshow(r_1, cmap='gray', vmin=vmin, vmax=vmax) \n",
    "ax1.set_title('VV Channel')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Plot the VH band\n",
    "r_1 = img_VH.GetRasterBand(bandno).ReadAsArray()\n",
    "vmin = np.percentile(r_1.flatten(), 5)\n",
    "vmax = np.percentile(r_1.flatten(), 95)\n",
    "ax2.imshow(r_1, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "ax2.set_title('VH Channel')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Plot the VV/VH band\n",
    "r_1 = img.GetRasterBand(bandno).ReadAsArray()/img_VH.GetRasterBand(bandno).ReadAsArray()\n",
    "vmin = np.percentile(r_1.flatten(), 5)\n",
    "vmax = np.percentile(r_1.flatten(), 95)\n",
    "ax3.imshow(r_1, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "ax3.set_title('VV/VH Ratio Channel')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Plot the RGB Composite band\n",
    "ax4.imshow(rgb_stretched_POL)\n",
    "ax4.set_title('Color Composite')\n",
    "_ = ax4.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again **export the RGB composite as a GeoTiff to allow further analysis in your favorite GIS environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "geotiff_from_plot(rgb_stretched_POL, product_path/'MadreDeDios-multipol-RGB', coords, utm_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<!-- ### <font color='rgba(200,0,0,0.2)'> **<u>EXERCISE</u>**:</font> Pick Different Time Steps and Interpret Resulting Dual-Pol RGB Images  -->\n",
    "### <span style='color:rgba(200,0,0,0.2)'> EXERCISE:</span> Pick Different Time Steps and Interpret Resulting Dual-Pol RGB Images \n",
    "\n",
    "Explore the 78 image deep data stack a bit more. Pick different time steps in the data stack and create additional RGB composites. You may see interesting changes in the color patterns. Answer the following questions for yourself:\n",
    "\n",
    "- What are the green areas and what are different shades of green mean?\n",
    "- Why is there fewer green in the visualization than you might have expected?\n",
    "- What are the blue-shaded regions?\n",
    "- What kind of differences in color patterns do you see between different time steps? What might be the reasons for these changes?\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **7. Plot the Time Series of Means Calculated Across the Subset**\n",
    "\n",
    "To create the time series of means, we will go through the following steps:\n",
    "1. Ensure that you use the data in **power scale** ($\\gamma^o_{pwr}$) for your mean calculations.\n",
    "1. compute means.\n",
    "1. convert the resulting mean values into dB scale for visualization.\n",
    "1. plot time series of means. \n",
    "<br><br>\n",
    "**Compute the means:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_means_pwr_VH = np.mean(rasterPwr_VH, axis=(1, 2))\n",
    "rs_means_pwr_VV = np.mean(rasterPwr, axis=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert resulting mean value time-series to dB scale for visualization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_means_dB_VH = 10.*np.log10(rs_means_pwr_VH)\n",
    "rs_means_dB_VV = 10.*np.log10(rs_means_pwr_VV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot and save the time series of means (`RCSoverTime.png`):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Now let's plot the time series of means\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(tindex_VV, rs_means_dB_VV)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('VV Channel $\\overline{\\gamma^o}$ [power]')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(tindex_VH, rs_means_dB_VH, color='red')\n",
    "ax2.set_ylabel('VH Channel $\\overline{\\gamma^o}$ [dB]')\n",
    "fig.legend(['VV Channel', 'VH Channel'], loc=3)\n",
    "plt.title('Time series profile of average band backscatter $\\gamma^o$ ')\n",
    "plt.savefig(product_path/\"time_series_means\", dpi=72, transparent='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **8. Computation and Visualization of Time Series Metrics**\n",
    "\n",
    "Once a time-series was constructed, we can compute **a set of metrics** that will aid us later in applications such as *change detection and active agriculture detection*. In the next code cells, we will compute the following variables for each pixel in the stack:\n",
    "\n",
    "- Mean \n",
    "- Median\n",
    "- Maximum\n",
    "- Minimum\n",
    "- Range (Maximum - Minimum)\n",
    "- 5th Percentile\n",
    "- 95th Percentile\n",
    "- PRange (95th - 5th Percentile)\n",
    "- Variance\n",
    "- Coefficient of Variation (Variance/Mean)\n",
    "\n",
    "---\n",
    "\n",
    "Let's first **define a function for calculating the intended time-series metrics**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_metrics(raster, ndv=0): \n",
    "    # Make us of numpy nan functions\n",
    "    # Check if type is a float array\n",
    "    if not raster.dtype.name.find('float') > -1:\n",
    "        raster = raster.astype(np.float32)\n",
    "    # Set ndv to nan\n",
    "    if ndv != np.nan:\n",
    "        raster[np.equal(raster,ndv)] = np.nan\n",
    "    # Build dictionary of the metrics\n",
    "    tsmetrics = {}\n",
    "    rperc = np.nanpercentile(raster, [5, 50, 95], axis=0)\n",
    "    tsmetrics['mean'] = np.nanmean(raster, axis=0)\n",
    "    tsmetrics['max'] = np.nanmax(raster, axis=0)\n",
    "    tsmetrics['min'] = np.nanmin(raster, axis=0)\n",
    "    tsmetrics['range'] = tsmetrics['max'] - tsmetrics['min']\n",
    "    tsmetrics['median'] = rperc[1]\n",
    "    tsmetrics['p5'] = rperc[0]\n",
    "    tsmetrics['p95'] = rperc[2]\n",
    "    tsmetrics['prange'] = rperc[2] - rperc[0]\n",
    "    tsmetrics['var'] = np.nanvar(raster, axis=0)\n",
    "    tsmetrics['cov'] = tsmetrics['var'] / tsmetrics['mean']\n",
    "    return tsmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = timeseries_metrics(rasterPwr.filled(np.nan), ndv=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_keys = list(metrics.keys())\n",
    "print(metric_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the histograms for the time series variance and coeficient of variation to aid displaying those images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "ax[0].hist(metrics['var'].flatten(), bins=100, range=(0, 0.001))\n",
    "ax[1].hist(metrics['cov'].flatten(), bins=100, range=(0, 0.01))\n",
    "_ = ax[0].set_title('Variance')\n",
    "_ = ax[1].set_title('Coefficient of Variation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use thresholds determined from those histograms to set the scaling in the time series visualization. For the backscatter metrics we choose a typical range appropriate for this ecosystem and radar sensor. A typical range is -30 dB (0.0001) to -5.2 dB (0.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the metrics keys you want to plot\n",
    "fig = plt.figure(figsize=(16, 40))\n",
    "for i, key in enumerate(metric_keys):\n",
    "    ax = fig.add_subplot(5, 2, i+1)\n",
    "    if key == 'var': \n",
    "        vmin, vmax = (0.0, 0.001)\n",
    "    elif key == 'cov': \n",
    "        vmin, vmax = (0.0, 0.004)\n",
    "    else:\n",
    "        vmin, vmax = (0.0001, 0.3)\n",
    "    ax.imshow(metrics[key], vmin=vmin, vmax=vmax, cmap='gray')\n",
    "    ax.set_title(key.upper())\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again **export the RGB composite as a GeoTiff to allow further analysis in your favorite GIS environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "Names = [] # List to keep track of all the names\n",
    "for i in metric_keys:\n",
    "    # Name, Array, DataType, NDV,bandnames=None,ref_image\n",
    "    Name = product_path/f'MadreDeDios-{i}'\n",
    "    print(Name)\n",
    "    if i == 'var': \n",
    "        vmin, vmax = (0.0, 0.001)\n",
    "    elif i == 'cov': \n",
    "        vmin, vmax = (0.0, 0.004)\n",
    "    else:\n",
    "        vmin, vmax = (0.0001, 0.3)\n",
    "    geotiff_from_plot(metrics[i], Name, coords, utm_zone, cmap='gray', vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **8. Conclusion**\n",
    "\n",
    "RGB Visualizations can be a powerful tool to visually analyze multi-temporal or multi-polarization SAR images. For multi-temporal data, the different color bands of an RGB composite relate to different time steps; hence, color tones represent changes of the landscape over time. \n",
    "    \n",
    "For multi-polarization data, the RGB representation gives some indication of the type of surface cover present in an area. Areas shown in green in these composites usually are areas of high biomass while areas in blue are often regions covered in water. \n",
    "\n",
    "Please use caution when interpreting multi-polarization composites made from Sentinel-1 dual-pol data. As Sentinel-1 does not penetrate deep into vegetation, the composites often are not a correct representation of biomass in an area (composites don't look as green as they should).  \n",
    "\n",
    "For a bit more information on change detection and SAR in general, please look at the recently published [SAR Handbook: Comprehensive Methodologies for Forest Monitoring and Biomass Estimation](https://gis1.servirglobal.net/TrainingMaterials/SAR/SARHB_FullRes.pdf).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Exercise2-RGBandMetricsVisualization.ipynb - Version 1.4.1 - November 2021\n",
    "\n",
    "**Version Changes:**\n",
    "\n",
    "- Replaced `os` and obsolete `asfn` methods with `pathlib` counterparts\n",
    "- Changed some bash script commands to properly attain dates\n",
    "- Converted `html` to `Markdown`\n",
    "</i>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e41a24faa16793d68fa1f6dbeb53a9f75fc7f120354fb3eea5238b266b13540"
  },
  "kernelspec": {
   "display_name": "rtc_analysis [conda env:.local-rtc_analysis]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
