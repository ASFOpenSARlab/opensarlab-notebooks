{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h1>\n",
    "<b>WARNING:</b> This notebook has been deprecated and no longer runs in an OSL supported conda environment.\n",
    "</h1>\n",
    "<div>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![OpenSARlab notebook banner](NotebookAddons/blackboard-banner.png)\n",
    "\n",
    "# Exercise8A-InSARTimeSeriesGIAnTPreparation\n",
    "\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" />\n",
    "\n",
    "### Franz J Meyer & Joshua J C Knicely; University of Alaska Fairbanks\n",
    "\n",
    "This notebook accompanies Exercise8B-InSARTimeSeriesGIANTProcessing.ipynb\n",
    "\n",
    "The aim is to provide you background on how you can prepare interferograms so GIAnT can process them. \n",
    "\n",
    "This notebook uses a dataset that was prepared in advance. **You do not need to follow or execute the code contained in this notebook.** It is designed to provide the background on how to prepare interferograms for processing in GIAnT so that you can generate an InSAR time series using your own dataset if desired.\n",
    "\n",
    "<br>\n",
    "<img style=\"padding:7px;\" src=\"NotebookAddons/OpenSARlab_logo.svg\" width=\"170\" align=\"right\" />\n",
    "\n",
    "The code blocks show what was used to generate the files prepared for the InSARTimeSeriesGIANTProcessing notebook, and they would need to be amended as appropriate to process a different InSAR stack. The code is provided for your reference, but cannot be used directly as written to prepare a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Important Note about JupyterHub**\n",
    "\n",
    "Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/rtc_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"rtc_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select \"rtc_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"rtc_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python Libraries:\n",
    "\n",
    "**Import the Python libraries and modules we will need to run this lab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import h5py # for is_hdf5\n",
    "import shutil\n",
    "\n",
    "from osgeo import osr\n",
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from opensarlab_lib import asf_unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download GIAnT from the asf-jupyter-data S3 bucket**\n",
    "\n",
    "GIAnT is no longer supported (Python 2). This unofficial version of GIAnT has been partially ported to Python 3 to run this notebook. Only the portions of GIAnT used in this notebook have been tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_path = Path(\"/home/jovyan/.local/GIAnT/SCR\")\n",
    "\n",
    "if not giant_path.parent.exists():\n",
    "    download_path = 's3://asf-jupyter-data-west/GIAnT_5_21.zip'\n",
    "    output_path = f\"/home/jovyan/.local/{Path(download_path).name}\"\n",
    "    !aws --region=us-west-2 --no-sign-request s3 cp $download_path $output_path\n",
    "    if Path(output_path).is_file():\n",
    "        !unzip $output_path -d /home/jovyan/.local/\n",
    "        Path(output_path).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Input Files And Code for GIAnT\n",
    "\n",
    "The code below shows how to create the input files and specialty code that GIAnT requires. For this lab, 'ifg.list' is not needed, 'date.mli.par' has already been provided, 'prepxml_SBAS.py' is not needed as the 'sbas.xml' and 'data.xml' files it would create have already been provided, and 'userfn.py' is not needed as we are skipping the step in which it would be used.\n",
    "\n",
    "The files that would be created are listed below:\n",
    "        \n",
    "- ifg.list\n",
    "    - List of the interferogram properties including master and slave date, perpendicular baseline, and sensor. \n",
    "- date.mli.par\n",
    "    - File from which GIAnT pulls requisite information about the sensor. \n",
    "    - This is specifically for GAMMA files. When using other interferogram processing techniques, an alternate file is required. \n",
    "- prepxml_SBAS.py\n",
    "    - Python function to create an xml file that specifies the processing options to GIAnT. \n",
    "    - This must be modified by the user for their particular application. \n",
    "- userfn.py\n",
    "    - Python function to map the interferogram dates to a phyiscal file on disk. \n",
    "    - This must be modified by the user for their particular application. \n",
    "    \n",
    "---\n",
    "    \n",
    "### 2.1 Create 'ifg.list' File\n",
    "\n",
    "This will create simple 4 column text file will communicate network information to GIAnT. It will be created within the `GIAnT` directory.\n",
    "\n",
    "**This step has already been done, so we will not actually create the 'ifg.list' file. This code is displayed for your potential future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Get one of each file name. This assumes the unwrapped phase geotiff has been converted to a '.flt' file\n",
    "files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')] \n",
    "\n",
    "# Get all of the master and slave dates. \n",
    "masterDates,slaveDates = [],[]\n",
    "for file in files:\n",
    "    masterDates.append(file[0:8])\n",
    "    slaveDates.append(file[9:17])\n",
    "# Sort the dates according to the master dates. \n",
    "master_dates,sDates = (list(t) for t in zip(*sorted(zip(masterDates,slaveDates))))\n",
    "\n",
    "with open( os.path.join('GIAnT', 'ifg.list'), 'w') as fid:\n",
    "    for i in range(len(master_dates)):\n",
    "        masterDate = master_dates[i] # pull out master Date (first set of numbers)\n",
    "        slaveDate = sDates[i] # pull out slave Date (second set of numbers)\n",
    "        bperp = '0.0' # according to JPL notebooks\n",
    "        sensor = 'S1' # according to JPL notebooks\n",
    "        fid.write(f'{masterDate}  {slaveDate}  {bperp}  {sensor}\\n') # write values to the 'ifg.list' file. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You may notice that the code above sets the perpendicular baseline to a value of 0.0 m. This is not the true perpendicular baseline. That value can be found in metadata file (titled '$<$master timestamp$>$_$<$slave timestamp$>$.txt') that comes with the original interferogram. Generally, we would want the true baseline for each interferogram. However, since Sentinel-1 has such a short baseline, a value of 0.0 m is sufficient for our purposes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create 'date.mli.par' File\n",
    "\n",
    "As we are using GAMMA products, we must create a 'date.mli.par' file from which GIAnT will pull necessary information. If another processing technique is used to create the interferograms, an alternate file name and file inputs are required.\n",
    "\n",
    "**Again, this step has already been completed and the code is only displayed for your potential future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create file 'date.mli.par'\n",
    "\n",
    "# Get file names\n",
    "files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')]\n",
    "\n",
    "# Get WIDTH (xsize) and FILE_LENGTH (ysize) information\n",
    "ds = gdal.Open(datadirectory+files[0], gdal.GA_ReadOnly)\n",
    "type(ds)\n",
    "\n",
    "nLines = ds.RasterYSize\n",
    "nPixels = ds.RasterXSize\n",
    "\n",
    "trans = ds.GetGeoTransform()\n",
    "ds = None\n",
    "\n",
    "# Get the center line UTC time stamp; can also be found inside <date>_<date>.txt file and hard coded\n",
    "dirName = os.listdir('ingrams')[0] # get original file name (any file can be used; the timestamps are different by a few seconds)\n",
    "vals = dirName.split('-') # break file name into parts using the separator '-'\n",
    "tstamp = vals[2][9:16] # extract the time stamp from the 2nd datetime (could be the first)\n",
    "c_l_utc = int(tstamp[0:2])*3600 + int(tstamp[2:4])*60 + int(tstamp[4:6])\n",
    "\n",
    "rfreq = 299792548.0 / 0.055465763 # radar frequency; speed of light divided by radar wavelength of Sentinel1 in meters\n",
    "\n",
    "# write the 'date.mli.par' file\n",
    "with open(os.path.join(path, 'date.mli.par'), 'w') as fid:\n",
    "    # Method 1\n",
    "    fid.write(f'radar_frequency: {rfreq} \\n') # when using GAMMA products, GIAnT requires the radar frequency. Everything else is in wavelength (m) \n",
    "    fid.write(f'center_time: {c_l_utc} \\n') # Method from Tom Logan's prepGIAnT code; can also be found inside <date>_<date>.txt file and hard coded\n",
    "    fid.write( 'heading: -11.9617913 \\n') # inside <date>_<date>.txt file; can be hardcoded or set up so code finds it. \n",
    "    fid.write(f'azimuth_lines: {nLines} \\n') # number of lines in direction of the satellite's flight path\n",
    "    fid.write(f'range_samples: {nPixels} \\n') # number of pixels in direction perpendicular to satellite's flight path\n",
    "    fid.close() # close the file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Make prepxml_SBAS.py\n",
    "\n",
    "We will create a prepxml_SBAS.py function and put it into our GIAnT working directory. Again, this is shown for anyone that may want to use GIAnT on their own.\n",
    "\n",
    "If we do wish to change 'sbas.xml' or 'data.xml', this can be done by creating and running a new 'prepxml_SBAS.py'.\n",
    "\n",
    "### 2.3.1 Necessary prepxml_SBAS.py edits\n",
    "\n",
    "GIAnT comes with an example prepxml_SBAS.py, but requries significant edits for our purposes. These alterations have already been made, so we don't have to do anything now, but it is good to know the kinds of things that have to be altered. The details of some of these options can be found in the GIAnT documentation. The rest must be found in the GIAnT processing files themselves, most notably the tsxml.py and tsio.py functions.\n",
    "\n",
    "The following alterations were made:\n",
    "\n",
    "- Changed 'example' &#9658; 'date.mli.par'\n",
    "- Removed 'xlim', 'ylim', 'ref_x_lim', and 'ref_y_lim'\n",
    "    - These are used for clipping the files in GIAnT. As we have already done this, it is not necessary. \n",
    "- Removed latfile='lat.map' and lonfile='lon.map'\n",
    "    - These are optional inputs for the latitude and longitude maps. \n",
    "- Removed hgtfile='hgt.map'\n",
    "    - This is an optional altitude file for the sensor. \n",
    "- Removed inc=21.\n",
    "    - This is the optional incidence angle information. \n",
    "    - It can be a constant float value or incidence angle file. \n",
    "    - For Sentinel1, it varies from 29.1-46.0&deg;.\n",
    "- Removed masktype='f4'\n",
    "    - This is the mask designation. \n",
    "    - We are not using any masks for this. \n",
    "- Changed unwfmt='RMG' &#9658; unwfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Removed demfmt='RMG'\n",
    "- Changed corfmt='RMG' &#9658; corfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Changed nvalid=30 -> nvalid=1\n",
    "    - This is the minimum number of interferograms in which a pixel must be coherent. A particular pixel will be included only if its coherence is above the coherence threshold, cohth, in more than nvalid number of interferograms. \n",
    "- Removed atmos='ECMWF'\n",
    "    - This is an amtospheric correction command. It depends on a library called 'pyaps' developed for GIAnT. This library has not been installed yet. \n",
    "- Changed masterdate='19920604' &#9658; masterdate='20161119'\n",
    "    - Use our actual masterdate. \n",
    "    - I simply selected the earliest date as the masterdate. \n",
    "\n",
    "Defining a reference region is a potentially important step. This is a region at which there should be no deformation. For a volcano, this should be some significant distance away from the volcano. GIAnT has the ability to automatically select a reference region which we will use for this exercise.\n",
    "\n",
    "Below is an example of how the reference region would be defined when creating the XML file with the processing parameters. If we look at the prepxml_SBAS.py code below, ref_x_lim and ref_y_lim, the pixel based location of the reference region, is within the code, but has been commented out. \n",
    "\n",
    "**Define reference region:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_x_lim, ref_y_lim = [0, 10], [95, 105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the reference region would be defined. Look at the prepxml_SBAS.py code below. Note that ref_x_lim and ref_y_lim (the pixel based location of the reference region) are within the code.\n",
    "\n",
    "**This has already been completed but the code is here as an example script for creating XML files for use with the SBAS processing chain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    'date.mli.par', proc='GAMMA', \n",
    "    #ref_x_lim = [{1},{2}], ref_y_lim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt=1.0)\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the master date and create a script for creating XML files for use with the SBAS processing chain:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = [f for f in os.listdir(datadirectory) if f.endswith('_unw_phase.flt')]\n",
    "#master_date = min([files[i][0:8] for i in range(len(files))], key=int)\n",
    "\n",
    "# master_date = '20161119'\n",
    "\n",
    "# prepxml_SBAS_Template = '''\n",
    "#!/usr/bin/env python\n",
    "\n",
    "'''Example script for creating XML files for use with the SBAS processing chain. This script is supposed to be copied to the working directory and modified as needed.\"\"\"\n",
    "\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    'date.mli.par', proc='GAMMA', \n",
    "    #ref_x_lim = [{1},{2}], ref_y_lim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt=1.0)\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''\n",
    "#with open(os.path.join(path,'prepxml_SBAS.py'), 'w') as fid:\n",
    "#    fid.write(prepxml_SBAS_Template.format(path,ref_x_lim[0],ref_x_lim[1],ref_y_lim[0],ref_y_lim[1],master_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new 'sbas.xml' and 'data.xml' file, we would modify the above code to give new parameters and to write to the appropriate folder (e.g., to change the time filter from 1 year to none and to write to the directory in which we are working; 'filt=1.0' -> 'filt=0.0'; and 'os.path.join(path,'prepxml_SBAS.py') -> 'prepxml_SBAS.py' OR '%cd ~' into your home directory). Then we would run it below.\n",
    "\n",
    "### 2.4 Run prepxml_SBAS.py\n",
    "\n",
    "Here we run **prepxml_SBAS.py** to create the 2 needed files\n",
    "\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "\n",
    "To use MinTS, we would run **prepxml_MinTS.py** to create\n",
    "\n",
    "- data.xml\n",
    "- mints.xml\n",
    "        \n",
    "These files are needed by **PrepIgramStack.py**.\n",
    "\n",
    "**Run prepxml_SBAS.py in python 2.7 and check the output to confirm that your input values are correct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python $giant_path/prepxml_SBAS.py # this has already been done. data.xml and sbas.xml already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure the two requisite xml files (data.xml and sbas.xml) were produced after running prepxml_SBAS.py.**\n",
    "\n",
    "**Display the contents of data.xml:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_path = next(giant_path.parent.rglob('data.xml'))\n",
    "\n",
    "if data_x_path.exists():\n",
    "    !cat {data_x_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the contents of sbas.xml:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbas_x_path = next(giant_path.parent.rglob('sbas.xml'))\n",
    "\n",
    "if sbas_x_path.exists():\n",
    "    !cat {sbas_x_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create userfn.py\n",
    "\n",
    "Before running the next piece of code, `PrepIgramStack.py`, we must create a python file called `userfn.py`. This file maps the interferogram dates to a physical file on disk. This python file must be in our working directory, `/GIAnT`. We can create this file from within the notebook using python. \n",
    "\n",
    "**Again, this step has already been preformed and is unnecessary, but the code is provided as an example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "userfnTemplate = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "import os \n",
    "\n",
    "def makefnames(dates1, dates2, sensor):\n",
    "    dirname = '{0}'\n",
    "    root = os.path.join(dirname, dates1+'-'+dates2)\n",
    "    #unwname = root+'_unw_phase.flt' # for potentially disruptive default values kept. \n",
    "    unwname = root+'_unw_phase_no_default.flt' # for potentially disruptive default values removed. \n",
    "    corname = root+'_corr.flt'\n",
    "    return unwname, corname\n",
    "\"\"\"\n",
    "\n",
    "with open('userfn.py', 'w') as fid:\n",
    "    fid.write(userfnTemplate.format(path))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise8A-InSARTimeSeriesGIANTPreparation.ipynb - Version 1.3 - November 2021*\n",
    "\n",
    "*Version Changes:*\n",
    "\n",
    "- *asf_notebook -> opensarlab_lib*\n",
    "- *url_widget*\n",
    "- *html -> markdown*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc_analysis [conda env:.local-rtc_analysis]",
   "language": "python",
   "name": "conda-env-.local-rtc_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
